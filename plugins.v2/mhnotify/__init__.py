import time
import re
import threading
from urllib.parse import quote

from typing import List, Tuple, Dict, Any, Optional, Union
from apscheduler.triggers.cron import CronTrigger

from app.core.config import settings
from app.core.event import eventmanager, Event
from app.schemas.types import EventType, NotificationType
from app.utils.http import RequestUtils
from app.log import logger
from app.plugins import _PluginBase
from app.db import SessionFactory
from app.db.subscribe_oper import SubscribeOper


class MHNotify(_PluginBase):
    # æ’ä»¶åç§°
    plugin_name = "MediaHelperå¢å¼º"
    # æ’ä»¶æè¿°
    plugin_desc = "é…åˆMediaHelperä½¿ç”¨çš„ä¸€äº›å°åŠŸèƒ½"
    # æ’ä»¶å›¾æ ‡
    plugin_icon = "https://raw.githubusercontent.com/ListeningLTG/MoviePilot-Plugins/refs/heads/main/icons/mh2.jpg"
    # æ’ä»¶ç‰ˆæœ¬
    plugin_version = "1.6.7.1"
    # æ’ä»¶ä½œè€…
    plugin_author = "ListeningLTG"
    # ä½œè€…ä¸»é¡µ
    author_url = "https://github.com/ListeningLTG"
    # æ’ä»¶é…ç½®é¡¹IDå‰ç¼€
    plugin_config_prefix = "mhnotify_"
    # åŠ è½½é¡ºåº
    plugin_order = 1
    # å¯ä½¿ç”¨çš„ç”¨æˆ·çº§åˆ«
    auth_level = 1

    # çº¿ç¨‹é”
    _tmdb_locks: Dict[int, threading.Lock] = {}
    _locks_lock = threading.Lock()

    # ç§æœ‰å±æ€§
    _mh_domain = None
    _mh_username = None
    _mh_password = None
    _mh_job_names = None
    _enabled = False
    _last_event_time = 0
    # ä¸‹ä¸€æ¬¡å…è®¸é€šçŸ¥çš„æ—¶é—´æˆ³ï¼ˆç”¨äºç­‰å¾…çª—å£ï¼‰
    _next_notify_time = 0
    # ç­‰å¾…é€šçŸ¥æ•°é‡
    _wait_notify_count = 0
    #ï¼ˆå·²åºŸå¼ƒï¼‰
    _wait_minutes = 5
    # mhè®¢é˜…è¾…åŠ©å¼€å…³
    _mh_assist_enabled: bool = False
    # mhè®¢é˜…è¾…åŠ©ï¼šMPè®¢é˜…å®Œæˆåè‡ªåŠ¨åˆ é™¤MHè®¢é˜…
    _mh_assist_auto_delete: bool = False
    # åŠ©æ‰‹ï¼šå¾…æ£€æŸ¥çš„mhè®¢é˜…æ˜ å°„ï¼ˆmp_sub_id -> {mh_uuid, created_at, type}ï¼‰
    _ASSIST_PENDING_KEY = "mhnotify_assist_pending"
    # åŠ©æ‰‹ï¼šç­‰å¾…MPå®Œæˆååˆ é™¤mhè®¢é˜…çš„ç›‘å¬æ˜ å°„ï¼ˆmp_sub_id -> {mh_uuid}ï¼‰
    _ASSIST_WATCH_KEY = "mhnotify_assist_watch"
    # åŠ©æ‰‹ï¼šäº‘ä¸‹è½½è¾…åŠ©æ˜ å°„ï¼ˆinfo_hash -> {sid, mh_uuid}ï¼‰
    _ASSIST_CLOUD_MAP_KEY = "mhnotify_assist_cloud_map"
    # HDHive é…ç½®
    _hdhive_enabled: bool = False
    _hdhive_query_mode: str = "playwright"  # playwright/api
    _hdhive_username: str = ""
    _hdhive_password: str = ""
    _hdhive_cookie: str = ""
    _hdhive_auto_refresh: bool = False
    _hdhive_refresh_before: int = 3600
    # HDHive å®šæ—¶èµ„æºåˆ·æ–°
    _hdhive_refresh_enabled: bool = False
    _hdhive_refresh_cron: str = "0 */6 * * *"
    _hdhive_refresh_once: bool = False
    _hdhive_max_subscriptions: int = 20
    # MHç™»å½•ç¼“å­˜
    _mh_token: Optional[str] = None
    _mh_token_expire_ts: int = 0
    _mh_token_ttl_seconds: int = 600  # é»˜è®¤ç¼“å­˜10åˆ†é’Ÿ
    # åŠ©æ‰‹è°ƒåº¦å»¶è¿Ÿ/é‡è¯•å¸¸é‡ï¼ˆé¦–æ¬¡æŸ¥è¯¢2åˆ†é’Ÿï¼Œä¹‹åæ¯1åˆ†é’Ÿé‡è¯•ï¼‰
    _assist_initial_delay_seconds: int = 120
    _assist_retry_interval_seconds: int = 60
    # 115 ç”Ÿæ´»äº‹ä»¶ç›‘å¬
    _p115_life_enabled: bool = False
    _p115_cookie: str = ""
    _p115_events: List[str] = []  # å¯é€‰ï¼šupload/move/receive/create/copy/delete
    _p115_poll_cron: str = "* * * * *"  # æ¯åˆ†é’Ÿ
    _P115_LAST_TS_KEY = "mhnotify_p115_life_last_ts"
    _P115_LAST_ID_KEY = "mhnotify_p115_life_last_id"
    _p115_watch_dirs: List[str] = []  # ä»…å½“æ–‡ä»¶è·¯å¾„å‘½ä¸­è¿™äº›ç›®å½•å‰ç¼€æ—¶è§¦å‘
    _p115_watch_rules: List[Dict[str, Any]] = []  # [{path: '/ç›®å½•', events: ['upload', ...]}]
    _p115_wait_minutes: int = 5  # ç”Ÿæ´»äº‹ä»¶é™é»˜çª—å£ï¼ˆåˆ†é’Ÿï¼‰
    _p115_next_notify_time: int = 0  # ç”Ÿæ´»äº‹ä»¶ä¸‹ä¸€æ¬¡å…è®¸è§¦å‘çš„æ—¶é—´æˆ³
    _p115_dir_cache: Dict[int, str] = {}  # parent_id -> dir path ç¼“å­˜
    _rule_count: int = 3  # è§„åˆ™è¡Œæ•°ï¼ˆè¡¨å•åŠ¨æ€æ˜¾ç¤ºï¼‰
    #ï¼ˆå·²åºŸå¼ƒï¼‰æ˜¯å¦æ£€æµ‹ MP æ•´ç†è¿è¡Œ
    _check_mp_transfer_enabled: bool = False
    # MP æ•´ç†/åˆ®å‰Šäº‹ä»¶è§¦å‘å¼€å…³
    _mp_event_enabled: bool = False
    # MP äº‹ä»¶ç­‰å¾…æ—¶é—´ï¼ˆåˆ†é’Ÿï¼‰
    _mp_event_wait_minutes: int = 5
    # MP äº‹ä»¶ç›‘å¬çš„å­˜å‚¨ç±»å‹ï¼ˆå¤šé€‰ï¼‰
    _mp_event_storages: List[str] = []
    # å¯ç”¨å­˜å‚¨åˆ—è¡¨ç¼“å­˜
    _available_storages: List[Dict[str, str]] = []
    # äº‘ä¸‹è½½å¼€å…³
    _cloud_download_enabled: bool = False
    # äº‘ä¸‹è½½ä¿å­˜è·¯å¾„
    _cloud_download_path: str = "/äº‘ä¸‹è½½"
    # äº‘ä¸‹è½½å‰”é™¤å°æ–‡ä»¶å¼€å…³
    _cloud_download_remove_small_files: bool = False
    # äº‘ä¸‹è½½ç§»åŠ¨æ•´ç†å¼€å…³
    _cloud_download_organize: bool = False
    # äº‘ä¸‹è½½è¾…åŠ©è®¢é˜…å¼€å…³ï¼ˆä»…æ–°ç”µå½±è®¢é˜…ï¼‰
    _cloud_download_assist: bool = False
    # é˜¿é‡Œäº‘ç›˜ç§’ä¼ å¼€å…³
    _ali2115_enabled: bool = False
    # é˜¿é‡Œäº‘ç›˜ Refresh Token
    _ali2115_token: str = ""
    # é˜¿é‡Œäº‘ç›˜ç§’ä¼ ä¸´æ—¶æ–‡ä»¶å¤¹è·¯å¾„
    _ali2115_ali_folder: str = "/ç§’ä¼ è½¬å­˜"
    # 115äº‘ç›˜ç§’ä¼ æ¥æ”¶æ–‡ä»¶å¤¹è·¯å¾„
    _ali2115_115_folder: str = "/ç§’ä¼ æ¥æ”¶"
    # é˜¿é‡Œäº‘ç›˜ç§’ä¼ åç§»åŠ¨æ•´ç†å¼€å…³
    _ali2115_organize: bool = False
    # BTLé‰´æƒå‚æ•°
    _btl_app_id: str = "83768d9ad4"
    _btl_identity: str = "23734adac0301bccdcb107c4aa21f96c"

    def init_plugin(self, config: dict = None):
        if config:
            self._enabled = config.get("enabled")
            self._mh_domain = config.get("mh_domain")
            self._mh_username = config.get('mh_username')
            self._mh_password = config.get('mh_password')
            self._mh_job_names = config.get('mh_job_names') or ""
            # ç§»é™¤ MP æ•´ç†å»¶è¿Ÿçª—å£é…ç½®ï¼ˆä¿ç•™å ä½ä¸ç”Ÿæ•ˆï¼‰
            try:
                _ = int(config.get('wait_minutes') or 5)
            except Exception:
                pass
            # mhè®¢é˜…è¾…åŠ©å¼€å…³
            self._mh_assist_enabled = bool(config.get("mh_assist", False))
            # mhè®¢é˜…è¾…åŠ©ï¼šMPè®¢é˜…å®Œæˆåè‡ªåŠ¨åˆ é™¤MHè®¢é˜…ï¼ˆé»˜è®¤å…³é—­ï¼‰
            self._mh_assist_auto_delete = bool(config.get("mh_assist_auto_delete", False))

            # HDHive è®¾ç½®
            self._hdhive_enabled = bool(config.get("hdhive_enabled", False))
            self._hdhive_query_mode = config.get("hdhive_query_mode", "api") or "api"
            self._hdhive_username = config.get("hdhive_username", "") or ""
            self._hdhive_password = config.get("hdhive_password", "") or ""
            self._hdhive_cookie = config.get("hdhive_cookie", "") or ""
            self._hdhive_auto_refresh = bool(config.get("hdhive_auto_refresh", False))
            try:
                self._hdhive_refresh_before = int(config.get("hdhive_refresh_before", 3600) or 3600)
            except Exception:
                self._hdhive_refresh_before = 3600
            # HDHive èµ„æºåˆ·æ–°é…ç½®
            self._hdhive_refresh_enabled = bool(config.get("hdhive_refresh_enabled", False))
            self._hdhive_refresh_cron = (config.get("hdhive_refresh_cron") or "0 */6 * * *").strip() or "0 */6 * * *"
            # è¿è¡Œä¸€æ¬¡ï¼ˆä¿å­˜åç«‹å³è§¦å‘ä¸€æ¬¡åˆ·æ–°å¹¶å¤ä½ï¼‰
            try:
                if bool(config.get("hdhive_refresh_once", False)):
                    logger.info("mhnotify: æ£€æµ‹åˆ°è¿è¡Œä¸€æ¬¡HDHiveèµ„æºåˆ·æ–°å¼€å…³ï¼Œå¼€å§‹æ‰§è¡Œ...")
                    try:
                        # åœ¨åå°çº¿ç¨‹ä¸­æ‰§è¡Œï¼Œé¿å…é˜»å¡ä¿å­˜æµç¨‹
                        import threading
                        threading.Thread(target=self._execute_hdhive_refresh, kwargs={"subscription_name": None}, daemon=True).start()
                        logger.info("mhnotify: HDHiveèµ„æºåˆ·æ–°å·²åœ¨åå°å¯åŠ¨")
                    except Exception:
                        # å›é€€ä¸ºåŒæ­¥æ‰§è¡Œ
                        logger.warning("mhnotify: å¯åŠ¨åå°åˆ·æ–°å¤±è´¥ï¼Œæ”¹ä¸ºåŒæ­¥æ‰§è¡Œ")
                        self._execute_hdhive_refresh(subscription_name=None)
                    # å¤ä½ä¸ºå…³é—­ï¼Œå¹¶æ›´æ–°é…ç½®
                    config["hdhive_refresh_once"] = False
                    self.update_config(config)
                    logger.info("mhnotify: HDHiveèµ„æºåˆ·æ–°ä¸€æ¬¡æ€§ä»»åŠ¡æ‰§è¡Œå®Œæˆï¼Œå·²è‡ªåŠ¨å¤ä½ä¸ºå…³é—­")
            except Exception:
                logger.error("mhnotify: è¿è¡Œä¸€æ¬¡HDHiveèµ„æºåˆ·æ–°å¤±è´¥", exc_info=True)
            try:
                self._hdhive_max_subscriptions = int(config.get("hdhive_max_subscriptions", 20) or 20)
            except Exception:
                self._hdhive_max_subscriptions = 20
            
            # 115 ç”Ÿæ´»äº‹ä»¶
            self._p115_life_enabled = bool(config.get("p115_life_enabled", False))
            self._p115_cookie = config.get("p115_cookie", "") or ""
            self._p115_events = config.get("p115_life_events", []) or []
            # å…¼å®¹å­—ç¬¦ä¸²é€—å·åˆ†éš”
            if isinstance(self._p115_events, str):
                self._p115_events = [x.strip() for x in self._p115_events.split(',') if x.strip()]
            # è½®è¯¢é¢‘ç‡ï¼ˆä¿ç•™ä¸º cronï¼Œæš‚ä»…æ”¯æŒæ¯åˆ†é’Ÿï¼‰
            self._p115_poll_cron = config.get("p115_life_cron", "* * * * *") or "* * * * *"
            # ç›®å½•å‰ç¼€è¿‡æ»¤ï¼ˆå…¼å®¹æ—§é…ç½®ï¼‰
            watch_dirs = config.get("p115_watch_dirs", []) or []
            if isinstance(watch_dirs, str):
                watch_dirs = [x.strip() for x in watch_dirs.split(',') if x.strip()]
            # è§„èŒƒåŒ–ä¸ºä»¥ '/' å¼€å¤´çš„ Posix è·¯å¾„
            norm_dirs: List[str] = []
            for d in watch_dirs:
                d = d.replace('\\', '/').strip()
                if not d:
                    continue
                if not d.startswith('/'):
                    d = '/' + d
                # å»é™¤å°¾éš '/'
                d = d.rstrip('/')
                norm_dirs.append(d)
            self._p115_watch_dirs = norm_dirs
            
            # ç›®å½•äº‹ä»¶è§„åˆ™ï¼šä¼˜å…ˆä» rule_path_X / rule_events_X å­—æ®µè§£æï¼ˆæ–°è¡¨å•æ ¼å¼ï¼‰
            norm_rules: List[Dict[str, Any]] = []
            max_rules = 10
            
            # ä»æ–°æ ¼å¼è§£æï¼šrule_path_0, rule_events_0, ...
            for i in range(max_rules):
                path_key = f'rule_path_{i}'
                events_key = f'rule_events_{i}'
                p = (config.get(path_key) or '').replace('\\', '/').strip()
                if not p:
                    continue
                if not p.startswith('/'):
                    p = '/' + p
                p = p.rstrip('/')
                evs = config.get(events_key) or []
                if isinstance(evs, str):
                    evs = [x.strip().lower() for x in evs.split(',') if x.strip()]
                elif isinstance(evs, list):
                    evs = [str(x).strip().lower() for x in evs if str(x).strip()]
                norm_rules.append({'path': p, 'events': evs})
            
            # è‹¥æ–°æ ¼å¼ä¸ºç©ºï¼Œå°è¯•ä»æ—§çš„ JSON åˆ—è¡¨è§£æï¼ˆå…¼å®¹æ—§é…ç½®ï¼‰
            if not norm_rules:
                rules = config.get("p115_watch_rules", []) or []
                if isinstance(rules, list):
                    for r in rules:
                        try:
                            p = (r.get('path') or '').replace('\\', '/').strip()
                            if not p:
                                continue
                            if not p.startswith('/'):
                                p = '/' + p
                            p = p.rstrip('/')
                            evs = r.get('events') or []
                            if isinstance(evs, str):
                                evs = [x.strip().lower() for x in evs.split(',') if x.strip()]
                            elif isinstance(evs, list):
                                evs = [str(x).strip().lower() for x in evs if str(x).strip()]
                            norm_rules.append({'path': p, 'events': evs})
                        except Exception:
                            continue
            
            self._p115_watch_rules = norm_rules
            # åŒæ­¥æ›´æ–° p115_watch_rules é…ç½®ï¼ˆä¾› API ä½¿ç”¨ï¼‰
            config['p115_watch_rules'] = norm_rules
            
            # è§„åˆ™è¡Œæ•°ï¼ˆç”¨äºè¡¨å•åŠ¨æ€æ˜¾ç¤ºï¼‰
            try:
                self._rule_count = int(config.get('rule_count', 3) or 3)
                if self._rule_count < 1:
                    self._rule_count = 1
                if self._rule_count > 10:
                    self._rule_count = 10
            except Exception:
                self._rule_count = 3
            
            try:
                self._p115_wait_minutes = int(config.get('p115_wait_minutes', 5) or 5)
            except Exception:
                self._p115_wait_minutes = 5
            # ç§»é™¤ MP æ•´ç†æ£€æµ‹å¼€å…³ï¼ˆä¸å†ç”Ÿæ•ˆï¼‰
            self._check_mp_transfer_enabled = False
            
            # MP æ•´ç†/åˆ®å‰Šäº‹ä»¶è§¦å‘å¼€å…³
            self._mp_event_enabled = bool(config.get("mp_event_enabled", False))
            try:
                self._mp_event_wait_minutes = int(config.get('mp_event_wait_minutes', 5) or 5)
            except Exception:
                self._mp_event_wait_minutes = 5
            
            # MP äº‹ä»¶ç›‘å¬çš„å­˜å‚¨ç±»å‹
            self._mp_event_storages = config.get("mp_event_storages", []) or []
            if isinstance(self._mp_event_storages, str):
                self._mp_event_storages = [x.strip() for x in self._mp_event_storages.split(',') if x.strip()]
            
            # åˆå§‹åŒ–æ—¶è·å–å¯ç”¨å­˜å‚¨åˆ—è¡¨
            self._available_storages = self.__get_available_storages()
            
            # äº‘ä¸‹è½½é…ç½®
            self._cloud_download_enabled = bool(config.get("cloud_download_enabled", False))
            self._cloud_download_path = config.get("cloud_download_path", "/äº‘ä¸‹è½½") or "/äº‘ä¸‹è½½"
            self._cloud_download_remove_small_files = bool(config.get("cloud_download_remove_small_files", False))
            self._cloud_download_organize = bool(config.get("cloud_download_organize", False))
            self._cloud_download_assist = bool(config.get("cloud_download_assist", False))
            if self._cloud_download_assist:
                logger.info("mhnotify: äº‘ä¸‹è½½è¾…åŠ©è®¢é˜…åŠŸèƒ½å·²å¼€å¯ï¼ˆä»…æ–°ç”µå½±è®¢é˜…ç”Ÿæ•ˆï¼‰")
            
            # é˜¿é‡Œäº‘ç›˜ç§’ä¼ é…ç½®
            self._ali2115_enabled = bool(config.get("ali2115_enabled", False))
            self._ali2115_token = config.get("ali2115_token", "") or ""
            self._ali2115_ali_folder = config.get("ali2115_ali_folder", "/ç§’ä¼ è½¬å­˜") or "/ç§’ä¼ è½¬å­˜"
            self._ali2115_115_folder = config.get("ali2115_115_folder", "/ç§’ä¼ æ¥æ”¶") or "/ç§’ä¼ æ¥æ”¶"
            self._ali2115_organize = bool(config.get("ali2115_organize", False))

            # æ‰“å°å½“å‰è®°å½•çš„sidä¿¡æ¯
            try:
                pending = self.get_data(self._ASSIST_PENDING_KEY) or {}
                watch = self.get_data(self._ASSIST_WATCH_KEY) or {}
                
                if pending:
                    pending_details = []
                    with SessionFactory() as db:
                        for sid in pending.keys():
                            try:
                                sub = SubscribeOper(db=db).get(int(sid))
                                if sub:
                                    s_name = getattr(sub, 'name', '')
                                    s_tmdb = getattr(sub, 'tmdbid', '')
                                    s_season = getattr(sub, 'season', '')
                                    pending_details.append(f"[{sid}]{s_name}({s_tmdb}) S{s_season}")
                                else:
                                    pending_details.append(f"[{sid}]æœªçŸ¥(å·²åˆ ?)")
                            except:
                                pending_details.append(f"[{sid}]æŸ¥è¯¢å¤±è´¥")
                    logger.info(f"mhnotify: å½“å‰å¾…æ£€æŸ¥è¿›åº¦è®¢é˜…æ•°={len(pending)} è¯¦æƒ…: {', '.join(pending_details)}")

                if watch:
                    watch_details = []
                    with SessionFactory() as db:
                        for sid in watch.keys():
                            try:
                                sub = SubscribeOper(db=db).get(int(sid))
                                if sub:
                                    s_name = getattr(sub, 'name', '')
                                    s_tmdb = getattr(sub, 'tmdbid', '')
                                    s_season = getattr(sub, 'season', '')
                                    watch_details.append(f"[{sid}]{s_name}({s_tmdb}) S{s_season}")
                                else:
                                    watch_details.append(f"[{sid}]æœªçŸ¥(å·²åˆ ?)")
                            except:
                                watch_details.append(f"[{sid}]æŸ¥è¯¢å¤±è´¥")
                    logger.info(f"mhnotify: å½“å‰å¾…å®Œæˆ/æ¸…ç†MHè®¢é˜…æ•°={len(watch)} è¯¦æƒ…: {', '.join(watch_details)}")
            except Exception as e:
                logger.warning(f"mhnotify: æ‰“å°SIDä¿¡æ¯å¤±è´¥: {e}")

    def get_state(self) -> bool:
        return self._enabled

    def get_service(self) -> List[Dict[str, Any]]:
        """
        æ³¨å†Œæ’ä»¶å…¬å…±æœåŠ¡
        [{
            "id": "æœåŠ¡ID",
            "name": "æœåŠ¡åç§°",
            "trigger": "è§¦å‘å™¨ï¼šcron/interval/date/CronTrigger.from_crontab()",
            "func": self.xxx,
            "kwargs": {} # å®šæ—¶å™¨å‚æ•°
        }]
        """
        services = []
        if self._enabled:
            services.append({
                "id": "MHNotify",
                "name": "MediaHelperå¢å¼º",
                "trigger": CronTrigger.from_crontab("* * * * *"),
                "func": self.__notify_mh,
                "kwargs": {}
            })
        # mhè®¢é˜…è¾…åŠ©è°ƒåº¦
        if self._mh_assist_enabled:
            services.append({
                "id": "MHAssist",
                "name": "mhè®¢é˜…è¾…åŠ©",
                "trigger": CronTrigger.from_crontab("*/5 * * * *"),
                "func": self.__assist_scheduler,
                "kwargs": {}
            })
        # 115 ç”Ÿæ´»äº‹ä»¶ç›‘å¬
        if self._p115_life_enabled and (self._p115_cookie or "").strip():
            try:
                services.append({
                    "id": "P115LifeWatch",
                    "name": "115ç”Ÿæ´»äº‹ä»¶ç›‘å¬",
                    "trigger": CronTrigger.from_crontab(self._p115_poll_cron),
                    "func": self.__watch_115_life,
                    "kwargs": {}
                })
            except Exception:
                # è‹¥ cron éæ³•ï¼Œå›é€€æ¯åˆ†é’Ÿ
                services.append({
                    "id": "P115LifeWatch",
                    "name": "115ç”Ÿæ´»äº‹ä»¶ç›‘å¬",
                    "trigger": CronTrigger.from_crontab("* * * * *"),
                    "func": self.__watch_115_life,
                    "kwargs": {}
                })
        # HDHive å®šæ—¶èµ„æºåˆ·æ–°
        if self._hdhive_refresh_enabled:
            try:
                services.append({
                    "id": "HDHiveRefresh",
                    "name": "HDHiveèµ„æºå®šæ—¶åˆ·æ–°",
                    "trigger": CronTrigger.from_crontab(self._hdhive_refresh_cron),
                    "func": self.hdhive_refresh_job,
                    "kwargs": {}
                })
            except Exception:
                services.append({
                    "id": "HDHiveRefresh",
                    "name": "HDHiveèµ„æºå®šæ—¶åˆ·æ–°",
                    "trigger": CronTrigger.from_crontab("0 */6 * * *"),
                    "func": self.hdhive_refresh_job,
                    "kwargs": {}
                })
        return services

    @staticmethod
    def get_command() -> List[Dict[str, Any]]:
        """å®šä¹‰è¿œç¨‹æ§åˆ¶å‘½ä»¤"""
        return [
            {
                "cmd": "/mhol",
                "event": EventType.PluginAction,
                "desc": "æ·»åŠ 115äº‘ä¸‹è½½ä»»åŠ¡",
                "category": "ä¸‹è½½",
                "data": {
                    "action": "mh_add_offline"
                }
            },
            {
                "cmd": "/mhaly2115",
                "event": EventType.PluginAction,
                "desc": "é˜¿é‡Œäº‘ç›˜åˆ†äº«ç§’ä¼ åˆ°115",
                "category": "ä¸‹è½½",
                "data": {
                    "action": "mh_ali_to_115"
                }
            },
            {
                "cmd": "/mhrefresh",
                "event": EventType.PluginAction,
                "desc": "HDHiveèµ„æºåˆ·æ–°ï¼ˆå¯æŒ‡å®šè®¢é˜…åç§°ï¼‰",
                "category": "ä¸‹è½½",
                "data": {
                    "action": "mh_hdhive_refresh"
                }
            }
        ]

    def get_api(self) -> List[Dict[str, Any]]:
        # æä¾› 115 ç›®å½•æµè§ˆ APIï¼Œä¾¿äºåšç›®å½•é€‰æ‹©å™¨
        return [
            {
                "path": "/p115/list_directories",
                "endpoint": self.api_p115_list_directories,
                "methods": ["GET"],
                "summary": "åˆ—å‡º115ç½‘ç›˜æŒ‡å®šè·¯å¾„ä¸‹çš„ç›®å½•"
            },
            {
                "path": "/p115/watch_rules",
                "endpoint": self.api_p115_watch_rules,
                "methods": ["GET"],
                "summary": "è·å–å½“å‰ç›®å½•äº‹ä»¶è§„åˆ™"
            },
            {
                "path": "/p115/add_watch_rule",
                "endpoint": self.api_p115_add_watch_rule,
                "methods": ["POST"],
                "summary": "æ·»åŠ ç›®å½•äº‹ä»¶è§„åˆ™ï¼ˆpath, eventsï¼‰"
            },
            {
                "path": "/p115/remove_watch_rule",
                "endpoint": self.api_p115_remove_watch_rule,
                "methods": ["POST"],
                "summary": "ç§»é™¤ç›®å½•äº‹ä»¶è§„åˆ™ï¼ˆpathï¼‰"
            }
        ]

    def api_p115_list_directories(self, path: str = "/", apikey: str = "") -> dict:
        try:
            if apikey != settings.API_TOKEN:
                return {"success": False, "error": "APIå¯†é’¥é”™è¯¯"}
            if not self._p115_cookie:
                return {"success": False, "error": "æœªé…ç½® 115 Cookie"}
            # å¤ç”¨ç°æœ‰çš„ P115 å®¢æˆ·ç«¯å°è£…
            try:
                from app.plugins.p115strgmsub.clients.p115 import P115ClientManager  # type: ignore
            except Exception:
                P115ClientManager = None
            if not P115ClientManager:
                return {"success": False, "error": "ç¼ºå°‘ P115 å®¢æˆ·ç«¯ä¾èµ–ï¼ˆp115strgmsubï¼‰"}
            mgr = P115ClientManager(cookies=self._p115_cookie)
            if not mgr.check_login():
                return {"success": False, "error": "115 ç™»å½•å¤±è´¥ï¼ŒCookie å¯èƒ½å·²è¿‡æœŸ"}
            # è§„èŒƒåŒ–è·¯å¾„
            path = (path or "/").replace("\\", "/")
            if not path.startswith("/"):
                path = "/" + path
            directories = mgr.list_directories(path)
            # æ„å»ºé¢åŒ…å±‘
            breadcrumbs = []
            if path and path != "/":
                parts = [p for p in path.split("/") if p]
                current_path = ""
                breadcrumbs.append({"name": "æ ¹ç›®å½•", "path": "/"})
                for part in parts:
                    current_path = f"{current_path}/{part}"
                    breadcrumbs.append({"name": part, "path": current_path})
            else:
                breadcrumbs.append({"name": "æ ¹ç›®å½•", "path": "/"})
            return {
                "success": True,
                "path": path,
                "breadcrumbs": breadcrumbs,
                "directories": directories
            }
        except Exception as e:
            logger.error(f"mhnotify: åˆ—å‡º115ç›®å½•å¤±è´¥: {e}")
            return {"success": False, "error": str(e)}

    def _rules_to_text(self, rules: List[Dict[str, Any]]) -> str:
        """å°†è§„åˆ™åˆ—è¡¨è½¬æ¢ä¸ºæ–‡æœ¬æ ¼å¼"""
        lines = []
        for rule in rules:
            path = rule.get('path', '')
            events = rule.get('events', [])
            if path:
                if events:
                    lines.append(f"{path}:{','.join(events)}")
                else:
                    lines.append(path)
        return '\n'.join(lines)

    def api_p115_watch_rules(self, apikey: str = "") -> dict:
        try:
            if apikey != settings.API_TOKEN:
                return {"success": False, "error": "APIå¯†é’¥é”™è¯¯"}
            return {"success": True, "rules": self._p115_watch_rules}
        except Exception as e:
            return {"success": False, "error": str(e)}

    def api_p115_add_watch_rule(self, path: str = "/", events: Any = None, apikey: str = "") -> dict:
        try:
            if apikey != settings.API_TOKEN:
                return {"success": False, "error": "APIå¯†é’¥é”™è¯¯"}
            if not path or path == "":
                return {"success": False, "error": "ç¼ºå°‘ç›®å½•è·¯å¾„"}
            p = path.replace('\\', '/').strip()
            if not p.startswith('/'):
                p = '/' + p
            p = p.rstrip('/')
            evs: List[str] = []
            if events:
                if isinstance(events, str):
                    evs = [x.strip().lower() for x in events.split(',') if x.strip()]
                elif isinstance(events, list):
                    evs = [str(x).strip().lower() for x in events if str(x).strip()]
            # æ›´æ–°å†…å­˜ä¸é…ç½®
            rules = [r for r in (self._p115_watch_rules or []) if r.get('path') != p]
            rules.append({'path': p, 'events': evs})
            self._p115_watch_rules = rules
            cfg = self.get_config()
            if isinstance(cfg, dict):
                cfg['p115_watch_rules'] = rules
                cfg['p115_watch_rules_text'] = self._rules_to_text(rules)
                self.update_config(cfg)
            return {"success": True, "rules": rules}
        except Exception as e:
            return {"success": False, "error": str(e)}

    def api_p115_remove_watch_rule(self, path: str = "/", apikey: str = "") -> dict:
        try:
            if apikey != settings.API_TOKEN:
                return {"success": False, "error": "APIå¯†é’¥é”™è¯¯"}
            p = path.replace('\\', '/').strip()
            if not p.startswith('/'):
                p = '/' + p
            p = p.rstrip('/')
            rules = [r for r in (self._p115_watch_rules or []) if r.get('path') != p]
            self._p115_watch_rules = rules
            cfg = self.get_config()
            if isinstance(cfg, dict):
                cfg['p115_watch_rules'] = rules
                cfg['p115_watch_rules_text'] = self._rules_to_text(rules)
                self.update_config(cfg)
            return {"success": True, "rules": rules}
        except Exception as e:
            return {"success": False, "error": str(e)}

    def _build_rule_row(self, index: int) -> dict:
        """æ„å»ºå•æ¡ç›®å½•è§„åˆ™çš„è¡¨å•è¡Œ"""
        return {
            'component': 'VRow',
            'props': {'class': 'align-center'},
            'content': [
                {
                    'component': 'VCol',
                    'props': {'cols': 12, 'md': 6},
                    'content': [
                        {
                            'component': 'VTextField',
                            'props': {
                                'model': f'rule_path_{index}',
                                'label': f'ç›®å½• {index + 1}',
                                'placeholder': '/æˆ‘çš„æ¥æ”¶/ç”µå½±',
                                'density': 'compact',
                                'hide-details': True
                            }
                        }
                    ]
                },
                {
                    'component': 'VCol',
                    'props': {'cols': 12, 'md': 6},
                    'content': [
                        {
                            'component': 'VSelect',
                            'props': {
                                'model': f'rule_events_{index}',
                                'label': 'ç›‘å¬äº‹ä»¶',
                                'items': [
                                    {'title': 'ä¸Šä¼ ', 'value': 'upload'},
                                    {'title': 'ç§»åŠ¨', 'value': 'move'},
                                    {'title': 'æ¥æ”¶', 'value': 'receive'},
                                    {'title': 'æ–°å»º', 'value': 'create'},
                                    {'title': 'å¤åˆ¶', 'value': 'copy'},
                                    {'title': 'åˆ é™¤', 'value': 'delete'}
                                ],
                                'multiple': True,
                                'chips': True,
                                'closable-chips': True,
                                'clearable': True,
                                'density': 'compact',
                                'hide-details': True,
                                'hint': 'ç•™ç©ºç›‘å¬å…¨éƒ¨äº‹ä»¶'
                            }
                        }
                    ]
                }
            ]
        }

    def get_form(self) -> Tuple[List[dict], Dict[str, Any]]:
        """
        æ‹¼è£…æ’ä»¶é…ç½®é¡µé¢ï¼Œéœ€è¦è¿”å›ä¸¤å—æ•°æ®ï¼š1ã€é¡µé¢é…ç½®ï¼›2ã€æ•°æ®ç»“æ„
        """
        # å¦‚æœå­˜å‚¨åˆ—è¡¨ä¸ºç©ºï¼Œå°è¯•è·å–ä¸€æ¬¡
        if not self._available_storages:
            self._available_storages = self.__get_available_storages()
        
        # é¢„è®¾æœ€å¤š10æ¡è§„åˆ™
        max_rules = 10
        
        # è·å–å½“å‰é…ç½®çš„è§„åˆ™è¡Œæ•°ï¼ˆé»˜è®¤3è¡Œï¼‰
        current_rule_count = getattr(self, '_rule_count', 3)
        if current_rule_count < 1:
            current_rule_count = 1
        if current_rule_count > max_rules:
            current_rule_count = max_rules
        
        # æ„å»ºè§„åˆ™è¡Œï¼ˆåªæ˜¾ç¤º current_rule_count è¡Œï¼‰
        rule_rows = []
        for i in range(current_rule_count):
            rule_rows.append(self._build_rule_row(i))
        
        # æ„å»ºé»˜è®¤å€¼å­—å…¸ï¼ŒåŒ…å«ç°æœ‰è§„åˆ™
        defaults = {
            "_tabs": "tab_basic",
            "enabled": False,
            "mh_username": "",
            "mh_password": "",
            "mh_job_names": "",
            "mh_domain": "",
            "wait_minutes": 5,
            "mh_assist": False,
            "mh_assist_auto_delete": False,
            "hdhive_enabled": False,
            "hdhive_query_mode": "api",
            "hdhive_username": "",
            "hdhive_password": "",
            "hdhive_cookie": "",
            "hdhive_auto_refresh": False,
            "hdhive_refresh_before": 3600,
            "hdhive_refresh_enabled": False,
            "hdhive_refresh_cron": "0 */6 * * *",
            "hdhive_refresh_once": False,
            "hdhive_max_subscriptions": 20,
            "p115_life_enabled": False,
            "p115_cookie": "",
            "p115_life_events": [],
            "p115_life_cron": "* * * * *",
            "p115_watch_dirs": [],
            "p115_watch_rules": [],
            "p115_wait_minutes": 5,
            "check_mp_transfer": False,
            "rule_count": current_rule_count,
            "mp_event_enabled": False,
            "mp_event_wait_minutes": 5,
            "mp_event_storages": [],
            "cloud_download_enabled": False,
            "cloud_download_path": "/äº‘ä¸‹è½½",
            "cloud_download_assist": False,
            "cloud_download_remove_small_files": False,
            "cloud_download_organize": False,
            "ali2115_enabled": False,
            "ali2115_token": "",
            "ali2115_ali_folder": "/ç§’ä¼ è½¬å­˜",
            "ali2115_115_folder": "/ç§’ä¼ æ¥æ”¶",
            "ali2115_organize": False
        }
        
        # å°†ç°æœ‰è§„åˆ™å¡«å……åˆ°å¯¹åº”çš„ rule_path_X å’Œ rule_events_X
        for i in range(max_rules):
            defaults[f'rule_path_{i}'] = ""
            defaults[f'rule_events_{i}'] = []
        
        if self._p115_watch_rules:
            for i, rule in enumerate(self._p115_watch_rules[:max_rules]):
                defaults[f'rule_path_{i}'] = rule.get('path', '')
                defaults[f'rule_events_{i}'] = rule.get('events', [])

        return [
            {
                'component': 'VForm',
                'content': [
                    {
                        'component': 'VTabs',
                        'props': {
                            'model': '_tabs',
                            'fixed-tabs': True,
                            'show-arrows': True,
                            'slider-color': 'primary'
                        },
                        'content': [
                            {'component': 'VTab', 'props': {'value': 'tab_basic'}, 'text': 'åŸºç¡€é…ç½®'},
                            {'component': 'VTab', 'props': {'value': 'tab_monitor'}, 'text': 'è®¢é˜…è¾…åŠ©'},
                            {'component': 'VTab', 'props': {'value': 'tab_life'}, 'text': 'ç”Ÿæ´»äº‹ä»¶'},
                            {'component': 'VTab', 'props': {'value': 'tab_cloud'}, 'text': 'äº‘ä¸‹è½½'},
                            {'component': 'VTab', 'props': {'value': 'tab_ali'}, 'text': 'é˜¿é‡Œäº‘ç›˜ç§’ä¼ '},
                            {'component': 'VTab', 'props': {'value': 'tab_query'}, 'text': 'èµ„æºæŸ¥è¯¢'}
                        ]
                    },
                    {
                        'component': 'VWindow',
                        'props': {
                            'model': '_tabs'
                        },
                        'content': [
                            # Tab 0: åŸºç¡€é…ç½®
                            {
                                'component': 'VWindowItem',
                                'props': {'value': 'tab_basic'},
                                'content': [
                                    {'component': 'VRow', 'content': [{'component': 'VCol', 'props': {'cols': 12, 'md': 6}, 'content': [{'component': 'VSwitch', 'props': {'model': 'enabled', 'label': 'å¯ç”¨æ’ä»¶'}}]}]},
                                    {'component': 'VRow', 'content': [{'component': 'VCol', 'props': {'cols': 12, 'md': 6}, 'content': [{'component': 'VTextField', 'props': {'model': 'mh_domain', 'label': 'MediaHelperåœ°å€'}}]}, {'component': 'VCol', 'props': {'cols': 12, 'md': 6}, 'content': [{'component': 'VTextField', 'props': {'model': 'mh_username', 'label': 'MediaHelper_ç”¨æˆ·å'}}]}]},
                                    {'component': 'VRow', 'content': [{'component': 'VCol', 'props': {'cols': 12, 'md': 6}, 'content': [{'component': 'VTextField', 'props': {'model': 'mh_password', 'label': 'MediaHelper_å¯†ç ', 'type': 'password'}}]}, {'component': 'VCol', 'props': {'cols': 12, 'md': 6}, 'content': [{'component': 'VTextField', 'props': {'model': 'mh_job_names', 'label': 'strmä»»åŠ¡åç§°ï¼ˆè‹±æ–‡é€—å·åˆ†éš”ï¼‰', 'placeholder': 'ä¾‹å¦‚ï¼š115ç½‘ç›˜1,115ç½‘ç›˜2', 'hint': 'å¡«å†™strmç”Ÿæˆä»»åŠ¡åç§°ï¼›ç•™ç©ºåˆ™é»˜è®¤åŒ¹é…åç§°å«â€œ115ç½‘ç›˜â€'}}]}]},
                                    {'component': 'VRow', 'content': [{'component': 'VCol', 'props': {'cols': 12}, 'content': [{'component': 'VTextField', 'props': {'model': 'p115_cookie', 'label': '115 Cookie', 'type': 'password', 'placeholder': 'UID=...; CID=...; SEID=...ï¼ˆç²˜è´´å®Œæ•´ Cookieï¼‰', 'hint': 'ä» 115 ç½‘é¡µç‰ˆå¤åˆ¶å®Œæ•´ Cookieï¼›ä»…æœ¬åœ°ä½¿ç”¨ï¼Œä¸ä¼šå¯¹å¤–å‘é€'}}]}]}
                                ]
                            },
                            # Tab 1: è®¢é˜…è¾…åŠ©
                            {
                                'component': 'VWindowItem',
                                'props': {'value': 'tab_monitor'},
                                'content': [
                                    {'component': 'VRow', 'content': [{'component': 'VCol', 'props': {'cols': 12, 'md': 6}, 'content': [{'component': 'VSwitch', 'props': {'model': 'mh_assist', 'label': 'mhè®¢é˜…è¾…åŠ©ï¼ˆä»…æ–°è®¢é˜…ç”Ÿæ•ˆï¼‰', 'hint': 'å¼€å¯åï¼Œæ–°æ·»åŠ çš„è®¢é˜…å°†é»˜è®¤åœ¨MPä¸­æš‚åœï¼Œå¹¶ç”±æ’ä»¶åœ¨MHåˆ›å»ºè®¢é˜…ã€å»¶æ—¶æŸ¥è¯¢è¿›åº¦ã€æŒ‰è§„åˆ™åˆ é™¤æˆ–æ¢å¤MPè®¢é˜…ï¼›ä¸å½±å“å·²æœ‰è®¢é˜…'}}]}, {'component': 'VCol', 'props': {'cols': 12, 'md': 6}, 'content': [{'component': 'VSwitch', 'props': {'model': 'mh_assist_auto_delete', 'label': 'å–æ¶ˆæˆ–å®Œæˆè®¢é˜…åè‡ªåŠ¨åˆ é™¤MHè®¢é˜…', 'hint': 'å¼€å¯åï¼Œå½“MPè®¢é˜…å®Œæˆæˆ–å–æ¶ˆæ—¶ï¼Œè‡ªåŠ¨åˆ é™¤æˆ–æ›´æ–°å¯¹åº”çš„MH115è®¢é˜…ã€‚å…³é—­åˆ™ä¿ç•™MHè®¢é˜…'}}]}]},
                                    {'component': 'VRow', 'content': [{'component': 'VCol', 'props': {'cols': 12, 'md': 6}, 'content': [{'component': 'VSwitch', 'props': {'model': 'mp_event_enabled', 'label': 'MPäº‹ä»¶è§¦å‘ï¼ˆæ•´ç†/åˆ®å‰Šå®Œæˆï¼‰', 'hint': 'å¼€å¯åï¼Œå½“MPæ•´ç†æˆ–åˆ®å‰Šåª’ä½“å®Œæˆæ—¶ï¼Œè‡ªåŠ¨é€šçŸ¥MHæ‰§è¡Œstrmç”Ÿæˆä»»åŠ¡ï¼ˆæ— è¿è¡Œä»»åŠ¡åˆ™ç«‹å³è§¦å‘ï¼‰'}}]}, {'component': 'VCol', 'props': {'cols': 12, 'md': 6}, 'content': [{'component': 'VTextField', 'props': {'model': 'mp_event_wait_minutes', 'label': 'MPäº‹ä»¶ç­‰å¾…åˆ†é’Ÿæ•°', 'type': 'number', 'placeholder': 'é»˜è®¤ 5', 'hint': 'MPæ•´ç†å®Œæˆåï¼Œç­‰å¾…è¯¥åˆ†é’Ÿæ•°ä»¥ç¡®ä¿æ‰€æœ‰æ•´ç†ä»»åŠ¡å®Œæˆåå†è§¦å‘MHä»»åŠ¡'}}]}]},
                                    {'component': 'VRow', 'content': [{'component': 'VCol', 'props': {'cols': 12}, 'content': [{'component': 'VSelect', 'props': {'model': 'mp_event_storages', 'label': 'ç›‘å¬çš„å­˜å‚¨ç±»å‹', 'items': self._available_storages or [{'title': 'æœ¬åœ°', 'value': 'local'}, {'title': '115ç½‘ç›˜', 'value': 'u115'}, {'title': 'é˜¿é‡Œäº‘ç›˜', 'value': 'alipan'}, {'title': 'RClone', 'value': 'rclone'}, {'title': 'OpenList', 'value': 'alist'}], 'multiple': True, 'chips': True, 'closable-chips': True, 'clearable': True, 'density': 'compact', 'hint': 'ç•™ç©ºåˆ™ç›‘å¬æ‰€æœ‰å­˜å‚¨ç±»å‹çš„æ•´ç†/åˆ®å‰Šäº‹ä»¶'}}]}]}
                                ]
                            },
                            # Tab 2: ç”Ÿæ´»äº‹ä»¶
                            {
                                'component': 'VWindowItem',
                                'props': {'value': 'tab_life'},
                                'content': [
                                    {'component': 'VRow', 'content': [{'component': 'VCol', 'props': {'cols': 12}, 'content': [{'component': 'VAlert', 'props': {'type': 'info', 'variant': 'tonal', 'text': 'å¯é€‰ï¼šç›‘å¬ 115 ç”Ÿæ´»äº‹ä»¶ï¼ˆä¸Šä¼ /ç§»åŠ¨/æ¥æ”¶/æ–°å»º/å¤åˆ¶/åˆ é™¤ï¼‰ä»¥è§¦å‘ MH çš„ strm ä»»åŠ¡ã€‚'}}]}]},
                                    {'component': 'VRow', 'content': [{'component': 'VCol', 'props': {'cols': 12, 'md': 3}, 'content': [{'component': 'VSwitch', 'props': {'model': 'p115_life_enabled', 'label': 'ç›‘å¬ 115 ç”Ÿæ´»äº‹ä»¶'}}]}, {'component': 'VCol', 'props': {'cols': 12, 'md': 9}, 'content': [{'component': 'VAlert', 'props': {'type': 'warning', 'variant': 'tonal', 'density': 'compact', 'text': 'ä¸‹æ–¹å¯é…ç½®æœ€å¤š10æ¡ç›®å½•è§„åˆ™ï¼Œæ¯æ¡è§„åˆ™åŒ…å«ç›®å½•è·¯å¾„å’Œè¦ç›‘å¬çš„äº‹ä»¶ç±»å‹ã€‚äº‹ä»¶ç•™ç©ºè¡¨ç¤ºç›‘å¬è¯¥ç›®å½•çš„æ‰€æœ‰äº‹ä»¶ã€‚'}}]}]},
                                    {'component': 'VRow', 'content': [{'component': 'VCol', 'props': {'cols': 12, 'md': 6}, 'content': [{'component': 'span', 'props': {'class': 'text-subtitle-1 font-weight-bold'}, 'text': 'ğŸ“ ç›®å½•ç›‘å¬è§„åˆ™'}]}, {'component': 'VCol', 'props': {'cols': 12, 'md': 6}, 'content': [{'component': 'VTextField', 'props': {'model': 'rule_count', 'label': 'è§„åˆ™è¡Œæ•°', 'type': 'number', 'min': 1, 'max': 10, 'density': 'compact', 'hint': 'ä¿®æ”¹åä¿å­˜å³å¯å¢å‡è§„åˆ™è¡Œï¼ˆ1-10ï¼‰'}}]}]},
                                    *rule_rows,
                                    {'component': 'VRow', 'content': [{'component': 'VCol', 'props': {'cols': 12, 'md': 6}, 'content': [{'component': 'VTextField', 'props': {'model': 'p115_wait_minutes', 'label': '115 äº‹ä»¶ç­‰å¾…åˆ†é’Ÿæ•°', 'type': 'number', 'placeholder': 'é»˜è®¤ 5', 'hint': 'æ£€æµ‹åˆ° 115 ç”Ÿæ´»äº‹ä»¶åï¼Œç­‰å¾…è¯¥åˆ†é’Ÿæ•°ï¼›ç­‰å¾…æœŸé—´å¦‚æœ‰æ–°ç”Ÿæ´»äº‹ä»¶å°†æ»šåŠ¨å»¶é•¿ï¼Œé™é»˜åæ‰è§¦å‘ç”Ÿæˆä»»åŠ¡'}}]}]},
                                    {'component': 'VRow', 'content': [{'component': 'VCol', 'props': {'cols': 12}, 'content': [{'component': 'VAlert', 'props': {'type': 'info', 'variant': 'tonal', 'text': 'å½“æ£€æµ‹åˆ°åŒ¹é…çš„ 115 ç”Ÿæ´»äº‹ä»¶åï¼Œå°†åœ¨é™é»˜æœŸç»“æŸæ—¶è§¦å‘ MediaHelper çš„ strm ä»»åŠ¡'}}]}]}
                                ]
                            },
                            # Tab 3: äº‘ä¸‹è½½
                            {
                                'component': 'VWindowItem',
                                'props': {'value': 'tab_cloud'},
                                'content': [
                                    {'component': 'VRow', 'content': [{'component': 'VCol', 'props': {'cols': 12, 'md': 6}, 'content': [{'component': 'VSwitch', 'props': {'model': 'cloud_download_enabled', 'label': 'å¯ç”¨115äº‘ä¸‹è½½åŠŸèƒ½', 'hint': 'å¼€å¯åï¼Œå¯ä½¿ç”¨ /mhol å‘½ä»¤æ·»åŠ 115ç¦»çº¿ä¸‹è½½ä»»åŠ¡'}}]}, {'component': 'VCol', 'props': {'cols': 12, 'md': 6}, 'content': [{'component': 'VTextField', 'props': {'model': 'cloud_download_path', 'label': '115äº‘ä¸‹è½½ä¿å­˜è·¯å¾„', 'placeholder': '/äº‘ä¸‹è½½', 'hint': '115ç½‘ç›˜ä¸­ä¿å­˜ç¦»çº¿ä¸‹è½½æ–‡ä»¶çš„ç›®å½•è·¯å¾„'}}]}]},
                                    {'component': 'VRow', 'content': [{'component': 'VCol', 'props': {'cols': 12, 'md': 4}, 'content': [{'component': 'VSwitch', 'props': {'model': 'cloud_download_remove_small_files', 'label': 'å‰”é™¤å°æ–‡ä»¶', 'hint': 'äº‘ä¸‹è½½å®Œæˆåè‡ªåŠ¨åˆ é™¤å°äº10MBçš„æ–‡ä»¶', 'persistent-hint': True}}]}, {'component': 'VCol', 'props': {'cols': 12, 'md': 4}, 'content': [{'component': 'VSwitch', 'props': {'model': 'cloud_download_organize', 'label': 'ç§»åŠ¨æ•´ç†', 'hint': 'äº‘ä¸‹è½½å®Œæˆåè‡ªåŠ¨ç§»åŠ¨åˆ°MHé»˜è®¤ç›®å½•å¹¶æ•´ç†', 'persistent-hint': True}}]}, {'component': 'VCol', 'props': {'cols': 12, 'md': 4}, 'content': [{'component': 'VSwitch', 'props': {'model': 'cloud_download_assist', 'label': 'äº‘ä¸‹è½½è¾…åŠ©è®¢é˜…ï¼ˆä»…æ–°ç”µå½±è®¢é˜…ï¼‰', 'hint': 'æ–°è®¢é˜…ç”µå½±æœªå®Œæˆæ—¶æŒ‰è´¨é‡ä¼˜å…ˆçº§è‡ªåŠ¨åŒ¹é…èµ„æºå¹¶è§¦å‘äº‘ä¸‹è½½ï¼›å¤±è´¥åˆ™æ¢å¤è®¢é˜…å¯ç”¨'}}]}]},
                                    {'component': 'VRow', 'content': [{'component': 'VCol', 'props': {'cols': 12}, 'content': [{'component': 'VSwitch', 'props': {'model': 'clear_cloud_download_once', 'label': 'æ¸…ç†åŠ©æ‰‹äº‘ä¸‹è½½è®°å½•ï¼ˆè¿è¡Œä¸€æ¬¡ï¼‰', 'hint': 'âš ï¸ å¼€å¯åç‚¹ä¿å­˜ç«‹å³æ¸…é™¤æœ¬åŠ©æ‰‹é‡Œçš„äº‘ä¸‹è½½ç›‘æ§è®°å½•ï¼Œæ¸…ç†åå°†æ— æ³•å†ç›‘å¬ä¹‹å‰æ·»åŠ çš„äº‘ä¸‹è½½ä»»åŠ¡è®°å½•ã€‚å½“å‰ç‰ˆæœ¬äº‘ä¸‹è½½ä½¿ç”¨å®æ—¶çº¿ç¨‹ç›‘æ§ï¼ˆé¢„ç•™æ¥å£ï¼‰ï¼Œæ“ä½œåè‡ªåŠ¨å¤ä½ä¸ºå…³é—­'}}]}]},
                                    {'component': 'VRow', 'content': [{'component': 'VCol', 'props': {'cols': 12}, 'content': [{'component': 'VAlert', 'props': {'type': 'info', 'variant': 'tonal', 'density': 'comfortable', 'text': '/mhol â€” æ·»åŠ 115äº‘ä¸‹è½½ä»»åŠ¡ï¼›ä¼ å…¥ç£åŠ›é“¾æ¥ï¼Œä¿å­˜åˆ°é…ç½®çš„äº‘ä¸‹è½½è·¯å¾„ã€‚æ”¯æŒå¤šä¸ªé“¾æ¥ï¼Œç”¨è‹±æ–‡é€—å·ã€ç©ºæ ¼æˆ–æ¢è¡Œåˆ†éš”ã€‚'}}]}]}
                                ]
                            },
                            # Tab 4: é˜¿é‡Œäº‘ç›˜ç§’ä¼ 
                            {
                                'component': 'VWindowItem',
                                'props': {'value': 'tab_ali'},
                                'content': [
                                    {'component': 'VRow', 'content': [{'component': 'VCol', 'props': {'cols': 12, 'md': 6}, 'content': [{'component': 'VSwitch', 'props': {'model': 'ali2115_enabled', 'label': 'å¯ç”¨é˜¿é‡Œäº‘ç›˜ç§’ä¼ ', 'hint': 'å¼€å¯åï¼Œå¯ä½¿ç”¨ /mhaly2115 å‘½ä»¤å°†é˜¿é‡Œäº‘ç›˜åˆ†äº«ç§’ä¼ åˆ°115'}}]}, {'component': 'VCol', 'props': {'cols': 12, 'md': 6}, 'content': [{'component': 'VSwitch', 'props': {'model': 'ali2115_organize', 'label': 'ç§’ä¼ åç§»åŠ¨æ•´ç†', 'hint': 'ç§’ä¼ å®Œæˆåè‡ªåŠ¨ç§»åŠ¨åˆ°MHé»˜è®¤ç›®å½•å¹¶æ•´ç†', 'persistent-hint': True}}]}]},
                                    {'component': 'VRow', 'content': [{'component': 'VCol', 'props': {'cols': 12, 'md': 6}, 'content': [{'component': 'VTextField', 'props': {'model': 'ali2115_ali_folder', 'label': 'é˜¿é‡Œäº‘ç›˜ä¸´æ—¶æ–‡ä»¶å¤¹', 'placeholder': '/ç§’ä¼ è½¬å­˜', 'hint': 'é˜¿é‡Œäº‘ç›˜ä¸­ç”¨äºä¸´æ—¶è½¬å­˜åˆ†äº«æ–‡ä»¶çš„ç›®å½•'}}]}, {'component': 'VCol', 'props': {'cols': 12, 'md': 6}, 'content': [{'component': 'VTextField', 'props': {'model': 'ali2115_115_folder', 'label': '115äº‘ç›˜ç§’ä¼ æ¥æ”¶æ–‡ä»¶å¤¹', 'placeholder': '/ç§’ä¼ æ¥æ”¶', 'hint': '115ç½‘ç›˜ä¸­æ¥æ”¶ç§’ä¼ æ–‡ä»¶çš„ç›®å½•'}}]}]},
                                    {'component': 'VRow', 'content': [{'component': 'VCol', 'props': {'cols': 12}, 'content': [{'component': 'VTextField', 'props': {'model': 'ali2115_token', 'label': 'é˜¿é‡Œäº‘ç›˜ Refresh Token', 'type': 'password', 'placeholder': 'è¾“å…¥é˜¿é‡Œäº‘ç›˜çš„ refresh_token', 'hint': 'ä»é˜¿é‡Œäº‘ç›˜å®¢æˆ·ç«¯æˆ–æµè§ˆå™¨è·å–çš„ refresh_tokenï¼Œç”¨äºè®¤è¯é˜¿é‡Œäº‘ç›˜API'}}]}]},
                                    {'component': 'VRow', 'content': [{'component': 'VCol', 'props': {'cols': 12}, 'content': [{'component': 'VAlert', 'props': {'type': 'info', 'variant': 'tonal', 'density': 'comfortable', 'text': '/mhaly2115 â€” é˜¿é‡Œäº‘ç›˜åˆ†äº«ç§’ä¼ åˆ°115ï¼›éœ€å·²é…ç½®é˜¿é‡Œäº‘ç›˜Refresh Tokenã€‚'}}]}]}
                                ]
                            },
                            # Tab 5: èµ„æºæŸ¥è¯¢
                            {
                                'component': 'VWindowItem',
                                'props': {'value': 'tab_query'},
                                'content': [
                                    {'component': 'VRow', 'content': [{'component': 'VCol', 'props': {'cols': 12}, 'content': [{'component': 'VAlert', 'props': {'type': 'info', 'variant': 'tonal', 'text': 'HDHiveèµ„æºæŸ¥è¯¢ï¼šæ”¯æŒ Playwright/API ä¸¤ç§æ¨¡å¼ï¼Œè·å–å…è´¹ 115 åˆ†äº«é“¾æ¥å¹¶è‡ªåŠ¨ä½œä¸ºè‡ªå®šä¹‰é“¾æ¥éšè®¢é˜…ä¼ å…¥'}}]}]},
                                    {'component': 'VRow', 'content': [{'component': 'VCol', 'props': {'cols': 12, 'md': 3}, 'content': [{'component': 'VSwitch', 'props': {'model': 'hdhive_enabled', 'label': 'å¯ç”¨ HDHive'}}]}, {'component': 'VCol', 'props': {'cols': 12, 'md': 3}, 'content': [{'component': 'VSelect', 'props': {'model': 'hdhive_query_mode', 'label': 'HDHive æŸ¥è¯¢æ¨¡å¼', 'items': [{'title': 'Playwright', 'value': 'playwright'}, {'title': 'API', 'value': 'api'}], 'clearable': False}}]}, {'component': 'VCol', 'props': {'cols': 12, 'md': 3}, 'content': [{'component': 'VTextField', 'props': {'model': 'hdhive_username', 'label': 'HDHive ç”¨æˆ·å'}}]}, {'component': 'VCol', 'props': {'cols': 12, 'md': 3}, 'content': [{'component': 'VTextField', 'props': {'model': 'hdhive_password', 'label': 'HDHive å¯†ç ', 'type': 'password'}}]}]},
                                    {'component': 'VRow', 'content': [{'component': 'VCol', 'props': {'cols': 12, 'md': 6}, 'content': [{'component': 'VTextField', 'props': {'model': 'hdhive_cookie', 'label': 'HDHive Cookieï¼ˆAPI æ¨¡å¼ï¼‰', 'type': 'password'}}]}, {'component': 'VCol', 'props': {'cols': 12, 'md': 3}, 'content': [{'component': 'VSwitch', 'props': {'model': 'hdhive_auto_refresh', 'label': 'è‡ªåŠ¨åˆ·æ–° Cookie'}}]}, {'component': 'VCol', 'props': {'cols': 12, 'md': 3}, 'content': [{'component': 'VTextField', 'props': {'model': 'hdhive_refresh_before', 'label': 'Cookieæå‰åˆ·æ–°ç§’æ•°', 'type': 'number', 'placeholder': 'é»˜è®¤ 3600'}}]}]},
                                    {'component': 'VRow', 'content': [{'component': 'VCol', 'props': {'cols': 12, 'md': 3}, 'content': [{'component': 'VSwitch', 'props': {'model': 'hdhive_refresh_enabled', 'label': 'å¯ç”¨HDHiveèµ„æºå®šæ—¶åˆ·æ–°', 'hint': 'å¼€å¯åæŒ‰Cronå‘¨æœŸåˆ·æ–°MHè®¢é˜…çš„è‡ªå®šä¹‰é“¾æ¥'}}]}, {'component': 'VCol', 'props': {'cols': 12, 'md': 9}, 'content': [{'component': 'VTextField', 'props': {'model': 'hdhive_refresh_cron', 'label': 'åˆ·æ–°è®¡åˆ’ Cron', 'placeholder': 'ä¾‹å¦‚ï¼š0 */6 * * *ï¼ˆæ¯6å°æ—¶ï¼‰', 'hint': 'ä½¿ç”¨æ ‡å‡†Crontabè¡¨è¾¾å¼'}}]}]},
                                    {'component': 'VRow', 'content': [{'component': 'VCol', 'props': {'cols': 12, 'md': 6}, 'content': [{'component': 'VTextField', 'props': {'model': 'hdhive_max_subscriptions', 'label': 'æœ€å¤šè®¢é˜…æ¡æ•°', 'type': 'number', 'placeholder': 'é»˜è®¤ 20', 'hint': 'ä»…å¤„ç†å¯ç”¨ä¸”ä¸º115çš„å‰ N æ¡è®¢é˜…'}}]}, {'component': 'VCol', 'props': {'cols': 12, 'md': 6}, 'content': [{'component': 'VSwitch', 'props': {'model': 'hdhive_refresh_once', 'label': 'è¿è¡Œä¸€æ¬¡HDHiveèµ„æºåˆ·æ–°', 'hint': 'å¼€å¯åä¿å­˜å°†ç«‹å³æ‰§è¡Œä¸€æ¬¡åˆ·æ–°ä»»åŠ¡ï¼Œæ‰§è¡Œåè‡ªåŠ¨å¤ä½ä¸ºå…³é—­'}}]}]},
                                    {'component': 'VRow', 'content': [{'component': 'VCol', 'props': {'cols': 12}, 'content': [{'component': 'VAlert', 'props': {'type': 'info', 'variant': 'tonal', 'density': 'comfortable', 'text': '/mhrefresh [è®¢é˜…1,è®¢é˜…2,...] â€” HDHiveèµ„æºåˆ·æ–°ï¼›æ”¯æŒå¤šä¸ªè®¢é˜…åç§°ç”¨è‹±æ–‡é€—å·åˆ†éš”ã€‚ä¸å¸¦å‚æ•°æ—¶åˆ·æ–°å‰Nä¸ªå¯ç”¨çš„115è®¢é˜…ï¼ˆNä¸ºâ€œæœ€å¤šè®¢é˜…æ¡æ•°â€ï¼‰ã€‚'}}]}]}
                                ]
                            }
                        ]
                    }
                ]
            }
        ], defaults

    def get_page(self) -> List[dict]:
        pass

    @eventmanager.register(EventType.TransferComplete)
    @eventmanager.register(EventType.DownloadAdded)
    def send(self, event):
        """
        ç›‘å¬ MP æ•´ç†å®Œæˆå’Œåˆ®å‰Šå®Œæˆäº‹ä»¶ï¼Œè§¦å‘ MH ç”Ÿæˆ strm ä»»åŠ¡
        éœ€è¦åœ¨é…ç½®ä¸­å¼€å¯ 'MPäº‹ä»¶è§¦å‘' å¼€å…³
        æ”¯æŒæŒ‰å­˜å‚¨ç±»å‹è¿‡æ»¤
        """
        if not self._enabled or not self._mp_event_enabled:
            return
        
        if not event or not event.event_type:
            return
        
        # è¾…åŠ©å‡½æ•°ï¼šå°†äº‹ä»¶å¯¹è±¡é€’å½’è½¬æ¢ä¸ºå­—å…¸
        def __to_dict(_event):
            if _event is None:
                return None
            elif isinstance(_event, dict):
                return {k: __to_dict(v) for k, v in _event.items()}
            elif isinstance(_event, list):
                return [__to_dict(item) for item in _event]
            elif isinstance(_event, tuple):
                return tuple(__to_dict(list(_event)))
            elif isinstance(_event, set):
                return set(__to_dict(list(_event)))
            elif hasattr(_event, 'to_dict'):
                return __to_dict(_event.to_dict())
            elif hasattr(_event, '__dict__'):
                return __to_dict(_event.__dict__)
            elif isinstance(_event, (int, float, str, bool, type(None))):
                return _event
            else:
                return str(_event)
        
        # è·å–äº‹ä»¶ç±»å‹
        version = getattr(settings, "VERSION_FLAG", "v1")
        event_type = event.event_type if version == "v1" else event.event_type.value
        
        # åªå¤„ç†æ•´ç†å®Œæˆå’Œåˆ®å‰Šå®Œæˆäº‹ä»¶
        if event_type not in ["transfer.complete", "metadata.scrape", EventType.TransferComplete, EventType.DownloadAdded]:
            return
        
        # è§£æäº‹ä»¶æ•°æ®
        event_data = __to_dict(event.event_data)
        storage = None
        name = None
        
        try:
            # æ•´ç†å®Œæˆäº‹ä»¶
            if event_type in ["transfer.complete", EventType.TransferComplete]:
                transferinfo = event_data.get("transferinfo", {})
                success = transferinfo.get("success", False)
                if not success:
                    return
                
                target_diritem = transferinfo.get("target_diritem", {})
                target_item = transferinfo.get("target_item", {})
                storage = target_diritem.get("storage")
                name = target_item.get("name")
            
            # åˆ®å‰Šå®Œæˆäº‹ä»¶
            elif event_type in ["metadata.scrape", EventType.DownloadAdded]:
                fileitem = event_data.get("fileitem", {})
                storage = fileitem.get("storage") if isinstance(fileitem, dict) else None
                name = event_data.get("name")
        
        except Exception as e:
            logger.error(f"mhnotify: è§£æäº‹ä»¶æ•°æ®å¤±è´¥: {e}")
            return
        
        # æ£€æŸ¥å­˜å‚¨ç±»å‹è¿‡æ»¤
        if self._mp_event_storages:
            if not storage or storage not in self._mp_event_storages:
                logger.debug(f"mhnotify: å­˜å‚¨ç±»å‹ [{storage}] ä¸åœ¨ç›‘å¬åˆ—è¡¨ä¸­ï¼Œå¿½ç•¥äº‹ä»¶")
                return
        
        logger.info(f"mhnotify: æ”¶åˆ° MP äº‹ä»¶ [{event_type}]ï¼Œå­˜å‚¨: [{storage}]ï¼Œæ–‡ä»¶: [{name}]")
        
        # å¢åŠ å¾…é€šçŸ¥è®¡æ•°
        self._wait_notify_count += 1
        self._last_event_time = self.__get_time()
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ­£åœ¨è¿è¡Œçš„æ•´ç†ä»»åŠ¡
        if self.__has_running_transfers():
            logger.info("mhnotify: æ£€æµ‹åˆ°æ­£åœ¨è¿è¡Œçš„æ•´ç†ä»»åŠ¡ï¼Œå»¶è¿Ÿè§¦å‘")
            # è®¾ç½®ç­‰å¾…çª—å£
            now_ts = self.__get_time()
            wait_seconds = self._mp_event_wait_minutes * 60
            self._next_notify_time = now_ts + wait_seconds
        else:
            logger.info("mhnotify: æ— è¿è¡Œä¸­çš„æ•´ç†ä»»åŠ¡ï¼Œå°†åœ¨ä¸‹æ¬¡è°ƒåº¦æ—¶ç«‹å³è§¦å‘")
            # æ¸…é›¶ç­‰å¾…æ—¶é—´ï¼Œä¸‹æ¬¡è°ƒåº¦ç«‹å³è§¦å‘
            self._next_notify_time = 0

    def __get_time(self):
        return int(time.time())
    
    def __get_available_storages(self) -> List[Dict[str, str]]:
        """
        ä»MPç³»ç»Ÿè·å–å¯ç”¨çš„å­˜å‚¨åˆ—è¡¨
        """
        try:
            from app.helper.storage import StorageHelper
            from app.db.systemconfig_oper import SystemConfigOper
            from app.schemas.types import SystemConfigKey
            
            # ç›´æ¥ä»æ•°æ®åº“è¯»å–å­˜å‚¨é…ç½®
            storage_confs = SystemConfigOper().get(SystemConfigKey.Storages)
            if storage_confs:
                storage_list = []
                for storage in storage_confs:
                    storage_type = storage.get("type", "")
                    storage_name = storage.get("name", storage_type)
                    if storage_type:
                        storage_list.append({
                            "title": storage_name,
                            "value": storage_type
                        })
                logger.info(f"mhnotify: æˆåŠŸè·å–å­˜å‚¨åˆ—è¡¨ï¼Œå…± {len(storage_list)} ä¸ª")
                return storage_list
            logger.debug("mhnotify: æœªé…ç½®å­˜å‚¨ï¼Œä½¿ç”¨é»˜è®¤åˆ—è¡¨")
        except Exception as e:
            logger.error(f"mhnotify: è·å–å­˜å‚¨åˆ—è¡¨å¼‚å¸¸: {e}")
        
        # è¿”å›é»˜è®¤å­˜å‚¨åˆ—è¡¨
        return [
            {"title": "æœ¬åœ°", "value": "local"},
            {"title": "115ç½‘ç›˜", "value": "u115"},
            {"title": "é˜¿é‡Œäº‘ç›˜", "value": "alipan"},
            {"title": "RClone", "value": "rclone"},
            {"title": "OpenList", "value": "alist"}
        ]

    def __has_running_transfers(self) -> bool:
        """
        æ£€æµ‹æ˜¯å¦æœ‰æ­£åœ¨è¿è¡Œçš„æ•´ç†ä»»åŠ¡
        """
        try:
            from app.chain.transfer import TransferChain
            # ä¸å‰ç«¯ä¸€è‡´ï¼Œä½¿ç”¨ get_queue_tasks()
            jobs = TransferChain().get_queue_tasks()
            if not jobs:
                logger.debug("mhnotify: å½“å‰æ•´ç†é˜Ÿåˆ—ä¸ºç©º []")
                return False
            for job in jobs:
                tasks = getattr(job, 'tasks', [])
                if any((getattr(t, 'state', '') == 'running') for t in tasks):
                    logger.debug("mhnotify: å‘ç° running ä»»åŠ¡ï¼Œåˆ¤å®šä¸ºæ­£åœ¨æ•´ç†")
                    return True
            logger.debug("mhnotify: é˜Ÿåˆ—éç©ºä½†æ—  running ä»»åŠ¡ï¼Œåˆ¤å®šä¸ºä¸åœ¨æ•´ç†")
            return False
        except Exception as e:
            # è®°å½•å¼‚å¸¸å¹¶è¿”å›ä¸åœ¨æ•´ç†ï¼Œé¿å…è¯¯æŠ¥
            logger.warning(f"mhnotify: æ£€æµ‹æ•´ç†ä»»åŠ¡çŠ¶æ€å¼‚å¸¸ï¼š{e}ï¼ŒæŒ‰æ— è¿è¡Œå¤„ç†")
            return False

    def __notify_mh(self):
        try:
            # å½“æœ‰å¾…é€šçŸ¥æ—¶ï¼Œæ ¹æ®æ˜¯å¦å­˜åœ¨è¿è¡Œä¸­æ•´ç†ä»»åŠ¡å†³å®šç«‹å³è§¦å‘æˆ–è¿›å…¥ç­‰å¾…çª—å£
            now_ts = self.__get_time()
            if self._wait_notify_count > 0:
                # è‹¥å¯ç”¨ 115 ç”Ÿæ´»äº‹ä»¶ç›‘å¬ï¼Œåˆ™å…ˆæ£€æŸ¥ç”Ÿæ´»äº‹ä»¶é™é»˜çª—å£
                if self._p115_life_enabled and self._p115_next_notify_time:
                    if now_ts < self._p115_next_notify_time:
                        logger.info(f"115 ç”Ÿæ´»äº‹ä»¶é™é»˜çª—å£æœªåˆ°æœŸï¼ˆ{self._p115_next_notify_time - now_ts}sï¼‰ï¼Œæš‚ä¸è§¦å‘é€šçŸ¥")
                        return
                    else:
                        # åˆ°æœŸåæ¸…é›¶çª—å£
                        self._p115_next_notify_time = 0
                
                # è‹¥å¯ç”¨ MP äº‹ä»¶è§¦å‘ï¼Œæ£€æŸ¥ MP äº‹ä»¶ç­‰å¾…çª—å£
                if self._mp_event_enabled and self._next_notify_time:
                    if now_ts < self._next_notify_time:
                        # å¦‚æœä»æœ‰è¿è¡Œä¸­çš„æ•´ç†ä»»åŠ¡ï¼Œå»¶é•¿ç­‰å¾…æ—¶é—´
                        if self.__has_running_transfers():
                            wait_seconds = self._mp_event_wait_minutes * 60
                            self._next_notify_time = now_ts + wait_seconds
                            logger.info(f"MPæ•´ç†ä»»åŠ¡ä»åœ¨è¿è¡Œï¼Œå»¶é•¿ç­‰å¾…çª—å£ {self._mp_event_wait_minutes} åˆ†é’Ÿ")
                        else:
                            logger.info(f"MPäº‹ä»¶ç­‰å¾…çª—å£æœªåˆ°æœŸï¼ˆ{self._next_notify_time - now_ts}sï¼‰ï¼Œæš‚ä¸è§¦å‘é€šçŸ¥")
                        return
                    else:
                        # åˆ°æœŸåæ¸…é›¶çª—å£
                        self._next_notify_time = 0
                # ç™»å½•è·å– access_token
                login_url = f"{self._mh_domain}/api/v1/auth/login"
                login_payload = {
                    "username": self._mh_username,
                    "password": self._mh_password
                }
                headers = {
                    "Accept": "application/json, text/plain, */*",
                    "Content-Type": "application/json;charset=UTF-8",
                    "Origin": self._mh_domain,
                    "Accept-Language": "zh-CN",
                    "User-Agent": "MoviePilot/Plugin MHNotify"
                }
                login_res = RequestUtils(headers=headers).post(login_url, json=login_payload)
                if not login_res or login_res.status_code != 200:
                    logger.error(f"MediaHelper ç™»å½•å¤±è´¥ï¼š{getattr(login_res, 'status_code', 'N/A')} - {getattr(login_res, 'text', '')}")
                    return
                try:
                    login_data = login_res.json()
                    access_token = (login_data or {}).get("data", {}).get("access_token")
                except Exception:
                    access_token = None
                if not access_token:
                    logger.error("MediaHelper ç™»å½•æˆåŠŸä½†æœªè·å–åˆ° access_token")
                    return
                # è·å–ä»»åŠ¡åˆ—è¡¨å¹¶ç­›é€‰ strm ä»»åŠ¡
                tasks_url = f"{self._mh_domain}/api/v1/scheduled/tasks"
                list_headers = {
                    "Accept": "application/json, text/plain, */*",
                    "Authorization": f"Bearer {access_token}",
                    "User-Agent": "MoviePilot/Plugin MHNotify",
                    "Accept-Language": "zh-CN"
                }
                list_res = RequestUtils(headers=list_headers).get_res(tasks_url)
                if not list_res or list_res.status_code != 200:
                    logger.error(f"è·å– MediaHelper ä»»åŠ¡åˆ—è¡¨å¤±è´¥ï¼š{getattr(list_res, 'status_code', 'N/A')} - {getattr(list_res, 'text', '')}")
                try:
                    list_data = list_res.json() or {}
                    tasks = list_data.get("data", [])
                except Exception:
                    tasks = []
                # è¿‡æ»¤ cloud_strm_sync ä»»åŠ¡
                strm_tasks = [t for t in tasks if t.get('task') == 'cloud_strm_sync' and t.get('enabled')]
                selected_uuids = []
                name_filters = []
                if self._mh_job_names:
                    name_filters = [n.strip() for n in self._mh_job_names.split(',') if n.strip()]
                if name_filters:
                    selected_uuids = [t.get('uuid') for t in strm_tasks if (t.get('name') or '') in name_filters]
                else:
                    selected_uuids = [t.get('uuid') for t in strm_tasks if '115ç½‘ç›˜' in (t.get('name') or '')]
                if not selected_uuids:
                    logger.warning("æœªæ‰¾åˆ°å¯æ‰§è¡Œçš„ strm ä»»åŠ¡ï¼ˆcloud_strm_syncï¼‰ï¼Œè¯·æ£€æŸ¥ä»»åŠ¡åç§°æˆ–åœ¨é…ç½®ä¸­å¡«å†™ä»»åŠ¡UUIDåˆ—è¡¨")
                    return
                # é€ä¸ªè§¦å‘ï¼Œé—´éš”5ç§’
                exec_headers = {
                    "Accept": "application/json, text/plain, */*",
                    "Content-Type": "application/json;charset=UTF-8",
                    "Authorization": f"Bearer {access_token}",
                    "Origin": self._mh_domain,
                    "Accept-Language": "zh-CN",
                    "User-Agent": "MoviePilot/Plugin MHNotify"
                }
                for uuid in selected_uuids:
                    exec_url = f"{self._mh_domain}/api/v1/scheduled/execute/{uuid}"
                    exec_res = RequestUtils(headers=exec_headers).post(exec_url, json={})
                    if exec_res and exec_res.status_code in (200, 204):
                        logger.info(f"å·²è§¦å‘ MediaHelper è®¡åˆ’ä»»åŠ¡ï¼š{uuid}")
                        success_any = True
                    elif exec_res is not None:
                        logger.error(f"è§¦å‘ä»»åŠ¡å¤±è´¥ï¼š{uuid} - {exec_res.status_code} - {exec_res.text}")
                    else:
                        logger.error(f"è§¦å‘ä»»åŠ¡å¤±è´¥ï¼š{uuid} - æœªè·å–åˆ°è¿”å›ä¿¡æ¯")
                    time.sleep(5)
                if success_any:
                    self._wait_notify_count = 0
            else:
                if self._wait_notify_count > 0:
                    logger.info(
                        f"ç­‰å¾…é€šçŸ¥æ•°é‡ï¼š{self._wait_notify_count}ï¼Œæœ€åäº‹ä»¶æ—¶é—´ï¼š{self._last_event_time}")
        except Exception as e:
            logger.error(f"é€šçŸ¥MediaHelperå‘ç”Ÿå¼‚å¸¸ï¼š{e}")

    def stop_service(self):
        """
        é€€å‡ºæ’ä»¶
        """
        pass

    def hdhive_refresh_job(self):
        """å®šæ—¶ä»»åŠ¡å…¥å£ï¼šHDHiveèµ„æºåˆ·æ–°"""
        try:
            logger.info("mhnotify: å®šæ—¶HDHiveèµ„æºåˆ·æ–°è§¦å‘")
            self._execute_hdhive_refresh(subscription_name=None)
        except Exception:
            logger.error("mhnotify: å®šæ—¶HDHiveèµ„æºåˆ·æ–°å¼‚å¸¸", exc_info=True)

    def _execute_hdhive_refresh(self, subscription_name: Optional[str] = None, channel: Optional[str] = None, userid: Optional[Union[str, int]] = None):
        """
        æ‰§è¡ŒHDHiveèµ„æºåˆ·æ–°ï¼š
        - è¯»å–MHè®¢é˜…åˆ—è¡¨
        - ä»…å¤„ç† cloud_type=drive115 çš„è®¢é˜…
        - æŸ¥è¯¢HDHiveå…è´¹115é“¾æ¥ï¼Œå’Œ user_custom_links æ¯”è¾ƒå¹¶è¿½åŠ æ–°å¢
        - æ›´æ–°è®¢é˜…å¹¶è§¦å‘ç«‹å³æ‰§è¡ŒæŸ¥è¯¢
        """
        try:
            access_token = self.__mh_login()
            if not access_token:
                logger.error("mhnotify: HDHiveåˆ·æ–°å¤±è´¥ï¼Œæ— æ³•ç™»å½•MH")
                if channel:
                    self.post_message(channel=channel, title="âŒ HDHiveèµ„æºåˆ·æ–°å¤±è´¥", text="ç™»å½•MediaHelperå¤±è´¥", userid=userid, mtype=NotificationType.Plugin)
                return
            if subscription_name:
                try:
                    page_n = int(self._hdhive_max_subscriptions or 20)
                    if page_n <= 0:
                        page_n = 20
                except Exception:
                    page_n = 20
                lst = self.__mh_list_subscriptions(access_token, status="active", search=subscription_name, page_size=page_n)
            else:
                lst = self.__mh_list_subscriptions(access_token, status="active", page_size=2000)
            subs = (lst.get("data") or {}).get("subscriptions") or []
            subs = [x for x in subs if (x.get("enabled", True) is not False)]
            if not subs:
                if subscription_name:
                    logger.info(f"mhnotify: æœªæ‰¾åˆ°æŒ‡å®šè®¢é˜…: {subscription_name}")
                    if channel:
                        self.post_message(channel=channel, title="æœªæ‰¾åˆ°ç›¸åº”è®¢é˜…", text=f"åç§°: {subscription_name}", userid=userid, mtype=NotificationType.Plugin)
                    else:
                        self.post_message(title="æœªæ‰¾åˆ°ç›¸åº”è®¢é˜…", text=f"åç§°: {subscription_name}", mtype=NotificationType.Plugin)
                else:
                    logger.info("mhnotify: MHè®¢é˜…åˆ—è¡¨ä¸ºç©ºï¼Œè·³è¿‡åˆ·æ–°")
                    if channel:
                        self.post_message(channel=channel, title="â„¹ï¸ HDHiveèµ„æºåˆ·æ–°", text="MHè®¢é˜…åˆ—è¡¨ä¸ºç©º", userid=userid, mtype=NotificationType.Plugin)
                    else:
                        self.post_message(title="â„¹ï¸ HDHiveèµ„æºåˆ·æ–°", text="MHè®¢é˜…åˆ—è¡¨ä¸ºç©º", mtype=NotificationType.Plugin)
                return
            # ç›®æ ‡ç­›é€‰
            targets = []
            if subscription_name:
                name_norm = subscription_name.strip().lower()
                matched = []
                for rec in subs:
                    params = rec.get("params") or {}
                    rec_name = (rec.get("name") or rec.get("task", {}).get("name") or params.get("custom_name") or params.get("title") or "").strip().lower()
                    if rec_name and (rec_name == name_norm or name_norm in rec_name):
                        matched.append(rec)
                if not matched:
                    logger.info(f"mhnotify: æœªæ‰¾åˆ°æŒ‡å®šè®¢é˜…: {subscription_name}")
                    if channel:
                        self.post_message(channel=channel, title="æœªæ‰¾åˆ°ç›¸åº”è®¢é˜…", text=f"åç§°: {subscription_name}", userid=userid, mtype=NotificationType.Plugin)
                    else:
                        self.post_message(title="æœªæ‰¾åˆ°ç›¸åº”è®¢é˜…", text=f"åç§°: {subscription_name}", mtype=NotificationType.Plugin)
                    return
                targets = [x for x in matched if (x.get("params") or {}).get("cloud_type", "").lower() == "drive115"]
                if not targets:
                    logger.info(f"mhnotify: æŒ‡å®šè®¢é˜…é115ç±»å‹ï¼Œè·³è¿‡åˆ·æ–°: {subscription_name}")
                    if channel:
                        self.post_message(channel=channel, title="æœªæ‰¾åˆ°ç›¸åº”è®¢é˜…", text=f"åç§°: {subscription_name}", userid=userid, mtype=NotificationType.Plugin)
                    else:
                        self.post_message(title="æœªæ‰¾åˆ°ç›¸åº”è®¢é˜…", text=f"åç§°: {subscription_name}", mtype=NotificationType.Plugin)
                    return
            else:
                targets = [x for x in subs if (x.get("params") or {}).get("cloud_type", "").lower() == "drive115"]
                try:
                    if isinstance(self._hdhive_max_subscriptions, int) and self._hdhive_max_subscriptions > 0:
                        targets = targets[:self._hdhive_max_subscriptions]
                except Exception:
                    pass
            updated = 0
            checked = 0
            updated_names: List[str] = []
            for rec in targets:
                try:
                    params = rec.get("params") or {}
                    cloud_type = (params.get("cloud_type") or "").lower()
                    if cloud_type != "drive115":
                        continue
                    tmdb_id = params.get("tmdb_id")
                    media_type = (params.get("media_type") or "movie").lower()
                    existing_links: List[str] = params.get("user_custom_links") or []
                    links = self.__fetch_hdhive_links(tmdb_id=tmdb_id, media_type=media_type)
                    checked += 1
                    if not links:
                        continue
                    # å»é‡åˆå¹¶
                    def extract_key(link: str) -> str:
                        key = link.replace("https://", "").replace("http://", "").rstrip("& ").lower()
                        return key
                    keys = set(extract_key(l) for l in existing_links)
                    merged = list(existing_links)
                    for l in links:
                        k = extract_key(l)
                        if k not in keys:
                            merged.append(l)
                            keys.add(k)
                    if len(merged) > len(existing_links):
                        uuid = rec.get("uuid") or rec.get("task", {}).get("uuid")
                        name = rec.get("name") or rec.get("task", {}).get("name") or (params.get("custom_name") or params.get("title"))
                        # æå– custom_nameï¼šä¼˜å…ˆå·²æœ‰ï¼Œå…¶æ¬¡ä» name å»æ‰å‰ç¼€ [è®¢é˜…]
                        def _extract_custom_name(n: Optional[str]) -> Optional[str]:
                            if not n:
                                return None
                            return re.sub(r'^\[è®¢é˜…\]\s*', '', n).strip()
                        custom_name = params.get("custom_name") or _extract_custom_name(name) or params.get("title")
                        try:
                            payload = {"user_custom_links": merged}
                            if custom_name:
                                payload["custom_name"] = custom_name
                            upd = self.__mh_update_subscription(access_token, uuid, payload)
                            if upd:
                                updated += 1
                                updated_names.append(str(name or uuid))
                                # è§¦å‘ç«‹å³æ‰§è¡Œ
                                try:
                                    self.__mh_execute_subscription(access_token, uuid)
                                except Exception:
                                    logger.warning(f"mhnotify: è§¦å‘è®¢é˜…ç«‹å³æ‰§è¡Œå¤±è´¥ uuid={uuid}")
                                try:
                                    new_count = len(merged) - len(existing_links)
                                    if new_count > 0:
                                        msg_text = f"è®¢é˜…: {str(name or uuid)}\næ–°å¢é“¾æ¥: {new_count}"
                                        if channel:
                                            self.post_message(channel=channel, title="HDHiveèµ„æºæ›´æ–°å¹¶å·²æ‰§è¡Œ", text=msg_text, userid=userid, mtype=NotificationType.Plugin)
                                        else:
                                            self.post_message(title="HDHiveèµ„æºæ›´æ–°å¹¶å·²æ‰§è¡Œ", text=msg_text, mtype=NotificationType.Plugin)
                                except Exception:
                                    pass
                        except Exception:
                            logger.warning(f"mhnotify: æ›´æ–°è®¢é˜…é“¾æ¥å¤±è´¥ uuid={uuid}", exc_info=True)
                except Exception:
                    logger.debug("mhnotify: åˆ·æ–°å•æ¡è®¢é˜…å¼‚å¸¸", exc_info=True)
                    continue
            # é€šçŸ¥
            if channel:
                if subscription_name:
                    if updated > 0:
                        self.post_message(channel=channel, title="âœ… HDHiveèµ„æºåˆ·æ–°å®Œæˆ", text=f"è®¢é˜…: {subscription_name}\næ–°å¢é“¾æ¥: {updated}", userid=userid, mtype=NotificationType.Plugin)
                    else:
                        self.post_message(channel=channel, title="æœªæ‰¾åˆ°æ–°çš„èµ„æºé“¾æ¥", text=f"è®¢é˜…: {subscription_name}", userid=userid, mtype=NotificationType.Plugin)
                else:
                    text = f"æ£€æŸ¥è®¢é˜…: {checked}\næœ‰æ›´æ–°: {updated}"
                    if updated_names:
                        text += f"\næ›´æ–°è®¢é˜…: {', '.join(updated_names[:10])}" + (" ..." if len(updated_names) > 10 else "")
                    self.post_message(channel=channel, title="âœ… HDHiveèµ„æºåˆ·æ–°å®Œæˆ", text=text, userid=userid, mtype=NotificationType.Plugin)
            else:
                # æ²¡æœ‰æŒ‡å®šæ¶ˆæ¯æ¸ é“ï¼Œèµ°æ’ä»¶æ¶ˆæ¯ï¼ˆç³»ç»Ÿé˜Ÿåˆ—+é€šçŸ¥æ¨¡å—æŒ‰å¼€å…³åˆ†å‘ï¼‰
                if subscription_name:
                    if updated > 0:
                        self.post_message(title="âœ… HDHiveèµ„æºåˆ·æ–°å®Œæˆ", text=f"è®¢é˜…: {subscription_name}\næ–°å¢é“¾æ¥: {updated}", mtype=NotificationType.Plugin)
                    else:
                        self.post_message(title="æœªæ‰¾åˆ°æ–°çš„èµ„æºé“¾æ¥", text=f"è®¢é˜…: {subscription_name}", mtype=NotificationType.Plugin)
                else:
                    text = f"æ£€æŸ¥è®¢é˜…: {checked}\næœ‰æ›´æ–°: {updated}"
                    if updated_names:
                        text += f"\næ›´æ–°è®¢é˜…: {', '.join(updated_names[:10])}" + (" ..." if len(updated_names) > 10 else "")
                    self.post_message(title="âœ… HDHiveèµ„æºåˆ·æ–°å®Œæˆ", text=text, mtype=NotificationType.Plugin)
        except Exception:
            logger.error("mhnotify: æ‰§è¡ŒHDHiveèµ„æºåˆ·æ–°å¼‚å¸¸", exc_info=True)

    @eventmanager.register(EventType.PluginAction)
    def handle_hdhive_refresh(self, event: Event):
        """è¿œç¨‹å‘½ä»¤è§¦å‘ï¼šHDHiveèµ„æºåˆ·æ–°ï¼ˆå¯æŒ‡å®šè®¢é˜…åç§°ï¼‰"""
        if not event:
            return
        event_data = event.event_data
        if not event_data or event_data.get("action") != "mh_hdhive_refresh":
            return
        arg_raw = (event_data.get("arg_str") or "").strip()
        names = [x.strip() for x in arg_raw.split(",") if x.strip()] if arg_raw else []
        # å¼‚æ­¥æ‰§è¡Œï¼Œé¿å…é˜»å¡æ¶ˆæ¯é€šé“
        try:
            import threading
            limit_val = 0
            try:
                limit_val = int(self._hdhive_max_subscriptions or 0)
            except Exception:
                limit_val = 0
            if names:
                start_text = "åˆ·æ–°æŒ‡å®šè®¢é˜…: " + ", ".join(names)
            else:
                start_text = f"åˆ·æ–°å‰{limit_val}ä¸ª115è®¢é˜…" if limit_val > 0 else "åˆ·æ–°æ‰€æœ‰115è®¢é˜…"
            self.post_message(
                channel=event_data.get("channel"),
                title="â³ HDHiveèµ„æºåˆ·æ–°å¼€å§‹",
                text=start_text,
                userid=event_data.get("user"),
                mtype=NotificationType.Plugin
            )
            if names:
                threading.Thread(
                    target=self._execute_hdhive_refresh_multi,
                    kwargs={"names": names, "channel": event_data.get("channel"), "userid": event_data.get("user")},
                    daemon=True
                ).start()
            else:
                threading.Thread(
                    target=self._execute_hdhive_refresh,
                    kwargs={"subscription_name": None, "channel": event_data.get("channel"), "userid": event_data.get("user")},
                    daemon=True
                ).start()
        except Exception:
            logger.warning("mhnotify: å¯åŠ¨HDHiveèµ„æºåˆ·æ–°åå°çº¿ç¨‹å¤±è´¥ï¼Œæ”¹ä¸ºåŒæ­¥æ‰§è¡Œ")
            if names:
                self._execute_hdhive_refresh_multi(names=names, channel=event_data.get("channel"), userid=event_data.get("user"))
            else:
                self._execute_hdhive_refresh(subscription_name=None, channel=event_data.get("channel"), userid=event_data.get("user"))
    def _execute_hdhive_refresh_multi(self, names: List[str], channel: Optional[str] = None, userid: Optional[Union[str, int]] = None):
        try:
            for nm in names:
                self._execute_hdhive_refresh(subscription_name=nm, channel=channel, userid=userid)
        except Exception:
            logger.error("mhnotify: æ‰§è¡Œå¤šè®¢é˜…HDHiveèµ„æºåˆ·æ–°å¼‚å¸¸", exc_info=True)
    def __watch_115_life(self):
        """ç›‘å¬ 115 ç”Ÿæ´»äº‹ä»¶ï¼Œæ»¡è¶³ç­›é€‰æ—¶è§¦å‘å¾…é€šçŸ¥è®¡æ•°"""
        try:
            if not self._p115_life_enabled:
                return
            cookie = (self._p115_cookie or "").strip()
            if not cookie:
                return
            # è¯»å–ä¸Šæ¬¡æŒ‡é’ˆ
            last_ts = int(self.get_data(self._P115_LAST_TS_KEY) or 0)
            last_id_raw = self.get_data(self._P115_LAST_ID_KEY)
            try:
                last_id = int(last_id_raw) if last_id_raw is not None else 0
            except Exception:
                last_id = 0
            
            # é¦–æ¬¡å¯ç”¨æ—¶ï¼Œä»å½“å‰æ—¶é—´å¼€å§‹ç›‘å¬ï¼Œé¿å…æ‹‰å–æ‰€æœ‰å†å²äº‹ä»¶
            if last_ts == 0:
                current_ts = int(time.time())
                logger.info(f"mhnotify: 115ç”Ÿæ´»äº‹ä»¶é¦–æ¬¡å¯ç”¨ï¼Œä»å½“å‰æ—¶é—´å¼€å§‹ç›‘å¬ (ts={current_ts})")
                self.save_data(self._P115_LAST_TS_KEY, current_ts)
                self.save_data(self._P115_LAST_ID_KEY, 0)
                return

            # ä¼˜å…ˆä½¿ç”¨ p115client çš„ life APIï¼ˆä¸ p115strmhelper ä¿æŒä¸€è‡´ï¼‰
            try:
                from p115client import P115Client  # type: ignore
                from p115client.tool.life import iter_life_behavior_once, life_show  # type: ignore
                client = P115Client(cookie, app="web")
                # ç¡®è®¤ç”Ÿæ´»äº‹ä»¶å·²å¼€å¯
                try:
                    resp = life_show(client)
                    if not (isinstance(resp, dict) and resp.get("state")):
                        logger.warning("mhnotify: 115 ç”Ÿæ´»äº‹ä»¶æœªå¼€å¯æˆ–è·å–å¤±è´¥ï¼Œè·³è¿‡æœ¬è½®")
                        return
                except Exception:
                    # life_show å¤±è´¥ä¸è‡´å‘½ï¼Œç»§ç»­å°è¯•æ‹‰å–
                    pass

                # æ‹‰å–ä¸€æ¬¡ï¼ˆä»ä¸Šæ¬¡æŒ‡é’ˆå¼€å§‹ï¼‰
                events_iter = iter_life_behavior_once(
                    client=client,
                    from_time=last_ts,
                    from_id=last_id,
                    app="web",
                    cooldown=1,
                )
                # æ”¶é›†åˆ°å†…å­˜ï¼ˆé™åˆ¶ä¸€å®šæ•°é‡é¿å…è¿‡å¤§ï¼‰
                events: List[Dict[str, Any]] = []
                max_collect = 200
                for idx, ev in enumerate(events_iter):
                    if idx >= max_collect:
                        break
                    events.append(ev)

                if not events:
                    return

                # å°†äº‹ä»¶ç±»å‹æ˜ å°„åˆ°ç®€åŒ–ç±»åˆ«ï¼Œä¾› UI é€‰æ‹©åŒ¹é…
                def map_type_to_simple(t: int) -> str:
                    """
                    115ç”Ÿæ´»äº‹ä»¶ç±»å‹æ˜ å°„ï¼ˆå‚è€ƒ p115strmhelperï¼‰
                    å·²çŸ¥ç±»å‹ï¼š
                    - type 1,2 â†’ upload (ä¸Šä¼ )
                    - type 5,6 â†’ move (ç§»åŠ¨)
                    - type 14 â†’ receive (æ¥æ”¶)
                    - type 17 â†’ create (æ–°å»º)
                    - type 18 â†’ copy (å¤åˆ¶)
                    - type 22 â†’ delete (åˆ é™¤)
                    å¦‚é‡æœªæ˜ å°„ç±»å‹ï¼Œå°†åœ¨æ—¥å¿—ä¸­è®°å½•è­¦å‘Š
                    """
                    if t in (1, 2):
                        return "upload"
                    if t in (5, 6):
                        return "move"
                    if t == 14:
                        return "receive"
                    if t == 17:
                        return "create"
                    if t == 18:
                        return "copy"
                    if t == 22:
                        return "delete"
                    return ""

                selected = set([x.lower() for x in (self._p115_events or [])])
                def _match_rules(full_path: str, ev_simple: str) -> bool:
                    rules = self._p115_watch_rules or []
                    if not rules:
                        return False
                    try:
                        for r in rules:
                            rp = (r.get('path') or '').strip()
                            evs = [str(x).strip().lower() for x in (r.get('events') or [])]
                            if not rp:
                                continue
                            if full_path.startswith(rp + '/') or full_path == rp:
                                if not evs:
                                    return True
                                return bool(ev_simple) and (ev_simple in evs)
                        return False
                    except Exception:
                        return False
                has_new = False
                new_last_ts = last_ts
                new_last_id = last_id
                triggered_events = []  # æ”¶é›†è§¦å‘çš„äº‹ä»¶ä¿¡æ¯
                
                # åªå¤„ç†æœ€è¿‘10åˆ†é’Ÿå†…çš„äº‹ä»¶
                current_time = int(time.time())
                time_window = 10 * 60  # 10åˆ†é’Ÿ
                cutoff_time = current_time - time_window
                
                # p115strmhelper åœ¨ once_pull ä¸­æœ€ç»ˆä»¥æœ€æ–°äº‹ä»¶æ›´æ–°æŒ‡é’ˆï¼›è¿™é‡ŒæŒ‰æ—¶é—´/IDå–æœ€å¤§
                for it in events:
                    try:
                        t = int(it.get("type", 0))
                        ut = int(it.get("update_time", 0))
                        eid = int(it.get("id", 0))
                        pid = int(it.get("parent_id", 0))
                        fname = str(it.get("file_name", "") or "")
                    except Exception:
                        continue
                    # è·³è¿‡æ—§äº‹ä»¶
                    if ut < last_ts or (ut == last_ts and eid <= last_id):
                        continue
                    
                    # è·³è¿‡è¶…è¿‡10åˆ†é’Ÿçš„æ—§äº‹ä»¶
                    if ut < cutoff_time:
                        logger.debug(f"mhnotify: è·³è¿‡10åˆ†é’Ÿå‰çš„æ—§äº‹ä»¶: {fname}, æ—¶é—´: {ut}")
                        # æ›´æ–°æŒ‡é’ˆä½†ä¸è§¦å‘
                        if ut > new_last_ts or (ut == new_last_ts and eid > new_last_id):
                            new_last_ts = ut
                            new_last_id = eid
                        continue
                    
                    # è¾“å‡ºåŸå§‹äº‹ä»¶æ•°æ®ç”¨äºè°ƒè¯•ï¼ˆä»…è®°å½•æ–°äº‹ä»¶ï¼‰
                    logger.debug(f"mhnotify: 115ç”Ÿæ´»äº‹ä»¶åŸå§‹æ•°æ® type={t}, id={eid}, file={fname}, parent_id={pid}, update_time={ut}, å®Œæ•´æ•°æ®={it}")
                    
                    simple = map_type_to_simple(t)
                    # å¦‚æœäº‹ä»¶ç±»å‹æœªèƒ½æ˜ å°„ï¼Œè®°å½•è­¦å‘Š
                    if not simple:
                        logger.warning(f"mhnotify: 115ç”Ÿæ´»äº‹ä»¶æœªæ˜ å°„ç±»å‹ type={t}, file={fname}, åŸå§‹æ•°æ®={it}")
                    
                    # ç±»å‹åŒ¹é…
                    type_ok = (not selected) or (simple and simple in selected)
                    dir_ok = True
                    full_path = ""
                    # ç›®å½•äº‹ä»¶è§„åˆ™ä¼˜å…ˆï¼ˆè‹¥é…ç½®äº†ï¼‰
                    if type_ok and (self._p115_watch_rules or self._p115_watch_dirs):
                        try:
                            full_dir = self._p115_dir_cache.get(pid)
                            if not full_dir:
                                from p115client.tool.attr import get_path  # type: ignore
                                full_dir = get_path(client=client, attr=pid, root_id=None) or ''
                                if full_dir.startswith('æ ¹ç›®å½•'):
                                    full_dir = full_dir[3:]
                                full_dir = full_dir.replace('\\', '/').strip()
                                if not full_dir.startswith('/'):
                                    full_dir = '/' + full_dir
                                full_dir = full_dir.rstrip('/')
                                self._p115_dir_cache[pid] = full_dir
                            full_path = (full_dir + '/' + fname).replace('\\', '/')
                            if self._p115_watch_rules:
                                dir_ok = _match_rules(full_path=full_path, ev_simple=simple)
                            elif self._p115_watch_dirs:
                                dir_ok = any(full_path.startswith(d + '/') or full_path == d for d in self._p115_watch_dirs)
                        except Exception:
                            dir_ok = False
                    if type_ok and dir_ok:
                        has_new = True
                        # è®°å½•è§¦å‘çš„äº‹ä»¶è¯¦æƒ…
                        event_name_map = {
                            "upload": "ä¸Šä¼ ",
                            "move": "ç§»åŠ¨",
                            "receive": "æ¥æ”¶",
                            "create": "æ–°å»º",
                            "copy": "å¤åˆ¶",
                            "delete": "åˆ é™¤"
                        }
                        event_name = event_name_map.get(simple, simple or f"type_{t}")
                        triggered_events.append({"path": full_path or fname, "event": event_name, "type": t, "time": ut})
                    if ut > new_last_ts or (ut == new_last_ts and eid > new_last_id):
                        new_last_ts = ut
                        new_last_id = eid

                if has_new:
                    self._wait_notify_count += 1
                    self._last_event_time = int(time.time())
                    # è¾“å‡ºè¯¦ç»†çš„è§¦å‘ä¿¡æ¯ï¼ˆåŒ…å«äº‹ä»¶å‘ç”Ÿæ—¶é—´ï¼‰
                    from datetime import datetime
                    for evt in triggered_events:
                        evt_time = datetime.fromtimestamp(evt.get('time', 0)).strftime('%Y-%m-%d %H:%M:%S') if evt.get('time') else 'æœªçŸ¥'
                        logger.info(f"mhnotify: 115ç”Ÿæ´»äº‹ä»¶è§¦å‘ - ç›®å½•: {evt['path']} | äº‹ä»¶: {evt['event']} | å‘ç”Ÿæ—¶é—´: {evt_time}")
                    logger.info(f"mhnotify: 115ç”Ÿæ´»äº‹ä»¶è§¦å‘ï¼ˆp115client.lifeï¼‰ï¼Œå…± {len(triggered_events)} ä¸ªäº‹ä»¶ï¼Œè®¡å…¥ä¸€æ¬¡strmè§¦å‘ä¿¡å·")
                    # è®¾ç½®/å»¶é•¿ç”Ÿæ´»äº‹ä»¶é™é»˜çª—å£
                    try:
                        delay_seconds = max(int(self._p115_wait_minutes) * 60, 0)
                    except Exception:
                        delay_seconds = 300
                    self._p115_next_notify_time = int(time.time()) + delay_seconds

                # ä¿å­˜æŒ‡é’ˆ
                if new_last_ts:
                    self.save_data(self._P115_LAST_TS_KEY, int(new_last_ts))
                if new_last_id:
                    self.save_data(self._P115_LAST_ID_KEY, int(new_last_id))
                return
            except Exception:
                # è‹¥ p115client ä¸å¯ç”¨æˆ–å¼‚å¸¸ï¼Œé€€å›åˆ°ç®€æ˜“ HTTP æ–¹æ¡ˆ
                pass

            # å›é€€ï¼šHTTP æ–¹æ¡ˆï¼ˆå…¼å®¹æ€§è¾ƒå·®ï¼Œä»…ä½œä¸ºå…œåº•ï¼‰
            headers = {
                "Accept": "application/json, text/plain, */*",
                "Cookie": cookie,
                "User-Agent": "MoviePilot/Plugin MHNotify",
                "Referer": "https://115.com/"
            }
            candidate_urls = [
                "https://webapi.115.com/life/events?limit=50",
                "https://webapi.115.com/files/new?aid=1&cid=0&show_dir=1&offset=0&limit=50",
            ]
            hit_url = None
            items: List[Dict[str, Any]] = []
            for url in candidate_urls:
                try:
                    res = RequestUtils(headers=headers, timeout=20).get_res(url)
                    if not res or res.status_code != 200:
                        continue
                    data = res.json()
                    if "events" in data:
                        items = data.get("events") or []
                    elif "data" in data and isinstance(data.get("data"), dict) and ("list" in data["data"]):
                        items = data.get("data", {}).get("list", [])
                    elif "list" in data:
                        items = data.get("list") or []
                    else:
                        items = []
                    hit_url = url
                    if items:
                        break
                except Exception:
                    continue
            if not items:
                return

            def normalize_event_name(item: Dict[str, Any]) -> str:
                name = (item.get("action") or item.get("event") or item.get("type") or "").lower()
                text = (item.get("action_text") or item.get("event_text") or item.get("name") or "").lower()
                m = {
                    "ä¸Šä¼ ": "upload", "upload": "upload",
                    "ç§»åŠ¨": "move", "move": "move",
                    "æ¥æ”¶": "receive", "receive": "receive",
                    "æ–°å»º": "create", "åˆ›å»º": "create", "create": "create",
                    "å¤åˆ¶": "copy", "copy": "copy",
                    "åˆ é™¤": "delete", "ç§»åˆ°å›æ”¶ç«™": "delete", "delete": "delete",
                }
                for k, v in m.items():
                    if k in name or k in text:
                        return v
                return name or text or ""

            def extract_ts(item: Dict[str, Any]) -> int:
                for key in ("update_time", "utime", "time", "ctime", "created_time"):
                    val = item.get(key)
                    if isinstance(val, (int, float)):
                        return int(val)
                    if isinstance(val, str) and val.isdigit():
                        return int(val)
                return 0

            def extract_id(item: Dict[str, Any]) -> int:
                for key in ("id", "eid", "event_id"):
                    val = item.get(key)
                    if val is not None and str(val).isdigit():
                        return int(val)
                return 0

            selected = set([x.lower() for x in (self._p115_events or [])])
            def _match_rules(full_path: str, ev_simple: str) -> bool:
                rules = self._p115_watch_rules or []
                if not rules:
                    return False
                try:
                    for r in rules:
                        rp = (r.get('path') or '').strip()
                        evs = [str(x).strip().lower() for x in (r.get('events') or [])]
                        if not rp:
                            continue
                        if full_path.startswith(rp + '/') or full_path == rp:
                            if not evs:
                                return True
                            return bool(ev_simple) and (ev_simple in evs)
                    return False
                except Exception:
                    return False
            has_new = False
            new_last_ts = last_ts
            new_last_id = last_id
            triggered_events = []  # æ”¶é›†è§¦å‘çš„äº‹ä»¶ä¿¡æ¯
            for it in items:
                # è¾“å‡ºåŸå§‹äº‹ä»¶æ•°æ®ç”¨äºè°ƒè¯•
                logger.debug(f"mhnotify: 115ç”Ÿæ´»äº‹ä»¶HTTPåŸå§‹æ•°æ®={it}")
                
                ev = normalize_event_name(it)
                ts = extract_ts(it)
                eid = extract_id(it)
                if ts < last_ts or (ts == last_ts and eid <= last_id):
                    continue
                
                # å¦‚æœäº‹ä»¶ç±»å‹æœªèƒ½è¯†åˆ«ï¼Œè®°å½•è­¦å‘Š
                if not ev:
                    logger.warning(f"mhnotify: 115ç”Ÿæ´»äº‹ä»¶HTTPæœªè¯†åˆ«ç±»å‹ï¼ŒåŸå§‹æ•°æ®={it}")
                
                type_ok = (not selected) or (ev and ev in selected)
                dir_ok = True
                full_path = ""
                # ç›®å½•äº‹ä»¶è§„åˆ™ä¼˜å…ˆï¼ˆHTTP å…œåº•ä¸‹å°½åŠ›è·å–è·¯å¾„ï¼Œå¯èƒ½ä¸å®Œæ•´ï¼‰
                if type_ok and (self._p115_watch_rules or self._p115_watch_dirs):
                    try:
                        pid = int(it.get('parent_id') or 0)
                        fname = str(it.get('file_name') or it.get('name') or '')
                        full_dir = self._p115_dir_cache.get(pid)
                        if not full_dir:
                            full_dir = ''
                        full_path = (full_dir + '/' + fname).replace('\\', '/')
                        if self._p115_watch_rules:
                            dir_ok = _match_rules(full_path=full_path, ev_simple=ev)
                        elif self._p115_watch_dirs:
                            dir_ok = any(full_path.startswith(d + '/') or full_path == d for d in self._p115_watch_dirs)
                    except Exception:
                        dir_ok = False
                if type_ok and dir_ok:
                    has_new = True
                    # è®°å½•è§¦å‘çš„äº‹ä»¶è¯¦æƒ…
                    event_name_map = {
                        "upload": "ä¸Šä¼ ",
                        "move": "ç§»åŠ¨",
                        "receive": "æ¥æ”¶",
                        "create": "æ–°å»º",
                        "copy": "å¤åˆ¶",
                        "delete": "åˆ é™¤"
                    }
                    event_name = event_name_map.get(ev, ev or "æœªçŸ¥")
                    fname = str(it.get('file_name') or it.get('name') or '')
                    triggered_events.append({"path": full_path or fname, "event": event_name})
                if ts > new_last_ts or (ts == new_last_ts and eid > new_last_id):
                    new_last_ts = ts
                    new_last_id = eid

            if has_new:
                self._wait_notify_count += 1
                self._last_event_time = int(time.time())
                # è¾“å‡ºè¯¦ç»†çš„è§¦å‘ä¿¡æ¯
                for evt in triggered_events:
                    logger.info(f"mhnotify: 115ç”Ÿæ´»äº‹ä»¶è§¦å‘ - ç›®å½•: {evt['path']} | äº‹ä»¶: {evt['event']}")
                logger.info(f"mhnotify: 115ç”Ÿæ´»äº‹ä»¶è§¦å‘ï¼ˆ{hit_url}ï¼‰ï¼Œå…± {len(triggered_events)} ä¸ªäº‹ä»¶ï¼Œè®¡å…¥ä¸€æ¬¡strmè§¦å‘ä¿¡å·")
                try:
                    delay_seconds = max(int(self._p115_wait_minutes) * 60, 0)
                except Exception:
                    delay_seconds = 300
                self._p115_next_notify_time = int(time.time()) + delay_seconds
            if new_last_ts:
                self.save_data(self._P115_LAST_TS_KEY, int(new_last_ts))
            if new_last_id:
                self.save_data(self._P115_LAST_ID_KEY, int(new_last_id))
        except Exception:
            logger.warning("mhnotify: ç›‘å¬115ç”Ÿæ´»äº‹ä»¶å¼‚å¸¸", exc_info=True)

    @eventmanager.register(EventType.SubscribeAdded)
    def _on_subscribe_added(self, event: Event):
        """
        mhè®¢é˜…è¾…åŠ©ï¼šä»…å¯¹æ–°è®¢é˜…ç”Ÿæ•ˆ
        - æš‚åœè¯¥è®¢é˜…ï¼ˆstate='S'ï¼Œä¸æ”¹åŠ¨å·²æœ‰è®¢é˜…ï¼‰
        - ç™»å½•MHå¹¶è¯»å–é»˜è®¤é…ç½®
        - æŒ‰åª’ä½“ç±»å‹åœ¨MHåˆ›å»ºè®¢é˜…
        - è®°å½•mh_uuidå¹¶åœ¨5åˆ†é’ŸåæŸ¥è¯¢è¿›åº¦ï¼ŒæŒ‰è§„åˆ™å¤„ç†ï¼ˆåˆ é™¤æˆ–æ¢å¤MPè®¢é˜…ï¼‰
        """
        try:
            if not event or not self._mh_assist_enabled:
                return
            event_data = event.event_data or {}
            try:
                mid = (event_data.get("mediainfo") or {}).get("tmdb_id") or (event_data.get("mediainfo") or {}).get("tmdbid")
                mtitle = (event_data.get("mediainfo") or {}).get("title") or (event_data.get("mediainfo") or {}).get("name")
                mseason = (event_data.get("mediainfo") or {}).get("season")
                mdouban = (event_data.get("mediainfo") or {}).get("douban_id") or (event_data.get("mediainfo") or {}).get("doubanid")
                logger.info(f"mhnotify: SubscribeAdded äº‹ä»¶: sub_id={event_data.get('subscribe_id')} tmdb_id={mid} douban_id={mdouban} title={mtitle} event.season={mseason}")
            except Exception:
                pass
            sub_id = event_data.get("subscribe_id")
            mediainfo_dict = event_data.get("mediainfo") or {}
            if not sub_id:
                return
            # æš‚åœè¯¥è®¢é˜…ï¼Œä»…é’ˆå¯¹æ–°è®¢é˜…
            with SessionFactory() as db:
                subscribe = SubscribeOper(db=db).get(sub_id)
                if not subscribe:
                    return
                SubscribeOper(db=db).update(sub_id, {"state": "S", "sites": [-1]})
                # é‡æ–°è·å–ï¼Œç¡®ä¿å­£å·ç­‰å­—æ®µå·²æ­£ç¡®åŠ è½½
                subscribe = SubscribeOper(db=db).get(sub_id)
                try:
                    logger.info(f"mhnotify: è®¢é˜…æš‚åœå®Œæˆ id={sub_id} type={getattr(subscribe,'type',None)} season={getattr(subscribe,'season',None)}")
                except Exception:
                    pass
            # ç™»å½• MH æ‹¿ token
            access_token = self.__mh_login()
            if not access_token:
                logger.error("mhnotify: ç™»å½•MediaHelperå¤±è´¥ï¼Œæ— æ³•åˆ›å»ºè®¢é˜…")
                return
            # è¯»å–é»˜è®¤é…ç½®
            defaults = self.__mh_get_defaults(access_token)

            # æå– tmdb_id å¹¶åŠ é”ï¼Œé˜²æ­¢å¤šå­£å¹¶å‘æ·»åŠ å¯¼è‡´é‡å¤åˆ›å»º MH è®¢é˜…
            tmdb_id = getattr(subscribe, 'tmdbid', None) or mediainfo_dict.get('tmdb_id') or mediainfo_dict.get('tmdbid')

            with self._get_tmdb_lock(tmdb_id):
                # è‹¥ä¸ºå‰§é›†ï¼ŒèšåˆåŒä¸€ TMDB çš„å¤šå­£è®¢é˜…ï¼ˆç”µå½±ä¸éœ€è¦èšåˆå­£ï¼‰
                aggregate_seasons: Optional[List[int]] = None
                # åˆ¤æ–­åª’ä½“ç±»å‹
                sub_type = (getattr(subscribe, 'type', '') or '').strip().lower()
                is_tv = sub_type in ('tv', 'ç”µè§†å‰§')
                aggregate_seasons = []  # åˆå§‹åŒ–ï¼Œç”µå½±æ—¶ä¸ºç©º
                
                # åªæœ‰ç”µè§†å‰§æ‰è¿›è¡Œèšåˆå­£é€»è¾‘
                if is_tv:
                    try:
                        # å– tmdb_id (å·²åœ¨ä¸Šæ–¹æå–)
                        # tmdb_id = getattr(subscribe, 'tmdbid', None) or mediainfo_dict.get('tmdb_id') or mediainfo_dict.get('tmdbid')
                        # æŸ¥è¯¢ MP å†…ç›¸åŒ tmdb çš„è®¢é˜…ï¼Œèšåˆå­£
                        if tmdb_id:
                            logger.info(f"mhnotify: èšåˆå­£å¼€å§‹ï¼Œtmdb_id={tmdb_id}")
                            with SessionFactory() as db:
                                all_subs = SubscribeOper(db=db).list_by_tmdbid(tmdb_id)
                                logger.info(f"mhnotify: MPå†…åŒtmdbè®¢é˜…æ•°={len(all_subs or [])}")
                                seasons = []
                                for s in all_subs or []:
                                    try:
                                        stype = (getattr(s, 'type', '') or '').strip()
                                        stype_lower = (stype or '').lower()
                                        if stype_lower == 'tv' or stype in {'ç”µè§†å‰§'}:
                                            # ä¼˜å…ˆä½¿ç”¨è®¢é˜…ä¸­çš„ seasonï¼Œå…¶æ¬¡ä»æ ‡é¢˜è§£æ
                                            s_season = getattr(s, 'season', None)
                                            if s_season is None:
                                                s_season = self.__extract_season_from_text(getattr(s, 'name', '') or '')
                                            seasons.append(s_season)
                                            logger.info(f"mhnotify: è®¢é˜…èšåˆå€™é€‰ id={getattr(s,'id',None)} type={stype} season={getattr(s,'season',None)} parsed={s_season}")
                                    except Exception:
                                        pass
                            # è½¬æ¢å­£ä¸ºæ•´æ•°ï¼ˆæ”¯æŒå­—ç¬¦ä¸²æ•°å­—ï¼‰
                            for x in seasons:
                                if isinstance(x, int):
                                    aggregate_seasons.append(x)
                                elif isinstance(x, str) and x.isdigit():
                                    aggregate_seasons.append(int(x))
                            # è¿‡æ»¤æ— æ•ˆå­£å·ï¼ˆNone/0/è´Ÿæ•°ï¼‰å¹¶å»é‡æ’åº
                            aggregate_seasons = sorted({s for s in aggregate_seasons if isinstance(s, int) and s > 0})
                            logger.info(f"mhnotify: èšåˆå­£ï¼ˆè½¬æ¢åï¼‰={aggregate_seasons}")
                            if aggregate_seasons:
                                logger.info(f"mhnotify: æ£€æµ‹åˆ°è¯¥å‰§å­˜åœ¨å¤šå­£è®¢é˜…ï¼Œèšåˆå­£ï¼š{aggregate_seasons}")
                            else:
                                logger.info("mhnotify: æœªèšåˆåˆ°å­£ä¿¡æ¯ï¼Œå°†å›é€€ä½¿ç”¨äº‹ä»¶æˆ–è®¢é˜…ä¸­çš„å­£")
                    except Exception:
                        logger.warning("mhnotify: èšåˆå­£ä¿¡æ¯å¤±è´¥", exc_info=True)
                else:
                    # ç”µå½±ç±»å‹ä¸éœ€è¦èšåˆå­£
                    logger.debug(f"mhnotify: åª’ä½“ç±»å‹ä¸ºç”µå½±ï¼Œè·³è¿‡èšåˆå­£é€»è¾‘")
                # æ„å»ºåˆ›å»ºå‚æ•°ï¼ˆè‹¥ä¸ºTVå°†å¸¦å…¥èšåˆå­£ï¼‰
                create_payload = self.__build_mh_create_payload(subscribe, mediainfo_dict, defaults, aggregate_seasons=aggregate_seasons)
                if not create_payload:
                    logger.error("mhnotify: æ„å»ºMHè®¢é˜…åˆ›å»ºå‚æ•°å¤±è´¥")
                    return
                # è‹¥å·²å­˜åœ¨ç›¸åŒ tmdb_id çš„ MH è®¢é˜…ï¼Œåˆ™å¤ç”¨æˆ–é‡å»ºï¼ˆä»¥èšåˆå­£ä¸ºå‡†ï¼‰
                existing_uuid: Optional[str] = None
                existing_selected: List[int] = []
                existing_custom_links: List[str] = []  # ä¿ç•™ç°æœ‰è®¢é˜…çš„è‡ªå®šä¹‰é“¾æ¥
                try:
                    lst = self.__mh_list_subscriptions(access_token)
                    subs = (lst.get("data") or {}).get("subscriptions") or []
                    for rec in subs:
                        params = rec.get("params") or {}
                        if (params.get("cloud_type") or "").strip().lower() != "drive115":
                            continue
                        if params.get("tmdb_id") == create_payload.get("tmdb_id") and (params.get("media_type") or '').lower() == (create_payload.get("media_type") or '').lower():
                            existing_uuid = rec.get("uuid") or rec.get("task", {}).get("uuid")
                            try:
                                existing_selected = [int(x) for x in (params.get("selected_seasons") or [])]
                            except Exception:
                                existing_selected = []
                            # è·å–ç°æœ‰è®¢é˜…çš„è‡ªå®šä¹‰é“¾æ¥
                            existing_custom_links = params.get("user_custom_links") or []
                            if existing_custom_links:
                                logger.info(f"mhnotify: ç°æœ‰MHè®¢é˜…å·²æœ‰ {len(existing_custom_links)} ä¸ªè‡ªå®šä¹‰é“¾æ¥")
                            logger.info(f"mhnotify: ç°æœ‰MHè®¢é˜…å‘½ä¸­ tmdb_id={params.get('tmdb_id')} uuid={existing_uuid} seasons={existing_selected}")
                            break
                    if existing_uuid:
                        agg_set = set(create_payload.get("selected_seasons") or [])
                        exist_set = set(existing_selected or [])
                        if agg_set and agg_set != exist_set:
                            # éœ€è¦åŒ…å«æ›´å¤šå­£ï¼šä¼˜å…ˆå°è¯•æ›´æ–°è®¢é˜…å­£é›†åˆï¼›å¤±è´¥åˆ™é‡å»º
                            # æ›´æ–°æ—¶ä¿ç•™ç°æœ‰çš„è‡ªå®šä¹‰é“¾æ¥
                            if existing_custom_links:
                                create_payload["user_custom_links"] = existing_custom_links
                            logger.info(f"mhnotify: å‘ç°ç°æœ‰MHè®¢é˜… {existing_uuid}ï¼Œå­£é›†åˆä¸ä¸€è‡´ï¼Œå°è¯•æ›´æ–°ä¸º {sorted(agg_set)}")
                            upd = self.__mh_update_subscription(access_token, existing_uuid, create_payload)
                            if upd:
                                logger.info(f"mhnotify: å·²æ›´æ–°ç°æœ‰è®¢é˜… {existing_uuid} ä¸ºèšåˆå­£ {sorted(agg_set)}")
                            else:
                                logger.info(f"mhnotify: æ›´æ–°å¤±è´¥ï¼Œæ”¹ä¸ºé‡å»ºè®¢é˜…ä¸ºèšåˆå­£ {sorted(agg_set)}")
                                self.__mh_delete_subscription(access_token, existing_uuid)
                                existing_uuid = None
                        else:
                            # å®Œå…¨ä¸€è‡´ï¼šç›´æ¥å¤ç”¨ï¼ˆæŒ‰åª’ä½“ç±»å‹è¾“å‡ºæç¤ºï¼‰
                            if is_tv:
                                logger.info(f"mhnotify: å‘ç°ç°æœ‰MHè®¢é˜… {existing_uuid}ï¼Œå­£é›†åˆä¸€è‡´ï¼Œå¤ç”¨è¯¥è®¢é˜…")
                            else:
                                logger.info(f"mhnotify: å‘ç°ç°æœ‰MHè®¢é˜… {existing_uuid}ï¼Œç”µå½±ä¿¡æ¯ä¸€è‡´ï¼Œå¤ç”¨è¯¥è®¢é˜…")
                except Exception:
                    logger.warning("mhnotify: æ£€æŸ¥ç°æœ‰MHè®¢é˜…å¤±è´¥", exc_info=True)
                # HDHive æŸ¥è¯¢è‡ªå®šä¹‰é“¾æ¥
                links: List[str] = []
                try:
                    links = self.__fetch_hdhive_links(
                        tmdb_id=create_payload.get("tmdb_id"),
                        media_type=create_payload.get("media_type")
                    )
                    if links:
                        logger.info(f"mhnotify: HDHive è·å–åˆ° {len(links)} ä¸ªå…è´¹115é“¾æ¥")
                except Exception:
                    logger.error("mhnotify: HDHive æŸ¥è¯¢é“¾æ¥å¤±è´¥", exc_info=True)
                
                # åˆå¹¶ç°æœ‰è‡ªå®šä¹‰é“¾æ¥ä¸æ–°æŸ¥è¯¢çš„ HDHive é“¾æ¥ï¼ˆå»é‡ï¼‰
                merged_links: List[str] = list(existing_custom_links)  # ä¿ç•™ç°æœ‰é“¾æ¥
                if links:
                    # æå–é“¾æ¥çš„æ ¸å¿ƒæ ‡è¯†ç”¨äºå»é‡ï¼ˆå»é™¤åè®®å‰ç¼€å’Œå°¾éƒ¨å‚æ•°å·®å¼‚ï¼‰
                    def extract_link_key(link: str) -> str:
                        """æå–é“¾æ¥çš„æ ¸å¿ƒéƒ¨åˆ†ç”¨äºå»é‡æ¯”è¾ƒ"""
                        # ç§»é™¤åè®®å‰ç¼€
                        key = link.replace("https://", "").replace("http://", "")
                        # ç§»é™¤å°¾éƒ¨çš„ & æˆ– ç©ºæ ¼
                        key = key.rstrip("& ")
                        return key.lower()
                    
                    existing_keys = set(extract_link_key(l) for l in existing_custom_links)
                    new_count = 0
                    for link in links:
                        link_key = extract_link_key(link)
                        if link_key not in existing_keys:
                            merged_links.append(link)
                            existing_keys.add(link_key)
                            new_count += 1
                    if new_count > 0:
                        logger.info(f"mhnotify: åˆå¹¶åå…± {len(merged_links)} ä¸ªè‡ªå®šä¹‰é“¾æ¥ï¼ˆæ–°å¢ {new_count} ä¸ªï¼‰")
                    else:
                        logger.info(f"mhnotify: HDHive é“¾æ¥å·²å­˜åœ¨äºç°æœ‰è‡ªå®šä¹‰é“¾æ¥ä¸­ï¼Œæ— éœ€æ·»åŠ ")
                
                # è®¾ç½® create_payload çš„è‡ªå®šä¹‰é“¾æ¥ï¼ˆç”¨äºæ–°å»ºè®¢é˜…ï¼‰
                if merged_links:
                    create_payload["user_custom_links"] = merged_links
                
                # åˆ›å»ºè®¢é˜…ï¼ˆæˆ–å¤ç”¨ç°æœ‰ï¼‰
                mh_uuid = None
                if existing_uuid:
                    mh_uuid = existing_uuid
                    # å¦‚æœæœ‰æ–°çš„ HDHive é“¾æ¥éœ€è¦æ·»åŠ ï¼Œæ›´æ–°ç°æœ‰è®¢é˜…
                    if links and len(merged_links) > len(existing_custom_links):
                        try:
                            update_payload = {"user_custom_links": merged_links}
                            upd_resp = self.__mh_update_subscription(access_token, existing_uuid, update_payload)
                            if upd_resp:
                                logger.info(f"mhnotify: å·²å°†è‡ªå®šä¹‰é“¾æ¥æ›´æ–°åˆ°ç°æœ‰è®¢é˜… {existing_uuid}ï¼ˆå…± {len(merged_links)} ä¸ªï¼‰")
                            else:
                                logger.warning(f"mhnotify: æ›´æ–°ç°æœ‰è®¢é˜…çš„è‡ªå®šä¹‰é“¾æ¥å¤±è´¥")
                        except Exception as e:
                            logger.warning(f"mhnotify: æ›´æ–°ç°æœ‰è®¢é˜…çš„è‡ªå®šä¹‰é“¾æ¥å¼‚å¸¸: {e}")
                else:
                    resp = self.__mh_create_subscription(access_token, create_payload)
                    mh_uuid = (resp or {}).get("data", {}).get("subscription_id") or (resp or {}).get("data", {}).get("task", {}).get("uuid")
                if not mh_uuid:
                    logger.error(f"mhnotify: MHè®¢é˜…åˆ›å»ºå¤±è´¥ï¼š{resp}")
                    return
                # ä¸è°ƒåº¦ä¿æŒä¸€è‡´ï¼šé¦–æ¬¡æŸ¥è¯¢å»¶è¿Ÿï¼ˆé»˜è®¤2åˆ†é’Ÿï¼‰
                delay_mins = max(1, int(self._assist_initial_delay_seconds / 60))
                if existing_uuid:
                    logger.info(f"mhnotify: å¤ç”¨ç°æœ‰MHè®¢é˜…ï¼Œuuid={mh_uuid}ï¼›{delay_mins}åˆ†é’ŸåæŸ¥è¯¢è¿›åº¦")
                    # å¤ç”¨ç°æœ‰è®¢é˜…æ—¶ï¼Œè§¦å‘ç«‹å³æ‰§è¡ŒæŸ¥è¯¢ï¼ˆMHä¸ä¼šè‡ªåŠ¨è§¦å‘ï¼‰
                    try:
                        access_token = self.__mh_login()
                        if access_token and self.__mh_execute_subscription(access_token, mh_uuid):
                            logger.info(f"mhnotify: å·²è§¦å‘å¤ç”¨è®¢é˜… {mh_uuid} ç«‹å³æ‰§è¡ŒæŸ¥è¯¢")
                        else:
                            logger.warning(f"mhnotify: è§¦å‘å¤ç”¨è®¢é˜…æ‰§è¡Œå¤±è´¥")
                    except Exception as e:
                        logger.warning(f"mhnotify: è§¦å‘å¤ç”¨è®¢é˜…æ‰§è¡Œå¼‚å¸¸: {e}")
                else:
                    logger.info(f"mhnotify: å·²åœ¨MHåˆ›å»ºè®¢é˜…ï¼Œuuid={mh_uuid}ï¼›{delay_mins}åˆ†é’ŸåæŸ¥è¯¢è¿›åº¦")
            
            # è®°å½•å¾…æ£€æŸ¥é¡¹
            pending: Dict[str, dict] = self.get_data(self._ASSIST_PENDING_KEY) or {}
            pending[str(sub_id)] = {
                "mh_uuid": mh_uuid,
                "created_at": int(time.time()),
                "type": (create_payload.get("media_type") or mediainfo_dict.get("type") or "movie"),
                "douban_id": (mediainfo_dict.get("douban_id") or mediainfo_dict.get("doubanid") or getattr(subscribe, 'doubanid', None))
            }
            self.save_data(self._ASSIST_PENDING_KEY, pending)
            try:
                import threading
                def _delayed_check():
                    try:
                        p = self.get_data(self._ASSIST_PENDING_KEY) or {}
                        info = p.get(str(sub_id))
                        if not info:
                            return
                        info["created_at"] = int(time.time()) - self._assist_initial_delay_seconds - 1
                        p[str(sub_id)] = info
                        self.save_data(self._ASSIST_PENDING_KEY, p)
                        self.__assist_scheduler()
                    except Exception:
                        logger.warning("mhnotify: æ–°è®¢é˜…å»¶è¿Ÿæ£€æŸ¥å¼‚å¸¸", exc_info=True)
                threading.Timer(self._assist_initial_delay_seconds, _delayed_check).start()
                logger.info(f"mhnotify: å·²å®‰æ’æ–°è®¢é˜…åœ¨ {int(self._assist_initial_delay_seconds/60)} åˆ†é’Ÿåç«‹å³æ£€æŸ¥è¿›åº¦ï¼ˆä¸å—å…¨å±€è°ƒåº¦é¢‘ç‡å½±å“ï¼‰")
            except Exception:
                logger.warning("mhnotify: å®‰æ’æ–°è®¢é˜…å»¶è¿Ÿæ£€æŸ¥å¤±è´¥", exc_info=True)
        except Exception as e:
            logger.error(f"mhnotify: å¤„ç†æ–°å¢è®¢é˜…äº‹ä»¶å¤±è´¥: {e}")

    # æ—§å±è”½é€»è¾‘ç§»é™¤

    # æ—§å±è”½é€»è¾‘ç§»é™¤

    def _get_tmdb_lock(self, tmdb_id: Union[int, str]) -> threading.Lock:
        """è·å–æŒ‡å®štmdb_idçš„é”"""
        if not tmdb_id:
            return self._locks_lock
        try:
            tid = int(tmdb_id)
        except:
            return self._locks_lock
            
        with self._locks_lock:
            if tid not in self._tmdb_locks:
                self._tmdb_locks[tid] = threading.Lock()
            return self._tmdb_locks[tid]

    def __mh_login(self) -> Optional[str]:
        """ç™»å½• MH è·å– access_token"""
        try:
            # ä½¿ç”¨ç¼“å­˜tokenï¼Œé¿å…æ¯åˆ†é’Ÿé‡å¤ç™»å½•
            now_ts = int(time.time())
            if self._mh_token and now_ts < self._mh_token_expire_ts:
                logger.debug("mhnotify: ä½¿ç”¨ç¼“å­˜çš„MH access_token")
                return self._mh_token
            logger.info(f"mhnotify: å‡†å¤‡ç™»å½•MHï¼Œdomain={self._mh_domain}, username={self._mh_username}")
            if not self._mh_domain or not self._mh_username or not self._mh_password:
                logger.error("mhnotify: ç™»å½•MHå¤±è´¥ï¼Œç¼ºå°‘åŸŸåæˆ–ç”¨æˆ·åæˆ–å¯†ç é…ç½®")
                return None
            login_url = f"{self._mh_domain}/api/v1/auth/login"
            payload = {"username": self._mh_username, "password": self._mh_password}
            headers = {
                "Accept": "application/json, text/plain, */*",
                "Content-Type": "application/json;charset=UTF-8",
                "Origin": self._mh_domain,
                "Accept-Language": "zh-CN",
                "User-Agent": "MoviePilot/Plugin MHNotify"
            }
            res = RequestUtils(headers=headers).post(login_url, json=payload)
            if res is None:
                logger.error("mhnotify: ç™»å½•MHæœªè·å–åˆ°ä»»ä½•å“åº”")
            else:
                logger.info(f"mhnotify: ç™»å½•MHå“åº” status={res.status_code}")
            if not res or res.status_code != 200:
                return None
            data = res.json() or {}
            token = (data.get("data") or {}).get("access_token")
            logger.info(f"mhnotify: ç™»å½•MHæˆåŠŸï¼Œaccess_tokenè·å–={'yes' if token else 'no'}")
            if token:
                # å†™å…¥ç¼“å­˜
                self._mh_token = token
                self._mh_token_expire_ts = now_ts + max(60, self._mh_token_ttl_seconds)
            return token
        except Exception:
            logger.error("mhnotify: ç™»å½•MHå‡ºç°å¼‚å¸¸", exc_info=True)
            return None

    def __auth_headers(self, access_token: str) -> Dict[str, str]:
        return {
            "Accept": "application/json, text/plain, */*",
            "Authorization": f"Bearer {access_token}",
            "User-Agent": "MoviePilot/Plugin MHNotify",
            "Accept-Language": "zh-CN"
        }

    def __mh_get_defaults(self, access_token: str) -> Dict[str, Any]:
        try:
            url = f"{self._mh_domain}/api/v1/subscription/config/defaults"
            logger.info(f"mhnotify: è·å–MHé»˜è®¤é…ç½® GET {url}")
            res = RequestUtils(headers=self.__auth_headers(access_token)).get_res(url)
            if res is None:
                logger.error("mhnotify: è·å–MHé»˜è®¤é…ç½®æœªè¿”å›å“åº”")
            elif res.status_code != 200:
                logger.error(f"mhnotify: è·å–MHé»˜è®¤é…ç½®å¤±è´¥ status={res.status_code} body={getattr(res, 'text', '')[:200]}")
            else:
                data = res.json() or {}
                core = (data or {}).get("data") or {}
                logger.info(
                    "mhnotify: é»˜è®¤é…ç½®æ‘˜è¦ cloud_type=%s account=%s target_directory=%s quality_preference=%s",
                    core.get("cloud_type"), core.get("account_identifier"), core.get("target_directory"), core.get("quality_preference")
                )
                return data
        except Exception:
            logger.error("mhnotify: è·å–MHé»˜è®¤é…ç½®å¼‚å¸¸", exc_info=True)
            pass
        return {}

    def __normalize_media_type(self, sub_type: Optional[str], info_type: Optional[str]) -> str:
        try:
            st = (sub_type or "").strip().lower()
            it = (info_type or "").strip().lower() if isinstance(info_type, str) else ""
            movie_alias = {"movie", "mov", "å½±ç‰‡", "ç”µå½±"}
            tv_alias = {"tv", "television", "ç”µè§†å‰§", "å‰§é›†", "series"}
            if st in movie_alias or it in movie_alias:
                return "movie"
            if st in tv_alias or it in tv_alias:
                return "tv"
            # å…œåº•ï¼šä¼˜å…ˆæŒ‰ info_typeï¼Œå…¶æ¬¡æŒ‰ sub_type
            if it in {"movie", "tv"}:
                return it
            return "movie"
        except Exception:
            return "movie"

    def __build_mh_create_payload(self, subscribe, mediainfo_dict: Dict[str, Any], defaults: Dict[str, Any], aggregate_seasons: Optional[List[int]] = None) -> Optional[Dict[str, Any]]:
        try:
            data = (defaults or {}).get("data") or {}
            quality_pref = data.get("quality_preference") or "auto"
            target_dir = data.get("target_directory") or "/å½±è§†"
            logger.info(f"m1hnotify: ç›®æ ‡ç›®å½•: {target_dir}")
            cron = data.get("cron") or "0 */6 * * *"
            cloud_type = data.get("cloud_type") or "drive115"
            account_identifier = data.get("account_identifier") or ""
            # å–è®¢é˜…å­—æ®µï¼ˆå…¼å®¹å¯¹è±¡æˆ–å­—å…¸ï¼‰
            def _get(field: str):
                try:
                    if hasattr(subscribe, field):
                        return getattr(subscribe, field)
                    if isinstance(subscribe, dict):
                        return subscribe.get(field)
                except Exception:
                    return None
                return None
            # åª’ä½“ä¿¡æ¯
            tmdb_id = _get('tmdbid') or mediainfo_dict.get('tmdb_id') or mediainfo_dict.get('tmdbid')
            title = _get('name') or mediainfo_dict.get('title')
            sub_type = _get('type')
            info_type = mediainfo_dict.get('type')
            mtype_norm = self.__normalize_media_type(sub_type, info_type)
            release_date = mediainfo_dict.get('release_date')
            overview = mediainfo_dict.get('overview')
            poster_path = mediainfo_dict.get('poster_path')
            vote_average = mediainfo_dict.get('vote_average')
            search_keywords = _get('keyword') or mediainfo_dict.get('search_keywords') or title
            if not title:
                title = mediainfo_dict.get('original_title') or mediainfo_dict.get('name') or "æœªçŸ¥æ ‡é¢˜"
            payload: Dict[str, Any] = {
                "tmdb_id": tmdb_id,
                "title": title,
                "original_title": mediainfo_dict.get('original_title'),
                "media_type": mtype_norm,
                "release_date": release_date,
                "overview": overview,
                "poster_path": poster_path,
                "vote_average": vote_average,
                "search_keywords": search_keywords,
                "quality_preference": quality_pref,
                "target_directory": target_dir,
                "target_dir_id": "",
                "target_path": "",
                "cron": cron,
                "cloud_type": cloud_type,
                "account_identifier": account_identifier,
                "custom_name": title,
                "user_custom_links": []
            }
            if payload["media_type"] == "tv":
                logger.info(f"mhnotify: è§£æå­£ä¿¡æ¯: event.season={mediainfo_dict.get('season')} subscribe.season={_get('season')}")
                # èšåˆå­£ä¿¡æ¯ï¼šè‹¥æä¾› aggregate_seasonsï¼Œåˆ™ä½¿ç”¨å…¶ä½œä¸ºè®¢é˜…çš„å­£é›†åˆ
                if aggregate_seasons:
                    # å»é‡å¹¶æ’åº
                    seasons = sorted({int(s) for s in aggregate_seasons if s is not None}) or [1]
                    src = "èšåˆ"
                else:
                    # ä»äº‹ä»¶æˆ–è®¢é˜…ä¸­è§£æå­£å·ï¼ˆæ”¯æŒå­—ç¬¦ä¸²æ•°å­—ï¼‰ï¼›å¤±è´¥åˆ™ä»æ ‡é¢˜è§£æï¼›ä»å¤±è´¥åˆ™é»˜è®¤1
                    raw_season = mediainfo_dict.get('season') or _get('season')
                    def _to_int(v):
                        if isinstance(v, int):
                            return v
                        if isinstance(v, str) and v.isdigit():
                            return int(v)
                        return None
                    season_num = _to_int(raw_season)
                    src = "äº‹ä»¶/è®¢é˜…"
                    if not season_num:
                        season_num = self.__extract_season_from_text(title or '')
                        src = "æ ‡é¢˜è§£æ" if season_num else "é»˜è®¤1"
                    season_num = season_num or 1
                    seasons = [season_num]
                payload["selected_seasons"] = seasons
                payload["episode_ranges"] = {str(s): {"min_episode": None, "max_episode": None, "exclude_episodes": [], "exclude_text": ""} for s in seasons}
                logger.info(f"mhnotify: TVè®¢é˜…å­£é€‰å®š: {seasons}; æ¥æº={src}")
            else:
                payload["selected_seasons"] = []
            # æ—¥å¿—æ‘˜è¦
            logger.info(
                "mhnotify: æ„å»ºMHè®¢é˜…åˆ›å»ºå‚æ•° tmdb_id=%s title=%s media_type=%s target_dir=%s cloud_type=%s account=%s",
                payload.get("tmdb_id"), payload.get("title"), payload.get("media_type"), target_dir, cloud_type, account_identifier
            )
            return payload
        except Exception:
            logger.error("mhnotify: __build_mh_create_payload å¼‚å¸¸ï¼Œsubscribeæˆ–mediainfoç¼ºå¤±å…³é”®å­—æ®µ")
            return None

    def __extract_season_from_text(self, text: str) -> Optional[int]:
        """ä»æ ‡é¢˜/æ–‡æœ¬ä¸­è§£æå­£å·ï¼Œæ”¯æŒä¸­æ–‡ä¸è‹±æ–‡å¸¸è§æ ¼å¼
        ä¾‹ï¼š"ç¬¬äºŒå­£"ã€"ç¬¬2å­£"ã€"Season 2"ã€"S02"ã€"2å­£"ã€"ç¬¬åå­£"ã€"ç¬¬åä¸€å­£"
        è¿”å›æ­£æ•´æ•°ï¼›æ— æ³•è§£æè¿”å› None
        """
        if not text:
            return None
        try:
            t = text.strip()
            # è‹±æ–‡æ ¼å¼ Season X / SXX
            m = re.search(r"(?:Season\s*)(\d{1,2})", t, re.IGNORECASE)
            if m:
                return int(m.group(1))
            m = re.search(r"\bS(\d{1,2})\b", t, re.IGNORECASE)
            if m:
                return int(m.group(1))
            # ä¸­æ–‡æ ¼å¼ ç¬¬Xå­£ / Xå­£
            m = re.search(r"ç¬¬([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾é›¶ã€‡ä¸¤\d]{1,3})å­£", t)
            if m:
                num = m.group(1)
                return self.__parse_chinese_numeral(num)
            m = re.search(r"([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾é›¶ã€‡ä¸¤\d]{1,3})å­£", t)
            if m:
                num = m.group(1)
                return self.__parse_chinese_numeral(num)
            # å…¶å®ƒï¼šç¬¬XæœŸ/éƒ¨ æœ‰æ—¶ä¹ŸæŒ‡å­£ï¼ˆå°½é‡è§£æä½†ä¸å¼ºåˆ¶ä½¿ç”¨ï¼‰
            m = re.search(r"ç¬¬([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾é›¶ã€‡ä¸¤\d]{1,3})(?:æœŸ|éƒ¨)", t)
            if m:
                num = m.group(1)
                val = self.__parse_chinese_numeral(num)
                return val if val and val > 0 else None
        except Exception:
            pass
        return None

    def __parse_chinese_numeral(self, s: str) -> Optional[int]:
        """è§£æä¸­æ–‡æ•°å­—åˆ°æ•´æ•°ï¼Œæ”¯æŒåˆ° 99 å·¦å³ï¼›ä¹Ÿæ”¯æŒçº¯æ•°å­—å­—ç¬¦ä¸²"""
        if not s:
            return None
        try:
            if s.isdigit():
                return int(s)
            mapping = {
                'é›¶': 0, 'ã€‡': 0,
                'ä¸€': 1, 'äºŒ': 2, 'ä¸¤': 2, 'ä¸‰': 3, 'å››': 4, 'äº”': 5,
                'å…­': 6, 'ä¸ƒ': 7, 'å…«': 8, 'ä¹': 9,
                'å': 10
            }
            total = 0
            # å¤„ç†åƒ "åä¸€"ã€"äºŒå"ã€"äºŒåä¸€"
            if 'å' in s:
                parts = s.split('å')
                if parts[0] == '':
                    total += 10
                else:
                    total += mapping.get(parts[0], 0) * 10
                if len(parts) > 1 and parts[1] != '':
                    total += mapping.get(parts[1], 0)
                return total if total > 0 else None
            # å•å­—æ•°å­—
            return mapping.get(s, None)
        except Exception:
            return None

    def __mh_create_subscription(self, access_token: str, payload: Dict[str, Any]) -> Dict[str, Any]:
        try:
            url = f"{self._mh_domain}/api/v1/subscription/create"
            headers = self.__auth_headers(access_token)
            headers.update({"Content-Type": "application/json;charset=UTF-8", "Origin": self._mh_domain})
            logger.info(f"mhnotify: åˆ›å»ºMHè®¢é˜… POST {url} media_type={payload.get('media_type')} tmdb_id={payload.get('tmdb_id')} title={str(payload.get('title'))[:50]}")
            # å¢åŠ æ˜¾å¼è¶…æ—¶ä¸å°æ¬¡æ•°é‡è¯•ï¼Œç¼“è§£ç¬æ—¶ç½‘ç»œæŠ–åŠ¨
            timeout_seconds = 30
            max_retries = 2  # æ€»å…±å°è¯• 1+2 æ¬¡
            for attempt in range(1, max_retries + 2):
                res = RequestUtils(headers=headers, timeout=timeout_seconds).post(url, json=payload)
                if res is None:
                    logger.error(f"mhnotify: åˆ›å»ºMHè®¢é˜…æœªè¿”å›å“åº”ï¼ˆç¬¬{attempt}æ¬¡ï¼Œå¯èƒ½è¶…æ—¶{timeout_seconds}sï¼‰")
                elif res.status_code not in (200, 204):
                    body_text = getattr(res, 'text', '')
                    logger.error(f"mhnotify: åˆ›å»ºMHè®¢é˜…å¤±è´¥ï¼ˆç¬¬{attempt}æ¬¡ï¼‰ status={res.status_code} body={body_text[:200]}")
                    # å¦‚æœå·²å­˜åœ¨ç›¸åŒé…ç½®çš„è®¢é˜…ï¼Œå°è¯•æŸ¥è¯¢å¹¶å¤ç”¨
                    try:
                        if res.status_code == 400 and ('å·²å­˜åœ¨ç›¸åŒé…ç½®' in body_text or 'already exists' in body_text.lower()):
                            lst = self.__mh_list_subscriptions(access_token)
                            subs = (lst.get("data") or {}).get("subscriptions") or []
                            cand_uuid = None
                            want_tmdb = payload.get('tmdb_id')
                            want_type = (payload.get('media_type') or '').lower()
                            want_seasons = set(payload.get('selected_seasons') or [])
                            for rec in subs:
                                params = rec.get('params') or {}
                                if params.get('tmdb_id') == want_tmdb and (params.get('media_type') or '').lower() == want_type:
                                    try:
                                        cur_seasons = set(int(x) for x in (params.get('selected_seasons') or []))
                                    except Exception:
                                        cur_seasons = set()
                                    if not want_seasons or cur_seasons == want_seasons:
                                        cand_uuid = rec.get('uuid') or rec.get('task', {}).get('uuid')
                                        break
                            if cand_uuid:
                                logger.info(f"mhnotify: å¤ç”¨å·²å­˜åœ¨çš„MHè®¢é˜… uuid={cand_uuid}")
                                return {"data": {"subscription_id": cand_uuid, "task": {"uuid": cand_uuid}}}
                    except Exception:
                        logger.warning("mhnotify: æ£€ç´¢å·²å­˜åœ¨çš„MHè®¢é˜…å¤±è´¥", exc_info=True)
                else:
                    data = res.json() or {}
                    uuid = (data.get("data") or {}).get("subscription_id") or (data.get("data") or {}).get("task", {}).get("uuid")
                    logger.info(f"mhnotify: åˆ›å»ºMHè®¢é˜…æˆåŠŸ uuid={uuid}")
                    return data
                # è¿˜æœ‰é‡è¯•æ¬¡æ•°æ—¶ï¼Œè¿›è¡ŒæŒ‡æ•°çº§çŸ­æš‚åœé¡¿
                if attempt <= max_retries:
                    time.sleep(2 * attempt)
        except Exception:
            logger.error("mhnotify: åˆ›å»ºMHè®¢é˜…å¼‚å¸¸", exc_info=True)
            pass
        return {}

    def __mh_list_subscriptions(self, access_token: str, status: Optional[str] = None, search: Optional[str] = None, page_size: int = 2000) -> Dict[str, Any]:
        try:
            url = f"{self._mh_domain}/api/v1/subscription/list?page=1&page_size={page_size}"
            if status:
                url += f"&status={status}"
            if search:
                try:
                    url += f"&search={quote(search)}"
                except Exception:
                    url += f"&search={search}"
            logger.info(f"mhnotify: æŸ¥è¯¢MHè®¢é˜…åˆ—è¡¨ GET {url}")
            res = RequestUtils(headers=self.__auth_headers(access_token)).get_res(url)
            if res is None:
                logger.error("mhnotify: æŸ¥è¯¢MHè®¢é˜…åˆ—è¡¨æœªè¿”å›å“åº”")
            elif res.status_code != 200:
                logger.error(f"mhnotify: æŸ¥è¯¢MHè®¢é˜…åˆ—è¡¨å¤±è´¥ status={res.status_code} body={getattr(res, 'text', '')[:200]}")
            else:
                data = res.json() or {}
                subs = (data.get("data") or {}).get("subscriptions") or []
                logger.info(f"mhnotify: æŸ¥è¯¢MHè®¢é˜…åˆ—è¡¨æˆåŠŸ count={len(subs)}")
                return data
        except Exception:
            logger.error("mhnotify: æŸ¥è¯¢MHè®¢é˜…åˆ—è¡¨å¼‚å¸¸", exc_info=True)
            pass
        return {}

    def __mh_delete_subscription(self, access_token: str, uuid: str) -> bool:
        try:
            url = f"{self._mh_domain}/api/v1/subscription/{uuid}"
            headers = self.__auth_headers(access_token)
            headers.update({"Origin": self._mh_domain})
            logger.info(f"mhnotify: åˆ é™¤MHè®¢é˜… DELETE {url}")
            res = RequestUtils(headers=headers).delete_res(url)
            ok = bool(res and res.status_code in (200, 204))
            if res is None:
                logger.error("mhnotify: åˆ é™¤MHè®¢é˜…æœªè¿”å›å“åº”")
            else:
                logger.info(f"mhnotify: åˆ é™¤MHè®¢é˜…å“åº” status={res.status_code} ok={ok}")
            return ok
        except Exception:
            logger.error("mhnotify: åˆ é™¤MHè®¢é˜…å¼‚å¸¸", exc_info=True)
            return False

    def __mh_delete_by_title(self, access_token: str, title: str) -> int:
        try:
            if not title:
                return 0
            lst = self.__mh_list_subscriptions(access_token, status="active", search=title, page_size=2000)
            subs = (lst.get("data") or {}).get("subscriptions") or []
            count = 0
            t_norm = str(title).strip().lower()
            for rec in subs:
                params = rec.get("params") or {}
                cloud = str(params.get("cloud_type") or "").strip().lower()
                name = (rec.get("name") or rec.get("task", {}).get("name") or params.get("title") or "").strip().lower()
                if cloud == "drive115" and name and name == t_norm:
                    uuid = rec.get("uuid") or rec.get("task", {}).get("uuid")
                    if uuid and self.__mh_delete_subscription(access_token, uuid):
                        count += 1
            logger.info(f"mhnotify: æŒ‰æ ‡é¢˜åˆ é™¤MHè®¢é˜… title={title} å·²åˆ é™¤æ•°é‡={count}")
            return count
        except Exception:
            logger.error("mhnotify: æŒ‰æ ‡é¢˜åˆ é™¤MHè®¢é˜…å¼‚å¸¸", exc_info=True)
            return 0

    def __mh_delete_by_tmdb(self, access_token: str, tmdb_id: Union[str, int], media_type: Optional[str] = None, season: Optional[int] = None) -> int:
        try:
            if not tmdb_id:
                return 0
            
            # ä½¿ç”¨ TMDB ID é”ï¼Œé˜²æ­¢å¹¶å‘æ“ä½œåŒä¸€è®¢é˜…
            with self._get_tmdb_lock(tmdb_id):
                lst = self.__mh_list_subscriptions(access_token, status="active", page_size=2000)
                subs = (lst.get("data") or {}).get("subscriptions") or []
                count = 0
                tmdb_norm = str(tmdb_id)
                mtype_norm = (str(media_type or "").lower().strip() or None)
                for rec in subs:
                    params = rec.get("params") or {}
                    cloud = str(params.get("cloud_type") or "").strip().lower()
                    ptmdb = str(params.get("tmdb_id") or "")
                    pmtype = str(params.get("media_type") or "").lower().strip()
                    if cloud == "drive115" and ptmdb == tmdb_norm and (not mtype_norm or pmtype == mtype_norm):
                        uuid = rec.get("uuid") or rec.get("task", {}).get("uuid")
                        if not uuid:
                            continue
                        
                        # æ£€æŸ¥æ˜¯å¦æŒ‡å®šäº†åˆ é™¤ç‰¹å®šå­£
                        if season is not None:
                            # è·å–MHè®¢é˜…å½“å‰çš„å­£åˆ—è¡¨
                            current_seasons = params.get("selected_seasons") or []
                            # è½¬æ¢ä¸ºintåˆ—è¡¨ä»¥ä¾¿æ¯”è¾ƒ
                            try:
                                current_seasons_int = [int(s) for s in current_seasons]
                            except:
                                current_seasons_int = []
                            
                            target_season = int(season)
                            
                            if target_season in current_seasons_int:
                                # å¦‚æœåŒ…å«è¯¥å­£ï¼Œåˆ™ç§»é™¤
                                new_seasons = [s for s in current_seasons_int if s != target_season]
                                
                                if not new_seasons:
                                    # å¦‚æœç§»é™¤åä¸ºç©ºï¼Œåˆ™åˆ é™¤æ•´ä¸ªè®¢é˜…
                                    logger.info(f"mhnotify: è®¢é˜… {rec.get('name')} ç§»é™¤å­£ {target_season} åä¸ºç©ºï¼Œæ‰§è¡Œåˆ é™¤")
                                    if self.__mh_delete_subscription(access_token, uuid):
                                        count += 1
                                else:
                                    # å¦‚æœè¿˜æœ‰å…¶ä»–å­£ï¼Œåˆ™æ›´æ–°è®¢é˜…
                                    logger.info(f"mhnotify: è®¢é˜… {rec.get('name')} ç§»é™¤å­£ {target_season}ï¼Œå‰©ä½™ {new_seasons}")
                                    update_payload = {"selected_seasons": new_seasons}
                                    self.__mh_update_subscription(access_token, uuid, update_payload)
                                    # è¿™é‡Œä¸ç®—ä½œåˆ é™¤æ•°é‡ï¼Œä½†å·²ç»å¤„ç†äº†
                            else:
                                logger.info(f"mhnotify: è®¢é˜… {rec.get('name')} ä¸åŒ…å«å­£ {target_season}ï¼Œè·³è¿‡å¤„ç†")
                        else:
                            if self.__mh_delete_subscription(access_token, uuid):
                                count += 1
                logger.info(f"mhnotify: æŒ‰tmdb_idåˆ é™¤MHè®¢é˜… tmdb_id={tmdb_norm} type={mtype_norm or '*'} season={season} å·²åˆ é™¤æ•°é‡={count}")
                return count
        except Exception:
            logger.error("mhnotify: æŒ‰tmdb_idåˆ é™¤MHè®¢é˜…å¼‚å¸¸", exc_info=True)
            return 0

    def __mh_update_subscription(self, access_token: str, uuid: str, payload: Dict[str, Any]) -> Dict[str, Any]:
        """æ›´æ–°MHè®¢é˜…ï¼ˆä¿®æ”¹å­£é›†åˆç­‰å‚æ•°ï¼‰
        å…¼å®¹ç¤ºä¾‹ï¼šPUT /api/v1/subscription/{uuid}ï¼Œbody åŒ…å« name/cron/params
        params ä¸­åŒ…å« selected_seasons ä¸ episode_ranges ä»¥åŠå…¶ä»–å­—æ®µ
        """
        try:
            url = f"{self._mh_domain}/api/v1/subscription/{uuid}"
            headers = self.__auth_headers(access_token)
            headers.update({"Content-Type": "application/json;charset=UTF-8", "Origin": self._mh_domain})
            # ç»„è£…æ›´æ–°ä½“ï¼šä»…æ›´æ–°æä¾›çš„å­—æ®µï¼Œé¿å…è¦†ç›–å·²æœ‰ name/cron
            update_body: Dict[str, Any] = {
                "params": payload
            }
            # ä»…å½“æ˜¾å¼ä¼ å…¥ name æˆ– title æ—¶æ‰æ›´æ–° name
            if payload.get("name") or payload.get("title"):
                update_body["name"] = payload.get("name") or f"[è®¢é˜…] {payload.get('title')}"
            # ä»…å½“æ˜¾å¼ä¼ å…¥ cron æ—¶æ‰æ›´æ–° cron
            if "cron" in payload:
                update_body["cron"] = payload.get("cron")
            logger.info(f"mhnotify: æ›´æ–°MHè®¢é˜… PUT {url} seasons={payload.get('selected_seasons')}")
            res = RequestUtils(headers=headers, timeout=30).put_res(url, json=update_body)
            if res is None:
                logger.error("mhnotify: æ›´æ–°MHè®¢é˜…æœªè¿”å›å“åº”")
            elif res.status_code not in (200, 204):
                logger.error(f"mhnotify: æ›´æ–°MHè®¢é˜…å¤±è´¥ status={res.status_code} body={getattr(res, 'text', '')[:200]}")
            else:
                data = res.json() or {}
                logger.info("mhnotify: æ›´æ–°MHè®¢é˜…æˆåŠŸ")
                return data
        except Exception:
            logger.error("mhnotify: æ›´æ–°MHè®¢é˜…å¼‚å¸¸", exc_info=True)
        return {}

    def __mh_execute_subscription(self, access_token: str, uuid: str) -> bool:
        """è§¦å‘MHè®¢é˜…ç«‹å³æ‰§è¡ŒæŸ¥è¯¢
        POST /api/v1/subscription/{uuid}/execute
        """
        try:
            url = f"{self._mh_domain}/api/v1/subscription/{uuid}/execute"
            headers = self.__auth_headers(access_token)
            headers.update({"Content-Length": "0", "Origin": self._mh_domain})
            logger.info(f"mhnotify: è§¦å‘MHè®¢é˜…æ‰§è¡Œ POST {url}")
            res = RequestUtils(headers=headers, timeout=30).post_res(url)
            if res is None:
                logger.error("mhnotify: è§¦å‘MHè®¢é˜…æ‰§è¡Œæœªè¿”å›å“åº”")
                return False
            elif res.status_code != 200:
                logger.error(f"mhnotify: è§¦å‘MHè®¢é˜…æ‰§è¡Œå¤±è´¥ status={res.status_code} body={getattr(res, 'text', '')[:200]}")
                return False
            else:
                data = res.json() or {}
                logger.info(f"mhnotify: è§¦å‘MHè®¢é˜…æ‰§è¡ŒæˆåŠŸï¼š{data.get('message', '')}")
                return True
        except Exception:
            logger.error("mhnotify: è§¦å‘MHè®¢é˜…æ‰§è¡Œå¼‚å¸¸", exc_info=True)
        return False

    def __btl_get_video_detail(self, access_token: str, douban_id: Union[str, int]) -> List[Dict[str, Any]]:
        try:
            base = "https://web5.mukaku.com/prod/api/v1/getVideoDetail"
            params = {
                "id": str(douban_id),
                "app_id": "83768d9ad4",
                "identity": "23734adac0301bccdcb107c4aa21f96c"
            }
            logger.info(f"111mhnotify: è·å–BTLè§†é¢‘è¯¦æƒ… GET {base} id={douban_id} app_id={params['app_id']} identity={params['identity'][:8]}***")
            import requests, time
            seeds = []
            for i in range(3):
                try:
                    resp = requests.get(base, params=params, headers={
                        "Accept": "application/json, text/plain, */*",
                        "Accept-Language": "zh_CN",
                        "Content-Type": "application/json;charset=UTF-8",
                        "Referer": "https://web5.mukaku.com/search",
                        "User-Agent": "Mozilla/5.0"
                    }, timeout=20)
                    try:
                        text = resp.text or ""
                        logger.info(f"mhnotify: BTLè¯¦æƒ…å“åº” status={resp.status_code} len={len(text)}")
                        logger.info(f"mhnotify: BTLè¯¦æƒ…å“åº”å†…å®¹: {text[:1000]}")
                    except Exception:
                        pass
                    if resp.status_code != 200:
                        time.sleep(1 + i)
                        continue
                    data = resp.json() or {}
                    if not isinstance(data, dict) or str(data.get("code")) != "200":
                        logger.info(f"mhnotify: BTLè¯¦æƒ…è¿”å›é”™è¯¯ code={data.get('code')} message={data.get('message')}")
                        time.sleep(1 + i)
                        continue
                    core = data.get("data") if isinstance(data.get("data"), dict) else {}
                    if core and "all_seeds" in core and isinstance(core["all_seeds"], list):
                        seeds = core["all_seeds"]
                        logger.info(f"mhnotify: BTLè¯¦æƒ…ä½¿ç”¨ all_seeds èµ„æºæ¡æ•°={len(seeds)}")
                    else:
                        seeds = (core.get("resources") or core.get("list") or
                                 data.get("resources") or data.get("list") or [])
                        logger.info(f"mhnotify: BTLè¯¦æƒ…ä½¿ç”¨å…¼å®¹èµ„æºå­—æ®µï¼Œæ¡æ•°={len(seeds) if isinstance(seeds, list) else 0}")
                    break
                except Exception:
                    time.sleep(1 + i)
            return seeds if isinstance(seeds, list) else []
        except Exception:
            logger.info("mhnotify: BTLè¯¦æƒ…è°ƒç”¨å¼‚å¸¸")
            return []

    def __btl_get_video_list(self, title: str, page: int = 1, limit: int = 24, app_id:str= "83768d9ad4" , identity: str="23734adac0301bccdcb107c4aa21f96c") -> List[Dict[str, Any]]:
        try:
            base = "https://web5.mukaku.com/prod/api/v1/getVideoList"
            params = {
                "sb": title,
                "page": page,
                "limit": limit,
                "app_id": app_id,
                "identity": identity
            }
            # params["app_id"] = (app_id or self._btl_app_id or "").strip()
            # params["identity"] = (identity or self._btl_identity or "").strip()
            logger.info(f"mhnotify: BTL getVideoList GET {base} sb={title} page={page} limit={limit} app_id={params['app_id']} identity={params['identity'][:8]}***")
            import requests, time
            inner = []
            for i in range(3):
                try:
                    resp = requests.get(base, params=params, headers={
                        "Accept": "application/json, text/plain, */*",
                        "Accept-Language": "zh_CN",
                        "Content-Type": "application/json;charset=UTF-8",
                        "Referer": "https://web5.mukaku.com/search",
                        "User-Agent": "Mozilla/5.0"
                    }, timeout=20)
                    if resp.status_code != 200:
                        time.sleep(1 + i)
                        continue
                    data = resp.json() or {}
                    if not isinstance(data, dict) or str(data.get("code")) != "200":
                        logger.info(f"mhnotify: BTL getVideoList è¿”å›é”™è¯¯ code={data.get('code')} message={data.get('message')}")
                        time.sleep(1 + i)
                        continue
                    inner = (data.get("data") or {}).get("data") or []
                    logger.info(f"mhnotify: BTL getVideoList è¿”å›æ¡ç›®æ•°={len(inner) if isinstance(inner, list) else 0}")
                    break
                except Exception:
                    time.sleep(1 + i)
            return inner if isinstance(inner, list) else []
        except Exception:
            logger.info("mhnotify: BTL getVideoList è°ƒç”¨å¼‚å¸¸")
            return []

    def __get_quality_priority(self, access_token: str) -> Dict[str, Any]:
        try:
            url = f"{self._mh_domain}/api/v1/task_rules/subscription"
            headers = self.__auth_headers(access_token)
            logger.info(f"mhnotify: è·å–è´¨é‡ä¼˜å…ˆçº§é…ç½® GET {url}")
            res = RequestUtils(headers=headers).get_res(url)
            if not res or res.status_code != 200:
                logger.error(f"mhnotify: è·å–è´¨é‡ä¼˜å…ˆçº§å¤±è´¥ status={getattr(res, 'status_code', 'N/A')} body={getattr(res, 'text', '')[:200]}")
                return {}
            data = res.json() or {}
            qp = (data.get("data") or {}).get("quality_priority") or {}
            rp = qp.get("resolution_priority") or []
            hp = qp.get("hdr_priority") or []
            cp = qp.get("codec_priority") or []
            logger.info(f"mhnotify: è´¨é‡ä¼˜å…ˆçº§æ‘˜è¦ resolution={rp} hdr={hp} codec={cp}")
            return qp
        except Exception:
            logger.error("mhnotify: è·å–è´¨é‡ä¼˜å…ˆçº§å¼‚å¸¸", exc_info=True)
            return {}

    def __select_btl_resources_by_priority(self, resources: List[Dict[str, Any]], quality_pref: Optional[str]) -> List[Dict[str, Any]]:
        def get_group(x: Dict[str, Any]) -> str:
            g = str(x.get("definition_group") or "").lower()
            n = str(x.get("zname") or x.get("name") or "").lower()
            if not g and n:
                if "remux" in n:
                    return "remux"
                if "web-dl" in n or "webdl" in n or "webrip" in n:
                    return "web-dl"
                if "hdtv" in n:
                    return "hdtv"
                if "blu" in n or "è“å…‰" in n:
                    return "blu-ray"
            if "3d" in g:
                return "3d"
            if "remux" in n or "remux" in g:
                return "remux"
            if "è“å…‰åŸç›˜" in g or "blu" in g or "è“å…‰" in g:
                return "blu-ray"
            if "web-dl" in g or "webdl" in g or "webrip" in g:
                return "web-dl"
            if "hdtv" in g:
                return "hdtv"
            return g or "other"
        def is_valid(x: Dict[str, Any]) -> bool:
            g = str(x.get("definition_group") or "")
            n = str(x.get("zname") or x.get("name") or "")
            if g.strip().lower() == "3d" or "3D" in g:
                return False
            if ("è“å…‰åŸç›˜" in g) and ("REMUX" not in n.upper()):
                return False
            return True
        filtered = [r for r in resources if is_valid(r)]
        pref = str(quality_pref or "").lower()
        if not pref or pref == "auto":
            order = ["remux", "blu-ray", "web-dl", "hdtv", "other"]
        else:
            order = []
            p = pref.replace(">", ",").replace("|", ",").replace("/", ",")
            for seg in [s.strip().lower() for s in p.split(",") if s.strip()]:
                if "remux" in seg:
                    order.append("remux")
                elif "blu" in seg or "è“å…‰" in seg:
                    order.append("blu-ray")
                elif "web" in seg:
                    order.append("web-dl")
                elif "hdtv" in seg:
                    order.append("hdtv")
            order += [x for x in ["remux", "blu-ray", "web-dl", "hdtv", "other"] if x not in order]
        grouped: Dict[str, List[Dict[str, Any]]] = {}
        for r in filtered:
            k = get_group(r)
            grouped.setdefault(k, []).append(r)
        def parse_size_bytes(v: Any) -> int:
            try:
                # ä¼˜å…ˆ zsizeï¼Œå¦‚ "39.76 GB"
                s = str(v or "").strip()
                if not s:
                    return 0
                # è‹¥ä¼ å…¥çš„æ˜¯èµ„æºé¡¹å­—å…¸ï¼Œæå– zsize æˆ–å…¶ä»–å­—æ®µ
                if isinstance(v, dict):
                    s = str(v.get("zsize") or v.get("size") or v.get("file_size") or v.get("filesize") or "").strip()
                    if not s:
                        return 0
                # çº¯æ•°å­—ï¼ˆå­—èŠ‚ï¼‰
                if s.isdigit():
                    return int(s)
                import re
                m = re.match(r"^\s*([\d\.]+)\s*([a-zA-Z]+)\s*$", s)
                if not m:
                    # å°è¯•ä»ä¸­æ–‡å•ä½ä¸­è§£æ
                    m2 = re.match(r"^\s*([\d\.]+)\s*(å­—èŠ‚|KB|MB|GB|TB)\s*$", s, re.IGNORECASE)
                    if not m2:
                        return 0
                    num = float(m2.group(1))
                    unit = m2.group(2).upper()
                else:
                    num = float(m.group(1))
                    unit = m.group(2).upper()
                # æ ‡å‡†å•ä½æ¢ç®—ï¼ˆæŒ‰ 1024ï¼‰
                if unit in ("B", "BYTE", "BYTES", "å­—èŠ‚"):
                    factor = 1
                elif unit in ("KB", "K", "KIB"):
                    factor = 1024
                elif unit in ("MB", "M", "MIB"):
                    factor = 1024 ** 2
                elif unit in ("GB", "G", "GIB"):
                    factor = 1024 ** 3
                elif unit in ("TB", "T", "TIB"):
                    factor = 1024 ** 4
                else:
                    factor = 1
                return int(num * factor)
            except Exception:
                return 0
        result: List[Dict[str, Any]] = []
        for k in order:
            group_list = grouped.get(k, [])
            if group_list:
                try:
                    group_list = sorted(group_list, key=lambda x: parse_size_bytes(x), reverse=True)
                except Exception:
                    pass
                result += group_list
        return result

    def __select_btl_resources_by_quality_priority(self, resources: List[Dict[str, Any]], qp: Dict[str, Any]) -> List[Dict[str, Any]]:
        try:
            res_pri: List[str] = [str(x).lower() for x in (qp.get("resolution_priority") or [])]
            hdr_pri: List[str] = [str(x).upper() for x in (qp.get("hdr_priority") or [])]
            codec_pri: List[str] = [str(x).upper() for x in (qp.get("codec_priority") or [])]
            exclude_res = set([str(x).lower() for x in (qp.get("exclude_resolutions") or [])])
            exclude_hdr = set([str(x).upper() for x in (qp.get("exclude_hdr_types") or [])])
            exclude_codec = set([str(x).upper() for x in (qp.get("exclude_codecs") or [])])
        except Exception:
            res_pri, hdr_pri, codec_pri = [], [], []
            exclude_res, exclude_hdr, exclude_codec = set(), set(), set()
        def parse_size_bytes(v: Dict[str, Any]) -> int:
            s = str(v.get("zsize") or v.get("size") or v.get("file_size") or v.get("filesize") or "").strip()
            if not s:
                return 0
            if s.isdigit():
                try:
                    return int(s)
                except Exception:
                    return 0
            try:
                import re
                m = re.match(r"^\s*([\d\.]+)\s*([a-zA-Z]+)\s*$", s)
                if not m:
                    return 0
                num = float(m.group(1)); unit = m.group(2).upper()
                if unit in ("B", "BYTE", "BYTES", "å­—èŠ‚"):
                    factor = 1
                elif unit in ("KB", "K", "KIB"):
                    factor = 1024
                elif unit in ("MB", "M", "MIB"):
                    factor = 1024 ** 2
                elif unit in ("GB", "G", "GIB"):
                    factor = 1024 ** 3
                elif unit in ("TB", "T", "TIB"):
                    factor = 1024 ** 4
                else:
                    factor = 1
                return int(num * factor)
            except Exception:
                return 0
        def extract_features(x: Dict[str, Any]) -> Tuple[str, str, str]:
            name = str(x.get("zname") or x.get("name") or "")
            name_u = name.upper()
            # resolution
            res = ""
            for r in ["2160P", "1080P", "720P", "480P"]:
                if r in name_u:
                    res = r.lower()
                    break
            # hdr
            hdr = ""
            for h in ["DV", "HDR10+", "HDR10", "HDR", "HDR VIVID"]:
                if h in name_u:
                    hdr = h
                    break
            # codec
            codec = ""
            if any(k in name_u for k in ["H265", "X265", "HEVC"]):
                codec = "H265"
            elif any(k in name_u for k in ["H264", "X264", "AVC"]):
                codec = "H264"
            elif "AV1" in name_u:
                codec = "AV1"
            return res, hdr, codec
        def excluded(x: Dict[str, Any]) -> bool:
            g = str(x.get("definition_group") or "")
            n = str(x.get("zname") or x.get("name") or "")
            if ("æ— å­—" in n) or ("ç„¡å­—" in n) or ("æ— å­—ç‰‡æº" in n):
                return True
            if g.strip().lower() == "3d" or "3D" in g:
                return True
            if ("è“å…‰åŸç›˜" in g) and ("REMUX" not in n.upper()):
                return True
            res, hdr, codec = extract_features(x)
            if res and res in exclude_res:
                return True
            if hdr and hdr in exclude_hdr:
                return True
            if codec and codec in exclude_codec:
                return True
            return False
        # è¿‡æ»¤
        filtered = [r for r in resources if not excluded(r)]
        # æ‰“åˆ†ï¼šå„ç»´åº¦æŒ‰åˆ—è¡¨ä¸‹æ ‡å€¼ï¼Œè¶Šå°è¶Šä¼˜ï¼›ç¼ºå¤±è§†ä¸ºæœ«ä½
        def pri_index(value: str, lst: List[str], normalize_upper: bool = False) -> int:
            if not value:
                return len(lst) + 1
            val = value.upper() if normalize_upper else value
            try:
                return lst.index(val) if not normalize_upper else lst.index(val)
            except ValueError:
                return len(lst) + 1
        def sort_key(x: Dict[str, Any]) -> Tuple[int, int, int, int]:
            res, hdr, codec = extract_features(x)
            ri = pri_index(res, res_pri)  # res_pri å·²ä¸ºå°å†™
            hi = pri_index(hdr, hdr_pri, normalize_upper=True)
            ci = pri_index(codec, codec_pri, normalize_upper=True)
            size = parse_size_bytes(x)
            return (ri, hi, ci, -size)
        try:
            sorted_list = sorted(filtered, key=sort_key)
        except Exception:
            # å…œåº•ï¼šä»…æŒ‰å¤§å°
            sorted_list = sorted(filtered, key=lambda x: parse_size_bytes(x), reverse=True)
        logger.info(f"mhnotify: è´¨é‡ä¼˜å…ˆçº§æ’åºå®Œæˆï¼Œå€™é€‰={len(sorted_list)}")
        if sorted_list:
            top = sorted_list[0]
            logger.info(f"mhnotify: é¦–é€‰èµ„æº name={str(top.get('zname') or top.get('name') or '')[:80]} size={str(top.get('zsize') or '')}")
        return sorted_list

    def __try_cloud_download_with_candidates(self, candidates: List[Dict[str, Any]], sid: Union[str, int], mh_uuid: str) -> bool:
        total = len(candidates or [])
        for idx, item in enumerate(candidates or [], start=1):
            url = str(item.get("zlink") or item.get("link") or "")
            if not url:
                continue
            name = str(item.get("zname") or item.get("name") or "")
            size = str(item.get("zsize") or "")
            logger.info(f"mhnotify: å°è¯•äº‘ä¸‹è½½å€™é€‰ {idx}/{total} name={name[:80]} size={size}")
            ok, msg, info = self._add_offline_download(url, start_monitor=True)
            if ok:
                ih = info.get("info_hash")
                if ih:
                    mapping = self.get_data(self._ASSIST_CLOUD_MAP_KEY) or {}
                    mapping[ih] = {"sid": int(sid), "mh_uuid": mh_uuid}
                    self.save_data(self._ASSIST_CLOUD_MAP_KEY, mapping)
                    logger.info(f"mhnotify: äº‘ä¸‹è½½ä»»åŠ¡å·²æäº¤ï¼Œinfo_hash={str(ih)[:16]}... å·²å»ºç«‹å›è°ƒæ˜ å°„ sid={sid}")
                return True
            else:
                logger.info(f"mhnotify: äº‘ä¸‹è½½å¤±è´¥ï¼Œç»§ç»­å°è¯•ä¸‹ä¸€å€™é€‰ã€‚åŸå› ={msg}")
        return False
    def __compute_progress(self, sub_rec: Dict[str, Any]) -> Tuple[str, int, int]:
        """è¿”å› (media_type, saved, expected_total)"""
        params = (sub_rec or {}).get("params") or {}
        mtype = (params.get("media_type") or (sub_rec.get("subscription_info") or {}).get("media_type") or "movie").lower()
        
        saved = 0
        expected_total = 1 if mtype == 'movie' else 0
        
        try:
            episodes = (sub_rec.get("episodes") or [])
            if episodes:
                # 1. è·å–è®¢é˜…çš„å­£
                selected_seasons = params.get("selected_seasons") or []
                selected_set = set(str(x) for x in selected_seasons)
                
                # 2. è·å–å·²ä¿å­˜çš„é›†æ•° (episodes_arr)
                # ç»“æ„: episodes[0].episodes_arr = {"1": [1,2,3], "2": [1,2]}
                episodes_arr = (episodes[0] or {}).get("episodes_arr") or {}
                
                # 3. è·å–æ‰€æœ‰å­£é›†æ•°ä¿¡æ¯ (episodes_count)
                counts = (episodes[0] or {}).get("episodes_count") or {}
                
                if mtype == 'tv':
                    for season_str, season_info in counts.items():
                        # å¦‚æœæŒ‡å®šäº† selected_seasonsï¼Œåˆ™åªè®¡ç®—é€‰ä¸­çš„å­£
                        if selected_set and str(season_str) not in selected_set:
                            continue
                        
                        # è®¡ç®—æ€»é›†æ•°
                        expected_total += int(season_info.get("count") or 0)
                        
                        # è®¡ç®—å·²ä¿å­˜é›†æ•° (è¯¥å­£ä¸‹å·²æœ‰çš„é›†æ•°åˆ—è¡¨é•¿åº¦)
                        saved_list = episodes_arr.get(str(season_str)) or []
                        saved += len(saved_list)
                else:
                    # movie: æ£€æŸ¥ episodes_arr æ˜¯å¦æœ‰å†…å®¹ï¼Œæˆ–è€… params é‡Œ saved_resources æ˜¯å¦ > 0
                    if episodes_arr or int(params.get("saved_resources") or 0) > 0:
                        saved = 1
                    expected_total = 1
        except Exception:
            pass
        return mtype, saved, expected_total

    def __assist_scheduler(self):
        """æ¯åˆ†é’Ÿæ‰§è¡Œï¼šå…ˆç­‰å¾…2åˆ†é’Ÿè¿›è¡Œé¦–æ¬¡æŸ¥è¯¢ï¼›æœªæŸ¥è¯¢åˆ°åˆ™æ¯1åˆ†é’Ÿé‡è¯•ï¼Œç›´åˆ°æŸ¥è¯¢åˆ°ï¼›å¹¶å¤„ç†MPå®Œæˆç›‘å¬"""
        try:
            # å¤„ç†å¾…æ£€æŸ¥
            pending: Dict[str, dict] = self.get_data(self._ASSIST_PENDING_KEY) or {}
            if pending:
                now_ts = int(time.time())
                # æ”¶é›†å·²åˆ°æŸ¥è¯¢æ—¶é—´çš„æ¡ç›®ï¼ˆé¦–æ¬¡æŸ¥è¯¢å»¶è¿Ÿï¼‰
                matured_items = {sid: info for sid, info in pending.items() if now_ts - int(info.get("created_at") or 0) >= self._assist_initial_delay_seconds}
                if matured_items:
                    logger.info(f"mhnotify: åŠ©æ‰‹åˆ°æœŸæŸ¥è¯¢æ¡ç›®æ•°={len(matured_items)}")
                    token = self.__mh_login()
                    if not token:
                        logger.error("mhnotify: ç™»å½•MHå¤±è´¥ï¼Œæ— æ³•æŸ¥è¯¢è®¢é˜…è¿›åº¦")
                    else:
                        lst = self.__mh_list_subscriptions(token)
                        subs = (lst.get("data") or {}).get("subscriptions") or []
                        subs_map = {}
                        for rec in subs:
                            uid = rec.get("uuid") or rec.get("task", {}).get("uuid")
                            if uid:
                                subs_map[uid] = rec
                        for sid, info in list(matured_items.items()):
                            mh_uuid = info.get("mh_uuid")
                            logger.info(f"mhnotify: åˆ°æœŸå¤„ç† sid={sid} mh_uuid={mh_uuid} type={info.get('type')} douban_id={info.get('douban_id')}")
                            target = subs_map.get(mh_uuid)
                            if not target:
                                # æœªæ‰¾åˆ°ï¼Œè®°å½•é‡è¯•æ¬¡æ•°ï¼Œè¶…è¿‡30æ¬¡åˆ™ç§»é™¤è®°å½•
                                attempts = int(info.get("attempts") or 0) + 1
                                info["attempts"] = attempts
                                info["last_attempt"] = now_ts
                                if attempts >= 30:
                                    logger.warning(f"mhnotify: è®¢é˜… {mh_uuid} æœªåœ¨MHåˆ—è¡¨ä¸­æ‰¾åˆ°ï¼Œå·²é‡è¯•{attempts}æ¬¡ï¼Œç§»é™¤è®°å½•")
                                    pending.pop(sid, None)
                                    self.save_data(self._ASSIST_PENDING_KEY, pending)
                                    continue
                                else:
                                    retry_mins = max(1, int(self._assist_retry_interval_seconds / 60))
                                    logger.warning(f"mhnotify: æœªåœ¨MHåˆ—è¡¨ä¸­æ‰¾åˆ°è®¢é˜… {mh_uuid}ï¼Œç¬¬{attempts}æ¬¡é‡è¯•ï¼Œ{retry_mins}åˆ†é’Ÿåç»§ç»­")
                                    pending[str(sid)] = info
                                    self.save_data(self._ASSIST_PENDING_KEY, pending)
                                    continue
                            mtype, saved, expected = self.__compute_progress(target)
                            logger.info(f"mhnotify: è®¢é˜… {mh_uuid} è¿›åº¦ saved={saved}/{expected} type={mtype}")
                            with SessionFactory() as db:
                                subscribe = SubscribeOper(db=db).get(int(sid))
                            if not subscribe:
                                # MPè®¢é˜…å·²ä¸å­˜åœ¨ï¼ˆå¯èƒ½ä¸ºå–æ¶ˆå•å­£ï¼‰
                                # ä¼˜å…ˆå°è¯•ï¼šæŒ‰åŒ TMDB çš„å‰©ä½™å­£æ›´æ–° MH è®¢é˜…ï¼›è‹¥æ— å‰©ä½™å­£åˆ™åˆ é™¤ MH
                                try:
                                    del_token = self.__mh_login()
                                except Exception:
                                    del_token = None
                                if del_token and mh_uuid:
                                    try:
                                        lst2 = self.__mh_list_subscriptions(del_token)
                                        subs2 = (lst2.get("data") or {}).get("subscriptions") or []
                                        rec2 = None
                                        for r in subs2:
                                            uid2 = r.get("uuid") or (r.get("task") or {}).get("uuid")
                                            if uid2 == mh_uuid:
                                                rec2 = r
                                                break
                                        tmdb_id = None
                                        if rec2:
                                            params2 = rec2.get("params") or {}
                                            tmdb_id = params2.get("tmdb_id")
                                        remaining_seasons: List[int] = []
                                        if tmdb_id:
                                            try:
                                                with SessionFactory() as db2:
                                                    all_subs = SubscribeOper(db=db2).list_by_tmdbid(tmdb_id)
                                                seasons = []
                                                for s in all_subs or []:
                                                    try:
                                                        stype = (getattr(s, 'type', '') or '').strip()
                                                        stype_lower = (stype or '').lower()
                                                        if stype_lower == 'tv' or stype in {'ç”µè§†å‰§'}:
                                                            s_season = getattr(s, 'season', None)
                                                            if s_season is None:
                                                                s_season = self.__extract_season_from_text(getattr(s, 'name', '') or '')
                                                            seasons.append(s_season)
                                                    except Exception:
                                                        pass
                                                tmp: List[int] = []
                                                for x in seasons:
                                                    if isinstance(x, int):
                                                        tmp.append(x)
                                                    elif isinstance(x, str) and x.isdigit():
                                                        tmp.append(int(x))
                                                remaining_seasons = sorted({s for s in tmp if isinstance(s, int) and s > 0})
                                            except Exception:
                                                remaining_seasons = []
                                        if remaining_seasons:
                                            # æ›´æ–° MH è®¢é˜…å­£é›†åˆä¸ºå‰©ä½™å­£
                                            try:
                                                base_params = (rec2 or {}).get("params") or {}
                                                base_params["selected_seasons"] = remaining_seasons
                                                base_params["episode_ranges"] = {str(s): {"min_episode": None, "max_episode": None, "exclude_episodes": [], "exclude_text": ""} for s in remaining_seasons}
                                                self.__mh_update_subscription(del_token, mh_uuid, base_params)
                                                logger.info(f"mhnotify: å–æ¶ˆå•å­£åæ›´æ–°MHè®¢é˜… seasons={remaining_seasons}")
                                            except Exception:
                                                logger.warning("mhnotify: æ›´æ–°MHè®¢é˜…å­£é›†åˆå¤±è´¥ï¼Œé™çº§ä¸ºåˆ é™¤", exc_info=True)
                                                self.__mh_delete_subscription(del_token, mh_uuid)
                                        else:
                                            # æ— å‰©ä½™å­£ï¼Œåˆ é™¤ MH è®¢é˜…
                                            self.__mh_delete_subscription(del_token, mh_uuid)
                                    except Exception:
                                        # é™çº§ç­–ç•¥ï¼šå‡ºç°å¼‚å¸¸åˆ™å°½é‡åˆ é™¤å¯¹åº” MH è®¢é˜…ï¼Œé¿å…é—ç•™æ— ä¸»è®¢é˜…
                                        try:
                                            self.__mh_delete_subscription(del_token, mh_uuid)
                                        except Exception:
                                            logger.warning("mhnotify: å¤„ç†å‰©ä½™å­£æ—¶å¼‚å¸¸ä¸”åˆ é™¤å¤±è´¥", exc_info=True)
                                pending.pop(sid, None)
                                self.save_data(self._ASSIST_PENDING_KEY, pending)
                                continue
                            if mtype == 'movie':
                                if expected <= 1 and saved >= 1:
                                    # å®Œæˆï¼šç›´æ¥å®ŒæˆMPè®¢é˜…ï¼ŒMHåˆ é™¤äº¤ç”± SubscribeComplete äº‹ä»¶å¤„ç†
                                    self.__finish_mp_subscribe(subscribe)
                                    pending.pop(sid, None)
                                    self.save_data(self._ASSIST_PENDING_KEY, pending)
                                else:
                                    if self._cloud_download_assist:
                                        try:
                                            logger.info(f"mhnotify: è¿›å…¥äº‘ä¸‹è½½è¾…åŠ©åˆ†æ”¯ sid={sid} mh_uuid={mh_uuid}")
                                            qp_dict = self.__get_quality_priority(token)
                                        except Exception:
                                            qp_dict = {}
                                        douban_id = info.get("douban_id")
                                        # è‹¥ç¼ºå°‘è±†ç“£IDï¼Œå…ˆæŒ‰æ ‡é¢˜æœç´¢å›é€€
                                        search_douban_id = douban_id
                                        if not search_douban_id:
                                            try:
                                                title_src = subscribe.name or mediainfo_dict.get("title") or ""
                                                search_list = self.__btl_get_video_list(title_src)
                                                pick = None
                                                for rec in search_list or []:
                                                    r_type = int(rec.get("type") or 0)
                                                    r_title = str(rec.get("title") or "").strip()
                                                    if r_type == 1 and r_title and r_title == title_src:
                                                        pick = rec
                                                        break
                                                if pick:
                                                    search_douban_id = pick.get("doub_id") or pick.get("douban_id") or pick.get("id") or pick.get("db_id")
                                                    logger.info(f"mhnotify: BTL getVideoList åŒ¹é…åˆ°è±†ç“£ID={search_douban_id}")
                                                else:
                                                    logger.info("mhnotify: BTL getVideoList æœªåŒ¹é…åˆ°ç”µå½±æ¡ç›®æˆ–æ ‡é¢˜ä¸ä¸€è‡´")
                                            except Exception:
                                                search_douban_id = None
                                        if search_douban_id:
                                            details = self.__btl_get_video_detail(token, search_douban_id)
                                            logger.info(f"mhnotify: äº‘ä¸‹è½½è¾…åŠ©ï¼šè´¨é‡ä¼˜å…ˆçº§å·²è·å– keys={list(qp_dict.keys()) if isinstance(qp_dict, dict) else []}")
                                            logger.info(f"mhnotify: äº‘ä¸‹è½½è¾…åŠ©ï¼šBTLèµ„æºæ¡æ•°={len(details) if isinstance(details, list) else 0}")
                                            if not details:
                                                logger.info("mhnotify: BTLè¯¦æƒ…æŸ¥è¯¢å¤±è´¥æˆ–æ— èµ„æºï¼Œæ¢å¤è®¢é˜…å¯ç”¨")
                                                # with SessionFactory() as db:
                                                #     SubscribeOper(db=db).update(subscribe.id, {"state": "R", "sites": []})
                                                # æ¢å¤è®¢é˜…å¯ç”¨æ—¶ï¼Œä¸åº”è¯¥å°† sites ç½®ç©ºï¼Œå¦åˆ™æ— æ³•æ´—ç‰ˆ
                                                with SessionFactory() as db:
                                                    # è·å–åŸå§‹è®¢é˜…ä¿¡æ¯
                                                    origin_sub = SubscribeOper(db=db).get(subscribe.id)
                                                    if origin_sub:
                                                        # æ¢å¤è®¢é˜…çŠ¶æ€ä¸º Rï¼Œä¸ä¿®æ”¹ sitesï¼Œä¿ç•™åŸæœ‰çš„ç«™ç‚¹é…ç½®ä»¥ä¾¿æ´—ç‰ˆ
                                                        SubscribeOper(db=db).update(subscribe.id, {"state": "R"})
                                                pending.pop(sid, None)
                                                self.save_data(self._ASSIST_PENDING_KEY, pending)
                                            else:
                                                candidates = self.__select_btl_resources_by_quality_priority(details, qp_dict)
                                                logger.info(f"mhnotify: äº‘ä¸‹è½½è¾…åŠ©ï¼šå€™é€‰æ¡æ•°ï¼ˆæ’åºåï¼‰={len(candidates)}")
                                                started = self.__try_cloud_download_with_candidates(candidates, sid, mh_uuid)
                                                if started:
                                                    logger.info(f"mhnotify: å·²è§¦å‘äº‘ä¸‹è½½ï¼ˆä¼˜å…ˆçº§é¦–é€‰ï¼‰ï¼Œç­‰å¾…ä¸‹è½½å®Œæˆåå›è°ƒå¤„ç†è®¢é˜… sid={sid}")
                                                    # ä¸ºåç»­å–æ¶ˆäº‹ä»¶æä¾›æ˜ å°„
                                                    watch: Dict[str, dict] = self.get_data(self._ASSIST_WATCH_KEY) or {}
                                                    try:
                                                        tmdb_id = getattr(subscribe, 'tmdbid', None)
                                                        sub_type = (getattr(subscribe, 'type', '') or '').lower()
                                                    except Exception:
                                                        tmdb_id = None
                                                        sub_type = ''
                                                    watch[str(sid)] = {"mh_uuid": mh_uuid, "tmdb_id": tmdb_id, "type": sub_type or 'movie'}
                                                    self.save_data(self._ASSIST_WATCH_KEY, watch)
                                                    pending.pop(sid, None)
                                                    self.save_data(self._ASSIST_PENDING_KEY, pending)
                                                else:
                                                    logger.info("mhnotify: äº‘ä¸‹è½½è¾…åŠ©æœªåŒ¹é…åˆ°å¯ç”¨èµ„æºæˆ–å…¨éƒ¨å¤±è´¥ï¼Œæ¢å¤è®¢é˜…å¯ç”¨")
                                                    with SessionFactory() as db:
                                                        SubscribeOper(db=db).update(subscribe.id, {"state": "R", "sites": []})
                                                    # æ¢å¤å¯ç”¨åï¼ŒåŠ å…¥watchæ˜ å°„ï¼Œç”¨äºå–æ¶ˆäº‹ä»¶å¿«é€Ÿåˆ é™¤MH
                                                    watch: Dict[str, dict] = self.get_data(self._ASSIST_WATCH_KEY) or {}
                                                    watch[str(sid)] = {"mh_uuid": mh_uuid}
                                                    self.save_data(self._ASSIST_WATCH_KEY, watch)
                                                    pending.pop(sid, None)
                                                    self.save_data(self._ASSIST_PENDING_KEY, pending)
                                        else:
                                            logger.info("mhnotify: äº‘ä¸‹è½½è¾…åŠ©è·³è¿‡ï¼šç¼ºå°‘è±†ç“£ID")
                                            with SessionFactory() as db:
                                                SubscribeOper(db=db).update(subscribe.id, {"state": "R", "sites": []})
                                            # åŠ å…¥watchæ˜ å°„ï¼Œç”¨äºå–æ¶ˆäº‹ä»¶å¿«é€Ÿåˆ é™¤MH
                                            watch: Dict[str, dict] = self.get_data(self._ASSIST_WATCH_KEY) or {}
                                            watch[str(sid)] = {"mh_uuid": mh_uuid}
                                            self.save_data(self._ASSIST_WATCH_KEY, watch)
                                            pending.pop(sid, None)
                                            self.save_data(self._ASSIST_PENDING_KEY, pending)
                                    else:
                                        with SessionFactory() as db:
                                            SubscribeOper(db=db).update(subscribe.id, {"state": "R", "sites": []})
                                        watch: Dict[str, dict] = self.get_data(self._ASSIST_WATCH_KEY) or {}
                                        try:
                                            tmdb_id = getattr(subscribe, 'tmdbid', None)
                                            sub_type = (getattr(subscribe, 'type', '') or '').lower()
                                        except Exception:
                                            tmdb_id = None
                                            sub_type = ''
                                        watch[str(sid)] = {"mh_uuid": mh_uuid, "tmdb_id": tmdb_id, "type": sub_type or 'movie'}
                                        self.save_data(self._ASSIST_WATCH_KEY, watch)
                                        pending.pop(sid, None)
                                        self.save_data(self._ASSIST_PENDING_KEY, pending)
                            else:
                                # TV
                                if expected > 0 and saved >= expected:
                                    # å®Œæˆï¼šç›´æ¥å®ŒæˆMPè®¢é˜…ï¼ŒMHåˆ é™¤äº¤ç”± SubscribeComplete äº‹ä»¶å¤„ç†
                                    self.__finish_mp_subscribe(subscribe)
                                    pending.pop(sid, None)
                                    self.save_data(self._ASSIST_PENDING_KEY, pending)
                                else:
                                    # æœªå®Œæˆï¼šä¸åˆ é™¤MHï¼Œå¯ç”¨MPè®¢é˜…ï¼Œå¹¶åŠ å…¥watchç­‰å¾…MPå®Œæˆ/å–æ¶ˆååˆ é™¤MH
                                    # æ³¨æ„ï¼šå¯¹äºå¤šå­£è®¢é˜…ï¼Œå¦‚æœéƒ¨åˆ†å­£å·²å®Œæˆï¼Œä¸åº”åœ¨æ­¤å¤„æ ‡è®°æ•´ä¸ªè®¢é˜…ä¸ºå®Œæˆ
                                    # å› ä¸ºMHè®¢é˜…æ˜¯èšåˆçš„ï¼Œè¿™é‡Œçš„ saved å’Œ expected æ˜¯é’ˆå¯¹èšåˆåçš„æ‰€æœ‰å­£
                                    # å¦‚æœå½“å‰MPè®¢é˜…å¯¹åº”çš„å­£å·²ç»ä¸‹è½½å®Œäº†ï¼Œä½†MHé‡Œè¿˜æœ‰å…¶ä»–å­£æ²¡ä¸‹è½½å®Œï¼Œè¿™é‡Œä¹Ÿæ˜¯ saved < expected
                                    # æ‰€ä»¥è¿™é‡Œåº”è¯¥è¿›ä¸€æ­¥æ£€æŸ¥ï¼šå½“å‰MPè®¢é˜…å¯¹åº”çš„å­£æ˜¯å¦å·²ç»å…¨éƒ¨ä¿å­˜äº†
                                    
                                    current_sub_completed = False
                                    try:
                                        # è·å–å½“å‰MPè®¢é˜…çš„å­£
                                        mp_season = getattr(subscribe, 'season', None)
                                        if mp_season is not None:
                                            # è·å–MHè®¢é˜…ä¸­è¯¥å­£çš„ä¿å­˜æƒ…å†µ
                                            episodes = (target.get("episodes") or [])
                                            if episodes:
                                                counts = (episodes[0] or {}).get("episodes_count") or {}
                                                episodes_arr = (episodes[0] or {}).get("episodes_arr") or {}
                                                
                                                season_str = str(mp_season)
                                                
                                                # è¯¥å­£æ€»é›†æ•°
                                                season_total = int((counts.get(season_str) or {}).get("count") or 0)
                                                # è¯¥å­£å·²ä¿å­˜é›†æ•°
                                                season_saved = len(episodes_arr.get(season_str) or [])
                                                
                                                if season_total > 0 and season_saved >= season_total:
                                                    current_sub_completed = True
                                                    logger.info(f"mhnotify: è®¢é˜… {mh_uuid} æ•´ä½“æœªå®Œæˆ({saved}/{expected})ï¼Œä½†MPè®¢é˜…å¯¹åº”çš„ç¬¬{mp_season}å­£å·²å®Œæˆ({season_saved}/{season_total})")
                                    except Exception:
                                        pass
                                    
                                    if current_sub_completed:
                                        # ä»…å®Œæˆå½“å‰MPè®¢é˜…
                                        self.__finish_mp_subscribe(subscribe)
                                        pending.pop(sid, None)
                                        self.save_data(self._ASSIST_PENDING_KEY, pending)
                                    else:
                                        # ç¡®å®æœªå®Œæˆ
                                        with SessionFactory() as db:
                                            SubscribeOper(db=db).update(subscribe.id, {"state": "R", "sites": []})
                                        watch: Dict[str, dict] = self.get_data(self._ASSIST_WATCH_KEY) or {}
                                        try:
                                            tmdb_id = getattr(subscribe, 'tmdbid', None)
                                            sub_type = (getattr(subscribe, 'type', '') or '').lower()
                                        except Exception:
                                            tmdb_id = None
                                            sub_type = ''
                                        watch[str(sid)] = {"mh_uuid": mh_uuid, "tmdb_id": tmdb_id, "type": sub_type or 'movie'}
                                        self.save_data(self._ASSIST_WATCH_KEY, watch)
                                        pending.pop(sid, None)
                                        self.save_data(self._ASSIST_PENDING_KEY, pending)
            # ç›‘å¬MPå®Œæˆååˆ é™¤MHï¼ˆå¯é€‰ï¼‰
            watch: Dict[str, dict] = self.get_data(self._ASSIST_WATCH_KEY) or {}
            if watch and self._mh_assist_auto_delete:
                for sid, info in list(watch.items()):
                    with SessionFactory() as db:
                        sub = SubscribeOper(db=db).get(int(sid))
                    if not sub:
                        # MPè®¢é˜…ä¸å­˜åœ¨ï¼ˆå–æ¶ˆ/å®Œæˆï¼‰ï¼Œå¤„ç†å¯¹åº”MHï¼šä¼˜å…ˆæ›´æ–°å‰©ä½™å­£ï¼Œå¦åˆ™åˆ é™¤
                        mh_uuid = info.get("mh_uuid")
                        try:
                            del_token = self.__mh_login()
                        except Exception:
                            del_token = None
                        if mh_uuid and del_token:
                            try:
                                lst2 = self.__mh_list_subscriptions(del_token)
                                subs2 = (lst2.get("data") or {}).get("subscriptions") or []
                                rec2 = None
                                for r in subs2:
                                    uid2 = r.get("uuid") or (r.get("task") or {}).get("uuid")
                                    if uid2 == mh_uuid:
                                        rec2 = r
                                        break
                                tmdb_id = None
                                if rec2:
                                    params2 = rec2.get("params") or {}
                                    tmdb_id = params2.get("tmdb_id")
                                remaining_seasons: List[int] = []
                                if tmdb_id:
                                    try:
                                        with SessionFactory() as db2:
                                            all_subs = SubscribeOper(db=db2).list_by_tmdbid(tmdb_id)
                                        seasons = []
                                        for s in all_subs or []:
                                            try:
                                                stype = (getattr(s, 'type', '') or '').strip()
                                                stype_lower = (stype or '').lower()
                                                if stype_lower == 'tv' or stype in {'ç”µè§†å‰§'}:
                                                    s_season = getattr(s, 'season', None)
                                                    if s_season is None:
                                                        s_season = self.__extract_season_from_text(getattr(s, 'name', '') or '')
                                                    seasons.append(s_season)
                                            except Exception:
                                                pass
                                        tmp: List[int] = []
                                        for x in seasons:
                                            if isinstance(x, int):
                                                tmp.append(x)
                                            elif isinstance(x, str) and x.isdigit():
                                                tmp.append(int(x))
                                        remaining_seasons = sorted({s for s in tmp if isinstance(s, int) and s > 0})
                                    except Exception:
                                        remaining_seasons = []
                                if remaining_seasons:
                                    try:
                                        base_params = (rec2 or {}).get("params") or {}
                                        base_params["selected_seasons"] = remaining_seasons
                                        base_params["episode_ranges"] = {str(s): {"min_episode": None, "max_episode": None, "exclude_episodes": [], "exclude_text": ""} for s in remaining_seasons}
                                        self.__mh_update_subscription(del_token, mh_uuid, base_params)
                                        logger.info(f"mhnotify: å–æ¶ˆå•å­£åæ›´æ–°MHè®¢é˜… seasons={remaining_seasons}")
                                    except Exception:
                                        logger.warning("mhnotify: æ›´æ–°MHè®¢é˜…å­£é›†åˆå¤±è´¥ï¼Œé™çº§ä¸ºåˆ é™¤", exc_info=True)
                                        self.__mh_delete_subscription(del_token, mh_uuid)
                                else:
                                    self.__mh_delete_subscription(del_token, mh_uuid)
                            except Exception:
                                # é™çº§ç­–ç•¥ï¼šå‡ºç°å¼‚å¸¸åˆ™å°½é‡åˆ é™¤å¯¹åº” MH è®¢é˜…ï¼Œé¿å…é—ç•™æ— ä¸»è®¢é˜…
                                try:
                                    self.__mh_delete_subscription(del_token, mh_uuid)
                                except Exception:
                                    logger.warning("mhnotify: watch åˆ†æ”¯å¤„ç†å‰©ä½™å­£æ—¶å¼‚å¸¸ä¸”åˆ é™¤å¤±è´¥", exc_info=True)
                        # æ¸…ç†å½“å‰ç›‘å¬é¡¹
                        watch.pop(sid, None)
                        self.save_data(self._ASSIST_WATCH_KEY, watch)
        except Exception as e:
            logger.error(f"mhnotify: åŠ©æ‰‹è°ƒåº¦å¼‚å¸¸: {e}")

    
    def _add_offline_download(self, url: str, start_monitor: bool = True) -> Tuple[bool, str, Dict[str, Any]]:
        """
        æ·»åŠ 115ç¦»çº¿ä¸‹è½½ä»»åŠ¡
        å‚è€ƒ p115client å®˜æ–¹åº“çš„ P115Offline.add æ–¹æ³•å®ç°
        :param url: ä¸‹è½½é“¾æ¥ï¼ˆç£åŠ›é“¾æ¥ã€ç§å­URLç­‰ï¼‰
        :param start_monitor: æ˜¯å¦å¯åŠ¨åå°ç›‘æ§çº¿ç¨‹ï¼ˆæ‰¹é‡ä¸‹è½½æ—¶è®¾ä¸ºFalseï¼Œç»Ÿä¸€ç›‘æ§ï¼‰
        :return: (æ˜¯å¦æˆåŠŸ, æ¶ˆæ¯æ–‡æœ¬, ä»»åŠ¡ä¿¡æ¯å­—å…¸)
        """
        task_info = {}  # ç”¨äºè¿”å›ä»»åŠ¡ä¿¡æ¯ï¼Œä¾›æ‰¹é‡å¤„ç†ä½¿ç”¨
        try:
            # å¯¼å…¥p115client
            try:
                from p115client import P115Client
            except ImportError:
                return False, "p115client æœªå®‰è£…ï¼Œè¯·å…ˆå®‰è£…ä¾èµ–", task_info

            # åˆ›å»º115å®¢æˆ·ç«¯
            client = P115Client(self._p115_cookie, app="web")
            
            # è·å–æˆ–åˆ›å»ºç›®æ ‡ç›®å½•ID
            target_path = self._cloud_download_path or "/äº‘ä¸‹è½½"
            # æ ‡å‡†åŒ–è·¯å¾„
            target_path = target_path.strip()
            if not target_path.startswith('/'):
                target_path = '/' + target_path
            target_path = target_path.rstrip('/')
            if not target_path:
                target_path = "/"
            
            target_cid = 0
            
            try:
                if target_path != "/":
                    # ä½¿ç”¨ fs_dir_getid è·å–ç›®å½•ID
                    resp = client.fs_dir_getid(target_path)
                    logger.debug(f"mhnotify: fs_dir_getid å“åº”: {resp}")
                    if resp and resp.get("id"):
                        target_cid = int(resp.get("id"))
                        logger.info(f"mhnotify: ç›®æ ‡ç›®å½• {target_path} ID: {target_cid}")
                    elif resp and resp.get("id") == 0:
                        # ç›®å½•ä¸å­˜åœ¨ï¼Œéœ€è¦åˆ›å»º
                        logger.info(f"mhnotify: ç›®å½• {target_path} ä¸å­˜åœ¨ï¼Œå°è¯•åˆ›å»º...")
                        mkdir_resp = client.fs_makedirs_app(target_path, pid=0)
                        logger.debug(f"mhnotify: fs_makedirs_app å“åº”: {mkdir_resp}")
                        if mkdir_resp and mkdir_resp.get("cid"):
                            target_cid = int(mkdir_resp.get("cid"))
                            logger.info(f"mhnotify: åˆ›å»ºç›®å½•æˆåŠŸï¼ŒID: {target_cid}")
                        else:
                            logger.warning(f"mhnotify: åˆ›å»ºç›®å½•å¤±è´¥: {mkdir_resp}")
                            target_cid = 0
                    else:
                        logger.warning(f"mhnotify: è·å–ç›®å½•IDå¤±è´¥: {resp}")
                        target_cid = 0
            except Exception as e:
                logger.warning(f"mhnotify: è·å–ç›®å½•IDå¼‚å¸¸: {e}", exc_info=True)
                target_cid = 0

            # æ„å»ºç¦»çº¿ä¸‹è½½payload
            # å‚è€ƒ p115client æºç ï¼šå•ä¸ªURLä½¿ç”¨ "url" é”®ï¼Œè°ƒç”¨ offline_add_url
            download_url = url.strip()
            payload = {"url": download_url}
            if target_cid:
                payload["wp_path_id"] = target_cid
            
            logger.info(f"mhnotify: æ·»åŠ ç¦»çº¿ä¸‹è½½ä»»åŠ¡ï¼Œç›®æ ‡ç›®å½•ID: {target_cid}, URL: {download_url[:80]}...")
            
            # è°ƒç”¨115ç¦»çº¿ä¸‹è½½APIï¼ˆå•ä¸ªURLç”¨ offline_add_urlï¼‰
            resp = client.offline_add_url(payload)
            logger.debug(f"mhnotify: offline_add_url å“åº”: {resp}")
            
            # æ£€æŸ¥å“åº”
            if not resp:
                return False, "115 API å“åº”ä¸ºç©º"
            
            # å“åº”å¯èƒ½æ˜¯dictæˆ–å…¶ä»–ç±»å‹
            if isinstance(resp, dict):
                state = resp.get('state', False)
                
                # è§£æè¿”å›çš„ä»»åŠ¡ä¿¡æ¯ï¼ˆæ— è®ºæˆåŠŸè¿˜æ˜¯å¤±è´¥ï¼Œdataä¸­å¯èƒ½éƒ½æœ‰info_hashï¼‰
                data = resp.get('data', {})
                if isinstance(data, dict):
                    info_hash = data.get('info_hash', '')
                    task_name = data.get('name', '')
                    files_list = data.get('files', [])
                else:
                    info_hash = ''
                    task_name = ''
                    files_list = []
                
                if not state:
                    error_msg = resp.get('error_msg', '') or resp.get('error', 'æœªçŸ¥é”™è¯¯')
                    error_code = resp.get('errcode', '')
                    
                    # ç‰¹æ®Šå¤„ç†é”™è¯¯ç 10008ï¼šä»»åŠ¡å·²å­˜åœ¨
                    if error_code == 10008:
                        logger.warning(f"mhnotify: ç¦»çº¿ä¸‹è½½ä»»åŠ¡å·²å­˜åœ¨: {info_hash}")
                        
                        # æ„é€ è¯¦ç»†çš„æç¤ºä¿¡æ¯
                        exist_msg = "âš ï¸ äº‘ä¸‹è½½ä»»åŠ¡å·²å­˜åœ¨\n"
                        exist_msg += f"é”™è¯¯ä¿¡æ¯: {error_msg}\n"
                        if info_hash:
                            exist_msg += f"ä»»åŠ¡Hash: {info_hash[:16]}...\n"
                        if files_list:
                            exist_msg += f"åŒ…å«æ–‡ä»¶: {len(files_list)} ä¸ª\n"
                            # æ˜¾ç¤ºä¸»è¦æ–‡ä»¶åï¼ˆè·³è¿‡å°å›¾ç‰‡ï¼‰
                            main_files = [f for f in files_list if f.get('size', 0) > 10*1024*1024]
                            if main_files:
                                exist_msg += f"ä¸»è¦æ–‡ä»¶: {main_files[0].get('name', 'æœªçŸ¥')[:50]}...\n"
                        exist_msg += f"ä¿å­˜è·¯å¾„: {target_path}\n"
                        exist_msg += "\nâ„¹ï¸ ä»»åŠ¡å·²å­˜åœ¨ï¼Œè¯·åœ¨115ç½‘ç›˜æŸ¥çœ‹ä¸‹è½½è¿›åº¦"
                        
                        # ä»»åŠ¡å·²å­˜åœ¨æ—¶ä¸å¯åŠ¨ç›‘æ§çº¿ç¨‹ï¼Œé¿å…é‡å¤ç›‘æ§
                        # ç”¨æˆ·å¯ä»¥æ‰‹åŠ¨åœ¨115ç½‘ç›˜æŸ¥çœ‹ä»»åŠ¡çŠ¶æ€
                        
                        return True, exist_msg, task_info
                    else:
                        # å…¶ä»–é”™è¯¯
                        logger.error(f"mhnotify: ç¦»çº¿ä¸‹è½½å¤±è´¥ï¼Œå“åº”: {resp}")
                        fail_msg = f"âŒ æ·»åŠ å¤±è´¥\n"
                        fail_msg += f"é”™è¯¯ä¿¡æ¯: {error_msg}\n"
                        fail_msg += f"é”™è¯¯ç : {error_code}"
                        if info_hash:
                            fail_msg += f"\nHash: {info_hash[:16]}..."
                        return False, fail_msg, task_info
                
                # æˆåŠŸæ·»åŠ 
                # å•ä¸ªURLè¿”å›çš„ç»“æ„å¯èƒ½ä¸åŒ
                
                if not task_name:
                    # å°è¯•ä»å…¶ä»–å­—æ®µè·å–
                    task_name = data.get('file_name', '') or data.get('title', '') or 'ä»»åŠ¡å·²æ·»åŠ '
                
                success_msg = f"ä»»åŠ¡å·²æ·»åŠ åˆ°115äº‘ä¸‹è½½\n"
                if task_name:
                    success_msg += f"ä»»åŠ¡åç§°: {task_name}\n"
                success_msg += f"ä¿å­˜è·¯å¾„: {target_path}"
                if info_hash:
                    success_msg += f"\nHash: {info_hash[:16]}..."
                
                logger.info(f"mhnotify: 115ç¦»çº¿ä¸‹è½½ä»»åŠ¡æ·»åŠ æˆåŠŸ: {task_name or info_hash or 'æœªçŸ¥'}")
                
                # å¡«å……ä»»åŠ¡ä¿¡æ¯ï¼Œä¾›æ‰¹é‡å¤„ç†ä½¿ç”¨
                task_info = {
                    "client": client,
                    "info_hash": info_hash,
                    "target_cid": target_cid,
                    "task_name": task_name,
                    "target_path": target_path
                }
                
                # å¦‚æœå¼€å¯äº†å‰”é™¤å°æ–‡ä»¶æˆ–ç§»åŠ¨æ•´ç†åŠŸèƒ½ï¼Œä¸”éœ€è¦å¯åŠ¨ç›‘æ§
                if (self._cloud_download_remove_small_files or self._cloud_download_organize) and info_hash and start_monitor:
                    try:
                        if self._cloud_download_remove_small_files:
                            logger.info(f"mhnotify: äº‘ä¸‹è½½å‰”é™¤å°æ–‡ä»¶å·²å¯ç”¨ï¼Œå°†ç­‰å¾…ä»»åŠ¡å®Œæˆåå¤„ç†...")
                        if self._cloud_download_organize:
                            logger.info(f"mhnotify: äº‘ä¸‹è½½ç§»åŠ¨æ•´ç†å·²å¯ç”¨ï¼Œå°†ç­‰å¾…ä»»åŠ¡å®Œæˆåå¤„ç†...")
                        
                        # å¯åŠ¨å¼‚æ­¥ä»»åŠ¡ç›‘æ§ä¸‹è½½å®Œæˆå¹¶å¤„ç†
                        import threading
                        threading.Thread(
                            target=self._monitor_and_remove_small_files,
                            args=(client, info_hash, target_cid, task_name, target_path),
                            daemon=True
                        ).start()
                    except Exception as e:
                        logger.warning(f"mhnotify: å¯åŠ¨åå¤„ç†ä»»åŠ¡å¤±è´¥: {e}")
                
                return True, success_msg, task_info
            else:
                # å¯èƒ½è¿”å›çš„æ˜¯å…¶ä»–ç±»å‹
                logger.info(f"mhnotify: ç¦»çº¿ä¸‹è½½å“åº”ç±»å‹: {type(resp)}, å†…å®¹: {resp}")
                return True, f"ä»»åŠ¡å·²æäº¤åˆ°115äº‘ä¸‹è½½\nä¿å­˜è·¯å¾„: {target_path}", task_info
            
        except ImportError as e:
            logger.error(f"mhnotify: å¯¼å…¥p115clientå¤±è´¥: {e}")
            return False, f"ä¾èµ–åº“å¯¼å…¥å¤±è´¥: {str(e)}", task_info
        except Exception as e:
            logger.error(f"mhnotify: æ·»åŠ 115ç¦»çº¿ä¸‹è½½ä»»åŠ¡å¤±è´¥: {e}", exc_info=True)
            return False, f"æ·»åŠ å¤±è´¥: {str(e)}", task_info

    def _monitor_batch_downloads(self, tasks: List[Dict[str, Any]]):
        """
        æ‰¹é‡ç›‘æ§å¤šä¸ªç¦»çº¿ä¸‹è½½ä»»åŠ¡ï¼Œç­‰å¾…å…¨éƒ¨å®Œæˆåç»Ÿä¸€æ¸…ç†å’Œæ•´ç†
        å¦‚æœæŸä¸ªä»»åŠ¡10åˆ†é’Ÿå†…ä»åœ¨ä¸‹è½½ä¸­ï¼Œå°†å…¶ç‹¬ç«‹å‡ºå»å•ç‹¬ç›‘æ§
        :param tasks: ä»»åŠ¡ä¿¡æ¯åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« client, info_hash, target_cid, task_name, target_path
        """
        import time
        import threading
        
        if not tasks:
            return
        
        logger.info(f"mhnotify: å¼€å§‹æ‰¹é‡ç›‘æ§ {len(tasks)} ä¸ªç¦»çº¿ä¸‹è½½ä»»åŠ¡")
        
        # ç­‰å¾…15ç§’ï¼Œè®©ä»»åŠ¡è¿›å…¥ä¸‹è½½é˜Ÿåˆ—
        logger.info(f"mhnotify: ç­‰å¾…15ç§’ï¼Œè®©ä»»åŠ¡è¿›å…¥ä¸‹è½½é˜Ÿåˆ—...")
        time.sleep(15)
        
        # ä»»åŠ¡çŠ¶æ€è·Ÿè¸ª
        task_status = {}  # info_hash -> {"completed": bool, "success": bool, "actual_cid": int, "is_directory": bool, "split_out": bool}
        task_first_seen_downloading = {}  # info_hash -> é¦–æ¬¡å‘ç°åœ¨ä¸‹è½½ä¸­çš„æ—¶é—´æˆ³
        
        for task in tasks:
            task_status[task["info_hash"]] = {
                "completed": False,
                "success": False,
                "actual_cid": task["target_cid"],
                "is_directory": False,
                "task_name": task["task_name"],
                "split_out": False  # æ˜¯å¦å·²è¢«ç‹¬ç«‹å‡ºå»
            }
        
        client = tasks[0]["client"]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªä»»åŠ¡çš„client
        target_path = tasks[0]["target_path"]  # å‡è®¾æ‰€æœ‰ä»»åŠ¡ä¿å­˜åˆ°åŒä¸€ç›®å½•
        
        # è¶…æ—¶é…ç½®
        split_timeout = 600  # 10åˆ†é’Ÿåå°†æ…¢ä»»åŠ¡ç‹¬ç«‹å‡ºå»
        check_interval = 30  # æ£€æŸ¥é—´éš”ï¼š30ç§’ï¼ˆä¸ºäº†æ›´å¿«æ£€æµ‹è¶…æ—¶ï¼‰
        max_checks = 1440  # æœ€å¤šæ£€æŸ¥12å°æ—¶ï¼ˆ30ç§’ * 1440 = 12å°æ—¶ï¼‰
        
        # ========== ç¬¬ä¸€é˜¶æ®µï¼šç›‘æ§æ‰€æœ‰ä»»åŠ¡ä¸‹è½½å®Œæˆ ==========
        logger.info(f"mhnotify: ç¬¬ä¸€é˜¶æ®µ - ç›‘æ§æ‰€æœ‰ä»»åŠ¡ä¸‹è½½çŠ¶æ€ï¼ˆ10åˆ†é’Ÿè¶…æ—¶åç‹¬ç«‹æ…¢ä»»åŠ¡ï¼‰...")
        
        for check_round in range(max_checks):
            all_done = True  # æ‰€æœ‰ä»»åŠ¡éƒ½å·²å®Œæˆæˆ–è¢«ç‹¬ç«‹å‡ºå»
            current_time = time.time()
            
            for task in tasks:
                info_hash = task["info_hash"]
                status = task_status[info_hash]
                
                # å·²å®Œæˆæˆ–å·²ç‹¬ç«‹å‡ºå»çš„ä»»åŠ¡è·³è¿‡
                if status["completed"] or status["split_out"]:
                    continue
                
                all_done = False
                
                try:
                    # å…ˆæŸ¥æ­£åœ¨ä¸‹è½½åˆ—è¡¨
                    downloading_task = self._query_downloading_task_by_hash(client, info_hash)
                    
                    if downloading_task and downloading_task.get('status', 0) == 1:
                        # ä»åœ¨ä¸‹è½½ä¸­
                        percent = downloading_task.get('percentDone', 0)
                        
                        # è®°å½•é¦–æ¬¡å‘ç°ä¸‹è½½ä¸­çš„æ—¶é—´
                        if info_hash not in task_first_seen_downloading:
                            task_first_seen_downloading[info_hash] = current_time
                            logger.info(f"mhnotify: ä»»åŠ¡å¼€å§‹ä¸‹è½½: {task['task_name']}")
                        
                        # æ£€æŸ¥æ˜¯å¦è¶…è¿‡10åˆ†é’Ÿ
                        downloading_duration = current_time - task_first_seen_downloading[info_hash]
                        if downloading_duration >= split_timeout:
                            # è¶…è¿‡10åˆ†é’Ÿï¼Œç‹¬ç«‹å‡ºå»å•ç‹¬ç›‘æ§
                            logger.info(f"mhnotify: ä»»åŠ¡ {task['task_name']} ä¸‹è½½è¶…è¿‡10åˆ†é’Ÿï¼ˆ{percent:.1f}%ï¼‰ï¼Œç‹¬ç«‹å‡ºå»å•ç‹¬ç›‘æ§")
                            status["split_out"] = True
                            
                            # å¯åŠ¨ç‹¬ç«‹çš„ç›‘æ§çº¿ç¨‹
                            threading.Thread(
                                target=self._monitor_and_remove_small_files,
                                args=(client, info_hash, task["target_cid"], task["task_name"], task["target_path"]),
                                daemon=True
                            ).start()
                        else:
                            # æ¯2åˆ†é’Ÿè®°å½•ä¸€æ¬¡è¿›åº¦
                            if check_round % 4 == 0:
                                remaining = int((split_timeout - downloading_duration) / 60)
                                logger.info(f"mhnotify: æ­£åœ¨ä¸‹è½½: {task['task_name']} - {percent:.1f}%ï¼ˆ{remaining}åˆ†é’Ÿåç‹¬ç«‹ï¼‰")
                        continue
                    
                    # ä¸åœ¨ä¸‹è½½åˆ—è¡¨ï¼ŒæŸ¥å·²å®Œæˆåˆ—è¡¨
                    current_task = self._query_offline_task_by_hash(client, info_hash)
                    
                    if current_task and isinstance(current_task, dict):
                        task_api_status = current_task.get('status', 0)
                        if task_api_status == 2:
                            # å·²å®Œæˆ
                            status["completed"] = True
                            status["success"] = True
                            actual_cid = current_task.get('file_id', '')
                            if actual_cid:
                                try:
                                    status["actual_cid"] = int(actual_cid)
                                except:
                                    pass
                            file_category = current_task.get('file_category', 1)
                            status["is_directory"] = (file_category == 0)
                            logger.info(f"mhnotify: ä»»åŠ¡å·²å®Œæˆ: {task['task_name']}")
                        elif task_api_status == 1:
                            # å¤±è´¥
                            status["completed"] = True
                            status["success"] = False
                            logger.warning(f"mhnotify: ä»»åŠ¡å¤±è´¥: {task['task_name']}")
                    else:
                        # ä¸¤å¤„éƒ½æ‰¾ä¸åˆ°ï¼Œå¯èƒ½è¢«åˆ é™¤
                        status["completed"] = True
                        status["success"] = False
                        logger.warning(f"mhnotify: ä»»åŠ¡å¯èƒ½å·²è¢«åˆ é™¤: {task['task_name']}")
                        
                except Exception as e:
                    logger.warning(f"mhnotify: æŸ¥è¯¢ä»»åŠ¡ {task['task_name']} å¼‚å¸¸: {e}")
            
            if all_done:
                logger.info(f"mhnotify: æ‰¹é‡ç›‘æ§çš„ä»»åŠ¡å·²å…¨éƒ¨å¤„ç†å®Œæˆ")
                break
            
            time.sleep(check_interval)
        
        # ========== ç¬¬äºŒé˜¶æ®µï¼šç»Ÿè®¡ç»“æœï¼ˆåªç»Ÿè®¡æœªè¢«ç‹¬ç«‹å‡ºå»çš„ä»»åŠ¡ï¼‰ ==========
        batch_tasks = [t for t in tasks if not task_status[t["info_hash"]]["split_out"]]
        success_tasks = [t["info_hash"] for t in batch_tasks if task_status[t["info_hash"]]["success"]]
        failed_tasks = [t["info_hash"] for t in batch_tasks if task_status[t["info_hash"]]["completed"] and not task_status[t["info_hash"]]["success"]]
        split_tasks = [t for t in tasks if task_status[t["info_hash"]]["split_out"]]
        
        logger.info(f"mhnotify: æ‰¹é‡ä»»åŠ¡ç»Ÿè®¡ - æˆåŠŸ: {len(success_tasks)}, å¤±è´¥: {len(failed_tasks)}, ç‹¬ç«‹ç›‘æ§: {len(split_tasks)}")
        
        # å¦‚æœæ²¡æœ‰æˆåŠŸçš„ä»»åŠ¡ï¼Œç›´æ¥å‘é€é€šçŸ¥å¹¶ç»“æŸ
        if not success_tasks:
            if split_tasks:
                # æœ‰ä»»åŠ¡è¢«ç‹¬ç«‹å‡ºå»ï¼Œå‘é€éƒ¨åˆ†é€šçŸ¥
                self._send_batch_cloud_download_notification(
                    tasks=batch_tasks,
                    task_status=task_status,
                    removed_count=0,
                    removed_size_mb=0,
                    split_count=len(split_tasks)
                )
            logger.info(f"mhnotify: æ‰¹é‡ç›‘æ§æ— æˆåŠŸä»»åŠ¡ï¼Œç»“æŸ")
            return
        
        # ========== ç¬¬ä¸‰é˜¶æ®µï¼šç»Ÿä¸€æ¸…ç†å°æ–‡ä»¶ ==========
        total_removed_count = 0
        total_removed_size = 0
        
        if self._cloud_download_remove_small_files and success_tasks:
            logger.info(f"mhnotify: å¼€å§‹ç»Ÿä¸€æ¸…ç†å°æ–‡ä»¶...")
            time.sleep(5)  # ç­‰å¾…æ–‡ä»¶åˆ—è¡¨åŒæ­¥
            
            for info_hash in success_tasks:
                status = task_status[info_hash]
                if status["is_directory"]:
                    try:
                        removed_count, removed_size = self._remove_small_files_in_directory(client, status["actual_cid"])
                        total_removed_count += removed_count
                        total_removed_size += removed_size
                        if removed_count > 0:
                            logger.info(f"mhnotify: ä»»åŠ¡ {status['task_name']} æ¸…ç†äº† {removed_count} ä¸ªå°æ–‡ä»¶")
                    except Exception as e:
                        logger.warning(f"mhnotify: æ¸…ç†ä»»åŠ¡ {status['task_name']} å°æ–‡ä»¶å¼‚å¸¸: {e}")
        
        # ========== ç¬¬å››é˜¶æ®µï¼šç»Ÿä¸€æ‰§è¡Œä¸€æ¬¡ç§»åŠ¨æ•´ç† ==========
        if self._cloud_download_organize and target_path and success_tasks:
            logger.info(f"mhnotify: å¼€å§‹ç»Ÿä¸€ç§»åŠ¨æ•´ç†...")
            try:
                access_token = self._get_mh_access_token()
                if access_token:
                    self._organize_cloud_download(access_token, target_path)
                else:
                    logger.error(f"mhnotify: æ— æ³•è·å–MH access tokenï¼Œè·³è¿‡ç§»åŠ¨æ•´ç†")
            except Exception as e:
                logger.error(f"mhnotify: ç§»åŠ¨æ•´ç†å¼‚å¸¸: {e}")
        
        # ========== ç¬¬äº”é˜¶æ®µï¼šå‘é€æ±‡æ€»é€šçŸ¥ ==========
        self._send_batch_cloud_download_notification(
            tasks=batch_tasks,
            task_status=task_status,
            removed_count=total_removed_count,
            removed_size_mb=total_removed_size / 1024 / 1024,
            split_count=len(split_tasks)
        )
        
        logger.info(f"mhnotify: æ‰¹é‡ç¦»çº¿ä¸‹è½½ç›‘æ§ä»»åŠ¡ç»“æŸ")

    def _send_batch_cloud_download_notification(self, tasks: List[Dict[str, Any]], 
                                                  task_status: Dict[str, Dict],
                                                  removed_count: int, removed_size_mb: float,
                                                  split_count: int = 0):
        """
        å‘é€æ‰¹é‡äº‘ä¸‹è½½å®Œæˆçš„æ±‡æ€»é€šçŸ¥
        :param split_count: è¢«ç‹¬ç«‹å‡ºå»å•ç‹¬ç›‘æ§çš„ä»»åŠ¡æ•°é‡
        """
        try:
            success_count = sum(1 for s in task_status.values() if s.get("success"))
            fail_count = sum(1 for s in task_status.values() if s.get("completed") and not s.get("success") and not s.get("split_out"))
            
            title = f"âœ… 115äº‘ä¸‹è½½æ‰¹é‡ä»»åŠ¡å®Œæˆ"
            if fail_count > 0:
                title = f"âš ï¸ 115äº‘ä¸‹è½½æ‰¹é‡ä»»åŠ¡å®Œæˆï¼ˆ{fail_count}ä¸ªå¤±è´¥ï¼‰"
            
            text_parts = [f"ğŸ“¦ å…± {len(tasks) + split_count} ä¸ªä»»åŠ¡"]
            status_line = f"âœ… æˆåŠŸ: {success_count} | âŒ å¤±è´¥: {fail_count}"
            if split_count > 0:
                status_line += f" | â³ ç‹¬ç«‹ç›‘æ§: {split_count}"
            text_parts.append(status_line)
            
            # åˆ—å‡ºä»»åŠ¡åç§°
            if tasks:
                text_parts.append("")
                for task in tasks:
                    info_hash = task["info_hash"]
                    status = task_status.get(info_hash, {})
                    if status.get("success"):
                        text_parts.append(f"âœ… {task['task_name'][:30]}")
                    elif status.get("split_out"):
                        text_parts.append(f"â³ {task['task_name'][:30]}")
                    else:
                        text_parts.append(f"âŒ {task['task_name'][:30]}")
            
            if split_count > 0:
                text_parts.append("")
                text_parts.append(f"â„¹ï¸ {split_count} ä¸ªæ…¢ä»»åŠ¡å·²ç‹¬ç«‹ç›‘æ§ï¼Œå®Œæˆåå°†å•ç‹¬é€šçŸ¥")
            
            if removed_count > 0:
                text_parts.append("")
                text_parts.append(f"ğŸ§¹ æ¸…ç†å°æ–‡ä»¶: {removed_count} ä¸ª")
                text_parts.append(f"ğŸ’¾ é‡Šæ”¾ç©ºé—´: {removed_size_mb:.2f} MB")
            
            text = "\n".join(text_parts)
            
            self.post_message(
                mtype=None,
                title=title,
                text=text
            )
            logger.info(f"mhnotify: æ‰¹é‡äº‘ä¸‹è½½å®Œæˆé€šçŸ¥å·²å‘é€")
        except Exception as e:
            logger.error(f"mhnotify: å‘é€æ‰¹é‡äº‘ä¸‹è½½é€šçŸ¥å¤±è´¥: {e}", exc_info=True)

    def _monitor_and_remove_small_files(self, client, info_hash: str, target_cid: int, task_name: str, target_path: str = ""):
        """
        ç›‘æ§ç¦»çº¿ä¸‹è½½ä»»åŠ¡å®Œæˆååˆ é™¤å°æ–‡ä»¶
        :param client: P115Clientå®ä¾‹
        :param info_hash: ä»»åŠ¡hash
        :param target_cid: ç›®æ ‡ç›®å½•ID
        :param task_name: ä»»åŠ¡åç§°
        :param target_path: äº‘ä¸‹è½½ç›®æ ‡è·¯å¾„
        """
        try:
            import time
            logger.info(f"mhnotify: å¼€å§‹ç›‘æ§ç¦»çº¿ä¸‹è½½ä»»åŠ¡: {task_name}")
            
            # æ·»åŠ ä»»åŠ¡åç­‰å¾…15ç§’ï¼Œè®©ä»»åŠ¡æœ‰æ—¶é—´å‡ºç°åœ¨ä¸‹è½½åˆ—è¡¨ä¸­
            logger.info(f"mhnotify: ç­‰å¾…15ç§’ï¼Œè®©ä»»åŠ¡è¿›å…¥ä¸‹è½½é˜Ÿåˆ—...")
            time.sleep(15)
            
            # ========== ç¬¬ä¸€é˜¶æ®µï¼šç›‘æ§æ­£åœ¨ä¸‹è½½ ==========
            # ä½¿ç”¨ stat=12 æŸ¥è¯¢æ­£åœ¨ä¸‹è½½çš„ä»»åŠ¡
            logger.info(f"mhnotify: ç¬¬ä¸€é˜¶æ®µ - ç›‘æ§æ­£åœ¨ä¸‹è½½çŠ¶æ€...")
            
            normal_check_interval = 120  # æ­£å¸¸æ£€æŸ¥é—´éš”ï¼š2åˆ†é’Ÿ
            max_downloading_checks = 720  # æœ€å¤šæ£€æŸ¥24å°æ—¶
            
            task_found = False  # æ ‡è®°æ˜¯å¦è‡³å°‘æ‰¾åˆ°è¿‡ä¸€æ¬¡ä»»åŠ¡
            not_found_count = 0  # è¿ç»­æœªæ‰¾åˆ°ä»»åŠ¡çš„æ¬¡æ•°
            max_not_found_after_found = 3  # ä»»åŠ¡å‡ºç°åæœ€å¤šå®¹å¿3æ¬¡æœªæ‰¾åˆ°æ‰è®¤ä¸ºå·²å®Œæˆ
            
            for i in range(max_downloading_checks):
                try:
                    # æŸ¥è¯¢æ­£åœ¨ä¸‹è½½çš„ä»»åŠ¡ï¼ˆstat=12ï¼‰
                    downloading_task = self._query_downloading_task_by_hash(client, info_hash)
                    
                    if downloading_task:
                        # æ‰¾åˆ°ä»»åŠ¡äº†
                        task_found = True
                        not_found_count = 0  # é‡ç½®æœªæ‰¾åˆ°è®¡æ•°
                        
                        percent = downloading_task.get('percentDone', 0)
                        status = downloading_task.get('status', 0)
                        
                        # status=1 è¡¨ç¤ºæ­£åœ¨ä¸‹è½½
                        if status == 1:
                            # ä½¿ç”¨æ­£å¸¸æ£€æŸ¥é—´éš”ï¼ˆ2åˆ†é’Ÿï¼‰
                            if i % 5 == 0:  # æ¯10åˆ†é’Ÿè®°å½•ä¸€æ¬¡è¿›åº¦
                                logger.info(f"mhnotify: æ­£åœ¨ä¸‹è½½: {task_name} - {percent:.1f}%")
                            time.sleep(normal_check_interval)
                            continue
                        else:
                            # status != 1ï¼Œå¯èƒ½ä¸‹è½½å®Œæˆæˆ–å¤±è´¥ï¼Œè·³å‡ºå¾ªç¯
                            logger.info(f"mhnotify: ä»»åŠ¡çŠ¶æ€å˜åŒ– (status={status})ï¼Œè¿›å…¥ä¸‹ä¸€é˜¶æ®µ...")
                            break
                    else:
                        # æ­£åœ¨ä¸‹è½½åˆ—è¡¨ä¸­æœªæ‰¾åˆ°ä»»åŠ¡
                        not_found_count += 1
                        
                        if not task_found:
                            # ä»»åŠ¡ä»æœªåœ¨ä¸‹è½½åˆ—è¡¨ä¸­æ‰¾åˆ°è¿‡ï¼Œç›´æ¥è¿›å…¥ç¬¬äºŒé˜¶æ®µæŸ¥æ‰¾å·²å®Œæˆä»»åŠ¡
                            logger.info(f"mhnotify: æœªåœ¨ä¸‹è½½åˆ—è¡¨ä¸­æ‰¾åˆ°ä»»åŠ¡ï¼Œè¿›å…¥å·²å®Œæˆæ£€æŸ¥é˜¶æ®µ...")
                            break
                        else:
                            # ä»»åŠ¡ä¹‹å‰æ‰¾åˆ°è¿‡ï¼Œç°åœ¨æ‰¾ä¸åˆ°äº†
                            if not_found_count >= max_not_found_after_found:
                                # è¿ç»­å¤šæ¬¡æ‰¾ä¸åˆ°ï¼Œè¯´æ˜å·²å®Œæˆæˆ–å¤±è´¥ï¼Œè¿›å…¥ç¬¬äºŒé˜¶æ®µ
                                logger.info(f"mhnotify: ä»»åŠ¡å·²ä¸åœ¨ä¸‹è½½åˆ—è¡¨ä¸­ï¼Œè¿›å…¥å·²å®Œæˆæ£€æŸ¥é˜¶æ®µ...")
                                break
                            else:
                                logger.debug(f"mhnotify: æš‚æ—¶æœªæ‰¾åˆ°ä»»åŠ¡ ({not_found_count}/{max_not_found_after_found})ï¼Œç»§ç»­ç­‰å¾…...")
                                time.sleep(normal_check_interval)
                                continue
                        
                except Exception as e:
                    logger.warning(f"mhnotify: æŸ¥è¯¢æ­£åœ¨ä¸‹è½½ä»»åŠ¡å¼‚å¸¸: {e}")
                    # å‡ºç°å¼‚å¸¸ç›´æ¥è¿›å…¥ç¬¬äºŒé˜¶æ®µ
                    logger.info(f"mhnotify: æŸ¥è¯¢å¼‚å¸¸ï¼Œè¿›å…¥å·²å®Œæˆæ£€æŸ¥é˜¶æ®µ...")
                    break
            
            # ========== ç¬¬äºŒé˜¶æ®µï¼šæ£€æŸ¥å·²å®Œæˆä»»åŠ¡ ==========
            logger.info(f"mhnotify: ç¬¬äºŒé˜¶æ®µ - æ£€æŸ¥å·²å®Œæˆä»»åŠ¡...")
            
            # ç­‰å¾…3ç§’ï¼Œç¡®ä¿ä»»åŠ¡çŠ¶æ€åŒæ­¥
            time.sleep(3)
            
            # ä½¿ç”¨ stat=11 æŸ¥è¯¢æ‰€æœ‰ä»»åŠ¡ï¼ˆåŒ…æ‹¬å·²å®Œæˆï¼‰
            max_completed_checks = 5  # æœ€å¤šæ£€æŸ¥5æ¬¡
            completed_check_interval = 10  # 10ç§’
            consecutive_failures = 0
            max_consecutive_failures = 3
            
            for i in range(max_completed_checks):
                try:
                    # ä½¿ç”¨115 Web APIæŸ¥è¯¢ç¦»çº¿ä»»åŠ¡åˆ—è¡¨ï¼ˆstat=11 æŸ¥è¯¢æ‰€æœ‰ä»»åŠ¡ï¼‰
                    current_task = self._query_offline_task_by_hash(client, info_hash)
                    
                    # ç±»å‹æ£€æŸ¥ï¼šç¡®ä¿è¿”å›çš„æ˜¯å­—å…¸
                    if current_task and not isinstance(current_task, dict):
                        logger.warning(f"mhnotify: æŸ¥è¯¢ä»»åŠ¡è¿”å›ç±»å‹é”™è¯¯: {type(current_task)}")
                        current_task = None
                    
                    if not current_task:
                        # æœªæ‰¾åˆ°ä»»åŠ¡
                        consecutive_failures += 1
                        logger.warning(f"mhnotify: æœªæ‰¾åˆ°å·²å®Œæˆä»»åŠ¡ {info_hash[:16]}... (å°è¯• {consecutive_failures}/{max_consecutive_failures})")
                        
                        if consecutive_failures >= max_consecutive_failures:
                            # è¿ç»­å¤šæ¬¡æœªæ‰¾åˆ°ï¼Œè¿›ä¸€æ­¥æ£€æŸ¥å¤±è´¥åˆ—è¡¨
                            failed_task = self._query_offline_failed_task_by_hash(client, info_hash)
                            if failed_task:
                                logger.error(f"mhnotify: ä»»åŠ¡ {info_hash[:16]}... åœ¨å¤±è´¥åˆ—è¡¨ä¸­å­˜åœ¨ï¼Œå‘é€å¤±è´¥é€šçŸ¥")
                                self._send_cloud_download_failed_notification(task_name)
                            else:
                                logger.error(f"mhnotify: æœªæ‰¾åˆ°äº‘ä¸‹è½½ä»»åŠ¡ï¼Œå¯èƒ½å·²è¢«åˆ é™¤")
                                self._send_cloud_download_deleted_notification(task_name)
                                try:
                                    mapping = self.get_data(self._ASSIST_CLOUD_MAP_KEY) or {}
                                    info = mapping.get(info_hash)
                                    if info:
                                        sid = info.get("sid")
                                        with SessionFactory() as db:
                                            SubscribeOper(db=db).update(int(sid), {"state": "R", "sites": []})
                                        mapping.pop(info_hash, None)
                                        self.save_data(self._ASSIST_CLOUD_MAP_KEY, mapping)
                                        logger.info(f"mhnotify: äº‘ä¸‹è½½ä»»åŠ¡å·²åˆ é™¤ï¼Œå·²æ¢å¤MPè®¢é˜…å¯ç”¨ sid={sid}")
                                except Exception:
                                    pass
                            break
                        
                        time.sleep(completed_check_interval)
                        continue
                    
                    # æŸ¥è¯¢æˆåŠŸï¼Œé‡ç½®å¤±è´¥è®¡æ•°
                    consecutive_failures = 0
                    
                    # æ£€æŸ¥ä»»åŠ¡çŠ¶æ€ï¼š2=å·²å®Œæˆ, 1=å¤±è´¥, 0=ä¸‹è½½ä¸­
                    status = current_task.get('status', 0)
                    if status == 2:
                        logger.info(f"mhnotify: ç¦»çº¿ä¸‹è½½ä»»åŠ¡å·²å®Œæˆ: {task_name}")
                        
                        # ä»ä»»åŠ¡ä¿¡æ¯ä¸­è·å–å®é™…æ–‡ä»¶/æ–‡ä»¶å¤¹IDï¼ˆfile_idï¼‰
                        # file_id æ˜¯ä¸‹è½½å®Œæˆåçš„æ–‡ä»¶æˆ–æ–‡ä»¶å¤¹çš„å®é™…ID
                        actual_cid = current_task.get('file_id', '')
                        if actual_cid:
                            try:
                                actual_cid = int(actual_cid)
                            except:
                                actual_cid = target_cid
                        else:
                            actual_cid = target_cid
                        
                        logger.info(f"mhnotify: å®é™…æ–‡ä»¶/æ–‡ä»¶å¤¹ID: {actual_cid}")
                        
                        # æ£€æŸ¥ file_categoryï¼Œåªæœ‰æ–‡ä»¶å¤¹æ‰éœ€è¦æ¸…ç†å°æ–‡ä»¶
                        file_category = current_task.get('file_category', 1)
                        is_directory = (file_category == 0)
                        
                        # è®°å½•æ¸…ç†ç»“æœç”¨äºé€šçŸ¥
                        removed_count = 0
                        removed_size_mb = 0.0
                        
                        # å¦‚æœå¼€å¯äº†å‰”é™¤å°æ–‡ä»¶ï¼Œå…ˆåˆ é™¤å°æ–‡ä»¶
                        if self._cloud_download_remove_small_files:
                            if is_directory:
                                logger.info(f"mhnotify: æ£€æµ‹åˆ°æ–‡ä»¶å¤¹ï¼Œå¼€å§‹æ¸…ç†å°æ–‡ä»¶...")
                                time.sleep(5)  # ç­‰å¾…5ç§’ç¡®ä¿æ–‡ä»¶åˆ—è¡¨åŒæ­¥
                                removed_count, removed_size = self._remove_small_files_in_directory(client, actual_cid)
                                removed_size_mb = removed_size / 1024 / 1024
                            else:
                                logger.info(f"mhnotify: æ£€æµ‹åˆ°å•ä¸ªæ–‡ä»¶ï¼Œè·³è¿‡å°æ–‡ä»¶æ¸…ç†")
                        
                        # å¦‚æœå¼€å¯äº†ç§»åŠ¨æ•´ç†ï¼Œæ‰§è¡Œç§»åŠ¨æ•´ç†
                        if self._cloud_download_organize and target_path:
                            logger.info(f"mhnotify: å¼€å§‹ç§»åŠ¨æ•´ç†...")
                            # è·å–MH access token
                            access_token = self._get_mh_access_token()
                            if access_token:
                                self._organize_cloud_download(access_token, target_path)
                            else:
                                logger.error(f"mhnotify: æ— æ³•è·å–MH access tokenï¼Œè·³è¿‡ç§»åŠ¨æ•´ç†")
                        
                        # å‘é€äº‘ä¸‹è½½å®Œæˆé€šçŸ¥
                        self._send_cloud_download_notification(task_name, removed_count, removed_size_mb)
                        
                        try:
                            mapping = self.get_data(self._ASSIST_CLOUD_MAP_KEY) or {}
                            info = mapping.get(info_hash)
                            if info:
                                sid = info.get("sid")
                                mh_uuid = info.get("mh_uuid")
                                # ä¸åœ¨æ­¤å¤„åˆ é™¤MHï¼Œæ”¹ä¸ºå®ŒæˆMPè®¢é˜…åç”± SubscribeComplete äº‹ä»¶åˆ é™¤MH
                                with SessionFactory() as db:
                                    sub = SubscribeOper(db=db).get(int(sid))
                                if sub:
                                    self.__finish_mp_subscribe(sub)
                                mapping.pop(info_hash, None)
                                self.save_data(self._ASSIST_CLOUD_MAP_KEY, mapping)
                                logger.info(f"mhnotify: äº‘ä¸‹è½½è¾…åŠ©å®Œæˆï¼Œå·²å®ŒæˆMPè®¢é˜…ï¼ŒMHåˆ é™¤ç”±äº‹ä»¶è§¦å‘ sid={sid}")
                        except Exception:
                            pass
                        
                        break
                    elif status == 1:
                        logger.warning(f"mhnotify: ç¦»çº¿ä¸‹è½½ä»»åŠ¡å¤±è´¥: {task_name}")
                        self._send_cloud_download_failed_notification(task_name)
                        try:
                            mapping = self.get_data(self._ASSIST_CLOUD_MAP_KEY) or {}
                            info = mapping.get(info_hash)
                            if info:
                                sid = info.get("sid")
                                with SessionFactory() as db:
                                    SubscribeOper(db=db).update(int(sid), {"state": "R", "sites": []})
                                mapping.pop(info_hash, None)
                                self.save_data(self._ASSIST_CLOUD_MAP_KEY, mapping)
                                logger.info(f"mhnotify: äº‘ä¸‹è½½è¾…åŠ©å¤±è´¥ï¼Œå·²æ¢å¤MPè®¢é˜…å¯ç”¨ sid={sid}")
                        except Exception:
                            pass
                        break
                    else:
                        # status ä¸ä¸º 2 ä¹Ÿä¸ä¸º 1ï¼Œç»§ç»­ç­‰å¾…
                        logger.info(f"mhnotify: ä»»åŠ¡çŠ¶æ€: {status}ï¼Œç»§ç»­ç­‰å¾…...")
                        time.sleep(completed_check_interval)
                        
                except Exception as e:
                    consecutive_failures += 1
                    logger.warning(f"mhnotify: æ£€æŸ¥å·²å®Œæˆä»»åŠ¡å¼‚å¸¸ ({consecutive_failures}/{max_consecutive_failures}): {e}")
                    
                    if consecutive_failures >= max_consecutive_failures:
                        logger.error(f"mhnotify: è¿ç»­{max_consecutive_failures}æ¬¡æ£€æŸ¥å¤±è´¥ï¼Œåœæ­¢ç›‘æ§ä»»åŠ¡: {task_name}")
                        break
                    
                    time.sleep(completed_check_interval)
            
            logger.info(f"mhnotify: ç¦»çº¿ä¸‹è½½ç›‘æ§ä»»åŠ¡ç»“æŸ: {task_name} (Hash: {info_hash[:16]}...)")
            
        except Exception as e:
            logger.error(f"mhnotify: ç›‘æ§ç¦»çº¿ä¸‹è½½ä»»åŠ¡å¼‚å¸¸: {e}", exc_info=True)
    
    def _query_offline_task_by_hash(self, client, info_hash: str) -> Optional[Dict[str, Any]]:
        """
        ä½¿ç”¨115 Web APIæŸ¥è¯¢ç¦»çº¿ä»»åŠ¡ï¼ˆé€šè¿‡info_hashåŒ¹é…ï¼‰
        :param client: P115Clientå®ä¾‹
        :param info_hash: ä»»åŠ¡hash
        :return: ä»»åŠ¡ä¿¡æ¯å­—å…¸æˆ–None
        """
        try:
            # æ„é€ è¯·æ±‚å‚æ•°
            # å‚è€ƒ 115-ol-list.txtï¼Œéœ€è¦ page, stat, uid, sign, time å‚æ•°
            import time as time_module
            import hashlib
            
            # è·å–ç”¨æˆ·IDï¼ˆä»cookieæˆ–clientä¸­è·å–ï¼‰
            uid = None
            try:
                # å°è¯•ä»clientè·å–ç”¨æˆ·ä¿¡æ¯
                user_info = client.fs_userinfo()
                if user_info and isinstance(user_info, dict):
                    uid = user_info.get('user_id')
            except:
                pass
            
            if not uid:
                # ä»cookieä¸­è§£æUID
                cookie_dict = {}
                for item in self._p115_cookie.split(';'):
                    item = item.strip()
                    if '=' in item:
                        k, v = item.split('=', 1)
                        cookie_dict[k.strip()] = v.strip()
                uid_str = cookie_dict.get('UID', '')
                if uid_str and '_' in uid_str:
                    uid = uid_str.split('_')[0]
            
            if not uid:
                logger.warning(f"mhnotify: æ— æ³•è·å–115ç”¨æˆ·ID")
                return None
            
            # æ„é€ ç­¾åï¼ˆå‚è€ƒ115-ol-list APIï¼‰
            timestamp = int(time_module.time())
            # ç­¾åç®—æ³•ï¼šmd5(uid + time)ï¼Œå®é™…ç®—æ³•å¯èƒ½ä¸åŒï¼Œè¿™é‡Œå…ˆå°è¯•ç®€å•æ–¹å¼
            sign_str = f"{uid}{timestamp}"
            sign = hashlib.md5(sign_str.encode()).hexdigest()
            
            # è°ƒç”¨ç¦»çº¿ä»»åŠ¡åˆ—è¡¨API
            # stat=11è¡¨ç¤ºæŸ¥è¯¢æ‰€æœ‰ä»»åŠ¡ï¼ˆåŒ…æ‹¬å·²å®Œæˆï¼‰
            url = "https://115.com/web/lixian/?ct=lixian&ac=task_lists"
            params = {
                'page': 1,
                'stat': 11,  # 11=æ‰€æœ‰ä»»åŠ¡
                'uid': uid,
                'sign': sign,
                'time': timestamp
            }
            
            # ä½¿ç”¨RequestUtilså‘é€è¯·æ±‚
            headers = {
                "Accept": "application/json, text/javascript, */*; q=0.01",
                "Cookie": self._p115_cookie,
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
            }
            
            response = RequestUtils(headers=headers).post_res(url, data=params)
            if not response or response.status_code != 200:
                logger.debug(f"mhnotify: æŸ¥è¯¢ç¦»çº¿ä»»åŠ¡åˆ—è¡¨å¤±è´¥: {response.status_code if response else 'No response'}")
                return None
            
            result = response.json()
            if not result or not result.get('state'):
                logger.debug(f"mhnotify: ç¦»çº¿ä»»åŠ¡åˆ—è¡¨å“åº”å¼‚å¸¸: {result}")
                return None
            
            # æŸ¥æ‰¾åŒ¹é…çš„ä»»åŠ¡
            tasks = result.get('tasks', [])
            for task in tasks:
                if task.get('info_hash', '').lower() == info_hash.lower():
                    return task
            
            return None
        
        except Exception as e:
            logger.debug(f"mhnotify: æŸ¥è¯¢ç¦»çº¿ä»»åŠ¡å¼‚å¸¸: {e}")
            return None

    def _query_offline_failed_task_by_hash(self, client, info_hash: str) -> Optional[Dict[str, Any]]:
        try:
            import time as time_module
            import hashlib
            uid = self._get_115_uid()
            if not uid:
                logger.warning(f"mhnotify: æ— æ³•è·å–115ç”¨æˆ·ID")
                return None
            timestamp = int(time_module.time())
            sign = hashlib.md5(f"{uid}{timestamp}".encode()).hexdigest()
            url = "https://115.com/web/lixian/?ct=lixian&ac=task_lists"
            params = {
                'page': 1,
                'stat': 1,
                'uid': uid,
                'sign': sign,
                'time': timestamp
            }
            headers = {
                "Accept": "application/json, text/javascript, */*; q=0.01",
                "Cookie": self._p115_cookie,
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
            }
            response = RequestUtils(headers=headers).post_res(url, data=params)
            if not response or response.status_code != 200:
                logger.debug(f"mhnotify: æŸ¥è¯¢å¤±è´¥ä»»åŠ¡åˆ—è¡¨å¤±è´¥: {response.status_code if response else 'No response'}")
                return None
            result = response.json()
            if not result or not result.get('state'):
                logger.debug(f"mhnotify: å¤±è´¥ä»»åŠ¡åˆ—è¡¨å“åº”å¼‚å¸¸: {result}")
                return None
            tasks = result.get('tasks', [])
            for task in tasks:
                if task.get('info_hash', '').lower() == info_hash.lower():
                    return task
            return None
        except Exception as e:
            logger.debug(f"mhnotify: æŸ¥è¯¢å¤±è´¥ä»»åŠ¡å¼‚å¸¸: {e}")
            return None

    def _remove_small_files_in_directory(self, client, cid: int) -> Tuple[int, int]:
        """
        åˆ é™¤ç›®å½•ä¸­å°äº10MBçš„æ–‡ä»¶ï¼ˆé€’å½’éå†å­ç›®å½•ï¼‰
        :param client: P115Clientå®ä¾‹
        :param cid: ç›®å½•ID (æ–‡ä»¶å¤¹çš„file_id)
        :return: (åˆ é™¤æ–‡ä»¶æ•°é‡, åˆ é™¤æ–‡ä»¶æ€»å¤§å°å­—èŠ‚æ•°)
        """
        try:
            logger.info(f"mhnotify: å¼€å§‹é€’å½’æ¸…ç†å°æ–‡ä»¶ï¼Œæ ¹ç›®å½•cid={cid}")
            
            min_size = 10 * 1024 * 1024  # 10MB
            removed_count = 0
            removed_size = 0
            
            # ä½¿ç”¨ p115client.tool.iterdir çš„ iter_files é€’å½’éå†æ‰€æœ‰æ–‡ä»¶
            try:
                from p115client.tool.iterdir import iter_files  # type: ignore
                logger.info("mhnotify: ä½¿ç”¨ iter_files é€’å½’éå†ç›®å½•...")
                
                # iter_files ä¼šé€’å½’è¿”å›æ‰€æœ‰æ–‡ä»¶ï¼ˆä¸å«ç›®å½•ï¼‰
                for attr in iter_files(client, cid):
                    try:
                        # attr æ˜¯ä¸€ä¸ª dictï¼ŒåŒ…å« id, parent_id, name, size, is_dir ç­‰
                        if not isinstance(attr, dict):
                            continue
                        
                        # iter_files åªè¿”å›æ–‡ä»¶ï¼Œä½†è¿˜æ˜¯æ£€æŸ¥ä¸€ä¸‹
                        is_dir = attr.get('is_dir', False)
                        if is_dir:
                            continue
                        
                        file_id = attr.get('id') or attr.get('fid') or attr.get('file_id')
                        file_name = attr.get('name') or attr.get('n') or attr.get('fn') or ''
                        file_size = attr.get('size') or attr.get('fs') or attr.get('s') or 0
                        
                        if isinstance(file_size, str):
                            try:
                                file_size = int(file_size)
                            except:
                                file_size = 0
                        
                        if not file_id:
                            logger.debug(f"mhnotify: æ–‡ä»¶æ— IDï¼Œè·³è¿‡: {file_name}")
                            continue
                        
                        logger.debug(f"mhnotify: æ£€æŸ¥æ–‡ä»¶: {file_name}, å¤§å°: {file_size/1024/1024:.2f}MB")
                        
                        # å¦‚æœæ–‡ä»¶å°äº10MBï¼Œåˆ é™¤
                        if file_size < min_size:
                            try:
                                logger.info(f"mhnotify: å‡†å¤‡åˆ é™¤å°æ–‡ä»¶: {file_name} ({file_size/1024/1024:.2f}MB)")
                                client.fs_delete(file_id)
                                removed_count += 1
                                removed_size += file_size
                                logger.info(f"mhnotify: æˆåŠŸåˆ é™¤å°æ–‡ä»¶: {file_name}")
                            except Exception as e:
                                logger.warning(f"mhnotify: åˆ é™¤æ–‡ä»¶å¤±è´¥ {file_name}: {e}")
                    except Exception as e:
                        logger.debug(f"mhnotify: å¤„ç†æ–‡ä»¶é¡¹å¼‚å¸¸: {e}")
                        continue
                        
            except ImportError:
                logger.warning("mhnotify: iter_files å¯¼å…¥å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ...")
                # å¤‡ç”¨æ–¹æ¡ˆï¼šæ‰‹åŠ¨é€’å½’éå†
                removed_count, removed_size = self._remove_small_files_recursive(client, cid, min_size)
            except Exception as e:
                logger.warning(f"mhnotify: iter_files è°ƒç”¨å¤±è´¥: {e}ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ...")
                removed_count, removed_size = self._remove_small_files_recursive(client, cid, min_size)
            
            if removed_count > 0:
                logger.info(f"mhnotify: äº‘ä¸‹è½½å°æ–‡ä»¶æ¸…ç†å®Œæˆï¼Œå…±åˆ é™¤ {removed_count} ä¸ªæ–‡ä»¶ï¼Œé‡Šæ”¾ç©ºé—´ {removed_size/1024/1024:.2f}MB")
            else:
                logger.info(f"mhnotify: äº‘ä¸‹è½½ç›®å½•ä¸­æ²¡æœ‰å°äº10MBçš„æ–‡ä»¶éœ€è¦åˆ é™¤")
            
            return removed_count, removed_size
                
        except Exception as e:
            logger.error(f"mhnotify: åˆ é™¤å°æ–‡ä»¶å¼‚å¸¸: {e}", exc_info=True)
            return 0, 0

    def _remove_small_files_recursive(self, client, cid: int, min_size: int) -> Tuple[int, int]:
        """
        å¤‡ç”¨æ–¹æ¡ˆï¼šæ‰‹åŠ¨é€’å½’éå†ç›®å½•åˆ é™¤å°æ–‡ä»¶
        :param client: P115Clientå®ä¾‹
        :param cid: ç›®å½•ID
        :param min_size: æœ€å°æ–‡ä»¶å¤§å°é˜ˆå€¼ï¼ˆå­—èŠ‚ï¼‰
        :return: (åˆ é™¤æ•°é‡, åˆ é™¤å¤§å°)
        """
        removed_count = 0
        removed_size = 0
        
        # ä½¿ç”¨æ ˆå®ç°éé€’å½’éå†ï¼Œé¿å…æ·±å±‚é€’å½’å¯¼è‡´æ ˆæº¢å‡º
        dir_stack = [cid]
        
        while dir_stack:
            current_cid = dir_stack.pop()
            logger.debug(f"mhnotify: éå†ç›®å½• cid={current_cid}")
            
            offset = 0
            limit = 1000
            
            while True:
                try:
                    # è°ƒç”¨ fs_files è·å–ç›®å½•å†…å®¹
                    resp = client.fs_files(cid=current_cid, limit=limit, offset=offset)
                except Exception as e:
                    logger.warning(f"mhnotify: fs_files è°ƒç”¨å¤±è´¥ (cid={current_cid}): {e}")
                    break
                
                # å¤„ç†å“åº”
                items = []
                if isinstance(resp, dict):
                    items = resp.get('data', []) or resp.get('files', [])
                elif hasattr(resp, '__iter__'):
                    try:
                        items = list(resp)
                    except:
                        break
                
                if not items:
                    break
                
                for item in items:
                    if not isinstance(item, dict):
                        continue
                    
                    # åˆ¤æ–­æ˜¯æ–‡ä»¶è¿˜æ˜¯ç›®å½•
                    # æ ¹æ® p115client çš„é€»è¾‘ï¼š
                    # - å¦‚æœæœ‰ 'n' å­—æ®µ: æ²¡æœ‰ 'fid' çš„æ˜¯ç›®å½•
                    # - å¦‚æœæœ‰ 'fn' å­—æ®µ: fc == "0" æ˜¯ç›®å½•ï¼Œfc == "1" æ˜¯æ–‡ä»¶
                    is_dir = False
                    if 'n' in item:
                        # æ–°æ ¼å¼ï¼šæ²¡æœ‰ fid çš„æ˜¯ç›®å½•
                        is_dir = 'fid' not in item
                    elif 'fn' in item:
                        # è€æ ¼å¼ï¼šfc å­—æ®µ
                        fc = item.get('fc')
                        is_dir = (fc == '0' or fc == 0)
                    else:
                        # å…¶ä»–æ ¼å¼ï¼šæ£€æŸ¥ file_category
                        fc = item.get('file_category')
                        is_dir = (fc == 0 or fc == '0')
                    
                    if is_dir:
                        # æ˜¯ç›®å½•ï¼Œè·å–ç›®å½•IDå¹¶åŠ å…¥æ ˆ
                        sub_cid = item.get('cid') or item.get('id') or item.get('category_id')
                        if sub_cid:
                            try:
                                sub_cid = int(sub_cid)
                                dir_stack.append(sub_cid)
                                dir_name = item.get('n') or item.get('fn') or item.get('name') or item.get('category_name') or ''
                                logger.debug(f"mhnotify: å‘ç°å­ç›®å½•: {dir_name} (cid={sub_cid})")
                            except:
                                pass
                    else:
                        # æ˜¯æ–‡ä»¶ï¼Œæ£€æŸ¥å¤§å°
                        file_id = item.get('fid') or item.get('file_id') or item.get('id')
                        file_name = item.get('n') or item.get('fn') or item.get('name') or item.get('file_name') or ''
                        file_size = item.get('s') or item.get('fs') or item.get('size') or item.get('file_size') or 0
                        
                        if isinstance(file_size, str):
                            try:
                                file_size = int(file_size)
                            except:
                                file_size = 0
                        
                        if not file_id:
                            continue
                        
                        logger.debug(f"mhnotify: æ£€æŸ¥æ–‡ä»¶: {file_name}, å¤§å°: {file_size/1024/1024:.2f}MB")
                        
                        if file_size < min_size:
                            try:
                                logger.info(f"mhnotify: å‡†å¤‡åˆ é™¤å°æ–‡ä»¶: {file_name} ({file_size/1024/1024:.2f}MB)")
                                client.fs_delete(file_id)
                                removed_count += 1
                                removed_size += file_size
                                logger.info(f"mhnotify: æˆåŠŸåˆ é™¤å°æ–‡ä»¶: {file_name}")
                            except Exception as e:
                                logger.warning(f"mhnotify: åˆ é™¤æ–‡ä»¶å¤±è´¥ {file_name}: {e}")
                
                if len(items) < limit:
                    break
                offset += limit
        
        return removed_count, removed_size

    def _send_cloud_download_notification(self, task_name: str, removed_count: int, removed_size_mb: float):
        """
        å‘é€äº‘ä¸‹è½½å®Œæˆé€šçŸ¥
        :param task_name: ä»»åŠ¡åç§°
        :param removed_count: åˆ é™¤çš„å°æ–‡ä»¶æ•°é‡
        :param removed_size_mb: åˆ é™¤çš„æ–‡ä»¶æ€»å¤§å°(MB)
        """
        try:
            # æ„å»ºé€šçŸ¥æ¶ˆæ¯
            title = "âœ… 115äº‘ä¸‹è½½å®Œæˆ"
            text_parts = [f"ğŸ“¦ ä»»åŠ¡: {task_name}"]
            
            if removed_count > 0:
                text_parts.append(f"ğŸ§¹ æ¸…ç†å°æ–‡ä»¶: {removed_count} ä¸ª")
                text_parts.append(f"ğŸ’¾ é‡Šæ”¾ç©ºé—´: {removed_size_mb:.2f} MB")
            
            text = "\n".join(text_parts)
            
            # å‘é€é€šçŸ¥
            self.post_message(
                mtype=None,
                title=title,
                text=text
            )
            logger.info(f"mhnotify: äº‘ä¸‹è½½å®Œæˆé€šçŸ¥å·²å‘é€: {task_name}")
        except Exception as e:
            logger.error(f"mhnotify: å‘é€äº‘ä¸‹è½½é€šçŸ¥å¤±è´¥: {e}", exc_info=True)

    def _get_mh_access_token(self) -> Optional[str]:
        """
        è·å–MH access token
        :return: access tokenæˆ–None
        """
        try:
            login_url = f"{self._mh_domain}/api/v1/auth/login"
            login_payload = {
                "username": self._mh_username,
                "password": self._mh_password
            }
            headers = {
                "Accept": "application/json, text/plain, */*",
                "Content-Type": "application/json;charset=UTF-8",
                "Origin": self._mh_domain,
                "Accept-Language": "zh-CN",
                "User-Agent": "MoviePilot/Plugin MHNotify"
            }
            login_res = RequestUtils(headers=headers).post(login_url, json=login_payload)
            if not login_res or login_res.status_code != 200:
                logger.error(f"mhnotify: MHç™»å½•å¤±è´¥")
                return None
            
            try:
                login_data = login_res.json()
                access_token = login_data.get("data", {}).get("access_token")
                return access_token
            except Exception:
                logger.error(f"mhnotify: è§£æMHç™»å½•å“åº”å¤±è´¥")
                return None
        except Exception as e:
            logger.error(f"mhnotify: è·å–MH access tokenå¼‚å¸¸: {e}")
            return None

    def _organize_cloud_download(self, access_token: str, target_path: str):
        """
        äº‘ä¸‹è½½å®Œæˆåç§»åŠ¨åˆ°MHé»˜è®¤ç›®å½•å¹¶æ•´ç†
        :param access_token: MH access token
        :param target_path: äº‘ä¸‹è½½ç›®æ ‡è·¯å¾„
        """
        try:
            logger.info(f"mhnotify: å¼€å§‹äº‘ä¸‹è½½ç§»åŠ¨æ•´ç†æµç¨‹ï¼Œç›®æ ‡è·¯å¾„: {target_path}")
            
            # 1. è·å–115ç½‘ç›˜è´¦æˆ·ä¿¡æ¯
            cloud_url = f"{self._mh_domain}/api/v1/cloud-accounts?active_only=true"
            headers = {
                "Accept": "application/json, text/plain, */*",
                "Authorization": f"Bearer {access_token}",
                "User-Agent": "MoviePilot/Plugin MHNotify",
                "Accept-Language": "zh-CN"
            }
            
            logger.info(f"mhnotify: æ­£åœ¨è·å–äº‘è´¦æˆ·åˆ—è¡¨...")
            cloud_res = RequestUtils(headers=headers).get_res(cloud_url)
            
            if not cloud_res:
                logger.error(f"mhnotify: è·å–äº‘è´¦æˆ·åˆ—è¡¨å¤±è´¥ - å“åº”ä¸ºç©º")
                return
            
            if cloud_res.status_code != 200:
                logger.error(f"mhnotify: è·å–äº‘è´¦æˆ·åˆ—è¡¨å¤±è´¥ - çŠ¶æ€ç : {cloud_res.status_code}, å“åº”: {cloud_res.text[:500]}")
                return
            
            try:
                cloud_data = cloud_res.json()
                logger.debug(f"mhnotify: äº‘è´¦æˆ·å“åº”æ•°æ®: {cloud_data}")
            except Exception as e:
                logger.error(f"mhnotify: è§£æäº‘è´¦æˆ·å“åº”å¤±è´¥ - {e}, åŸå§‹å“åº”: {cloud_res.text[:500]}")
                return
            
            # æŸ¥æ‰¾ç¬¬ä¸€ä¸ª115ç½‘ç›˜è´¦æˆ·
            accounts = cloud_data.get("data", {}).get("accounts", [])
            logger.info(f"mhnotify: æ‰¾åˆ° {len(accounts)} ä¸ªäº‘è´¦æˆ·")
            
            drive115_account = None
            for account in accounts:
                account_type = account.get("cloud_type")
                account_name = account.get("name")
                logger.debug(f"mhnotify: æ£€æŸ¥è´¦æˆ·: {account_name} (ç±»å‹: {account_type})")
                if account_type == "drive115":
                    drive115_account = account
                    break
            
            if not drive115_account:
                logger.error(f"mhnotify: æœªæ‰¾åˆ°115ç½‘ç›˜è´¦æˆ·ï¼Œå¯ç”¨è´¦æˆ·ç±»å‹: {[a.get('cloud_type') for a in accounts]}")
                return
            
            account_identifier = drive115_account.get("external_id")
            logger.info(f"mhnotify: æ‰¾åˆ°115ç½‘ç›˜è´¦æˆ·: {drive115_account.get('name')}, ID: {account_identifier}")
            
            # 2. æäº¤ç½‘ç›˜ç›®å½•åˆ†æä»»åŠ¡
            analyze_url = f"{self._mh_domain}/api/v1/library-tool/analyze-cloud-directory-async"
            analyze_payload = {
                "cloud_type": "drive115",
                "account_identifier": account_identifier,
                "cloud_path": target_path
            }
            
            logger.info(f"mhnotify: æ­£åœ¨æäº¤ç½‘ç›˜åˆ†æä»»åŠ¡...")
            logger.debug(f"mhnotify: åˆ†æä»»åŠ¡å‚æ•°: {analyze_payload}")
            
            analyze_res = RequestUtils(headers=headers).post_res(analyze_url, json=analyze_payload)
            
            if not analyze_res:
                logger.error(f"mhnotify: æäº¤ç½‘ç›˜åˆ†æä»»åŠ¡å¤±è´¥ - å“åº”ä¸ºç©º")
                return
            
            if analyze_res.status_code != 200:
                logger.error(f"mhnotify: æäº¤ç½‘ç›˜åˆ†æä»»åŠ¡å¤±è´¥ - çŠ¶æ€ç : {analyze_res.status_code}, å“åº”: {analyze_res.text[:500]}")
                return
            
            try:
                analyze_data = analyze_res.json()
                logger.debug(f"mhnotify: åˆ†æä»»åŠ¡å“åº”æ•°æ®: {analyze_data}")
            except Exception as e:
                logger.error(f"mhnotify: è§£æåˆ†æä»»åŠ¡å“åº”å¤±è´¥ - {e}, åŸå§‹å“åº”: {analyze_res.text[:500]}")
                return
            
            task_id = analyze_data.get("data", {}).get("task_id")
            if not task_id:
                message = analyze_data.get("message", "")
                logger.error(f"mhnotify: æœªè·å–åˆ°åˆ†æä»»åŠ¡ID - message: {message}, å®Œæ•´å“åº”: {analyze_data}")
                return
            
            logger.info(f"mhnotify: ç½‘ç›˜åˆ†æä»»åŠ¡å·²æäº¤ï¼Œtask_id: {task_id}")
            
            # 3. å¾ªç¯æŸ¥è¯¢è¿›åº¦ç›´åˆ°å®Œæˆ
            import time
            max_wait = 300  # æœ€å¤šç­‰å¾…5åˆ†é’Ÿ
            elapsed = 0
            
            while elapsed < max_wait:
                time.sleep(2)
                elapsed += 2
                
                progress_url = f"{self._mh_domain}/api/v1/library-tool/analysis-task/{task_id}/progress"
                progress_res = RequestUtils(headers=headers).get_res(progress_url)
                
                if not progress_res or progress_res.status_code != 200:
                    logger.warning(f"mhnotify: æŸ¥è¯¢åˆ†æè¿›åº¦å¤±è´¥ï¼Œç»§ç»­ç­‰å¾…...")
                    continue
                
                try:
                    progress_data = progress_res.json()
                except Exception:
                    logger.warning(f"mhnotify: è§£æè¿›åº¦å“åº”å¤±è´¥ï¼Œç»§ç»­ç­‰å¾…...")
                    continue
                
                task_info = progress_data.get("data", {})
                progress = task_info.get("progress", 0)
                status = task_info.get("status", "")
                current_step = task_info.get("current_step", "")
                
                logger.debug(f"mhnotify: åˆ†æè¿›åº¦ {progress}% - {current_step}")
                
                if progress >= 100 and status == "completed":
                    logger.info(f"mhnotify: ç½‘ç›˜åˆ†æä»»åŠ¡å®Œæˆ")
                    break
                elif status == "failed":
                    error = task_info.get("error", "æœªçŸ¥é”™è¯¯")
                    logger.error(f"mhnotify: ç½‘ç›˜åˆ†æä»»åŠ¡å¤±è´¥: {error}")
                    return
            
            if elapsed >= max_wait:
                logger.warning(f"mhnotify: ç½‘ç›˜åˆ†æä»»åŠ¡è¶…æ—¶ï¼Œè·³è¿‡ç§»åŠ¨æ•´ç†")
                return
            
            # ç­‰å¾…3ç§’åå†è¿›è¡Œä¸‹ä¸€æ­¥ï¼Œç¡®ä¿åç«¯å¤„ç†å®Œæˆ
            logger.info(f"mhnotify: ç½‘ç›˜åˆ†æå®Œæˆï¼Œç­‰å¾…3ç§’åç»§ç»­...")
            time.sleep(3)
            
            # 4. è·å–é»˜è®¤ç›®å½•é…ç½®
            defaults_url = f"{self._mh_domain}/api/v1/subscription/config/cloud-defaults"
            logger.info(f"mhnotify: æ­£åœ¨è·å–é»˜è®¤ç›®å½•é…ç½®...")
            defaults_res = RequestUtils(headers=headers).get_res(defaults_url)
            
            if not defaults_res:
                logger.error(f"mhnotify: è·å–é»˜è®¤ç›®å½•é…ç½®å¤±è´¥ - å“åº”ä¸ºç©º")
                return
            
            if defaults_res.status_code != 200:
                logger.error(f"mhnotify: è·å–é»˜è®¤ç›®å½•é…ç½®å¤±è´¥ - çŠ¶æ€ç : {defaults_res.status_code}, å“åº”: {defaults_res.text[:500]}")
                return
            
            try:
                defaults_data = defaults_res.json()
                logger.debug(f"mhnotify: é»˜è®¤ç›®å½•é…ç½®æ•°æ®: {defaults_data}")
            except Exception as e:
                logger.error(f"mhnotify: è§£æé»˜è®¤ç›®å½•é…ç½®å¤±è´¥ - {e}, åŸå§‹å“åº”: {defaults_res.text[:500]}")
                return
            
            account_configs = defaults_data.get("data", {}).get("account_configs", {})
            account_config = account_configs.get(account_identifier, {})
            default_directory = account_config.get("default_directory", "/å½±è§†")

            
            logger.info(f"m2hnotify: è·å–åˆ°é»˜è®¤ç›®å½•: {default_directory}")
            logger.debug(f"mhnotify: è´¦æˆ·é…ç½®: {account_config}")
            
            # 5. æäº¤æ–‡ä»¶æ•´ç†ä»»åŠ¡
            logger.info(f"mhnotify: ç­‰å¾…3ç§’åæäº¤æ–‡ä»¶æ•´ç†ä»»åŠ¡...")
            time.sleep(3)
            
            organize_url = f"{self._mh_domain}/api/v1/library-tool/organize-files-async"
            organize_payload = {
                "task_id": task_id,  # ä½¿ç”¨ç½‘ç›˜åˆ†æä»»åŠ¡çš„task_id
                "cloud_type": "drive115",
                "source_path": target_path,
                "account_identifier": account_identifier,
                "target_folder_path": default_directory,
                "is_share_link": False,
                "operation_mode": "move",
                "include_series": True,
                "include_movies": True
            }
            
            logger.info(f"mhnotify: å‡†å¤‡æäº¤æ–‡ä»¶æ•´ç†ä»»åŠ¡")
            logger.info(f"mhnotify: è¯·æ±‚URL: {organize_url}")
            logger.info(f"mhnotify: è¯·æ±‚å‚æ•°: {organize_payload}")
            logger.info(f"mhnotify: è¯·æ±‚å¤´Authorization: Bearer {access_token[:20]}...")
            
            try:
                organize_res = RequestUtils(headers=headers).post_res(organize_url, json=organize_payload)
                
                # æ£€æŸ¥å“åº”å¯¹è±¡
                if organize_res is None:
                    logger.error(f"mhnotify: æäº¤æ–‡ä»¶æ•´ç†ä»»åŠ¡å¤±è´¥ - RequestUtilsè¿”å›None")
                    logger.error(f"mhnotify: è¿™å¯èƒ½æ˜¯ç½‘ç»œé”™è¯¯æˆ–è¯·æ±‚è¶…æ—¶")
                    return
                
                # æ‰“å°å“åº”çš„åŸºæœ¬ä¿¡æ¯
                logger.info(f"mhnotify: å“åº”å¯¹è±¡ç±»å‹: {type(organize_res)}")
                logger.info(f"mhnotify: å“åº”å¯¹è±¡å±æ€§: {dir(organize_res)}")
                
                # å°è¯•è·å–çŠ¶æ€ç 
                try:
                    status_code = organize_res.status_code
                    logger.info(f"mhnotify: æ–‡ä»¶æ•´ç†ä»»åŠ¡å“åº”çŠ¶æ€ç : {status_code}")
                except Exception as e:
                    logger.error(f"mhnotify: æ— æ³•è·å–å“åº”çŠ¶æ€ç : {e}")
                    return
                
                # å°è¯•è·å–å“åº”å†…å®¹
                try:
                    response_text = organize_res.text
                    logger.info(f"mhnotify: å“åº”å†…å®¹é•¿åº¦: {len(response_text)} å­—èŠ‚")
                    logger.info(f"mhnotify: å“åº”å†…å®¹: {response_text[:1000]}")
                except Exception as e:
                    logger.error(f"mhnotify: æ— æ³•è·å–å“åº”å†…å®¹: {e}")
                    return
                
            except Exception as e:
                logger.error(f"mhnotify: æäº¤æ–‡ä»¶æ•´ç†ä»»åŠ¡è¯·æ±‚å¼‚å¸¸: {e}", exc_info=True)
                return
            
            if status_code != 200:
                try:
                    error_text = response_text
                    logger.error(f"mhnotify: æäº¤æ–‡ä»¶æ•´ç†ä»»åŠ¡å¤±è´¥ - çŠ¶æ€ç : {status_code}")
                    logger.error(f"mhnotify: é”™è¯¯å“åº”å†…å®¹: {error_text}")
                except:
                    logger.error(f"mhnotify: æäº¤æ–‡ä»¶æ•´ç†ä»»åŠ¡å¤±è´¥ - çŠ¶æ€ç : {status_code}, æ— æ³•è¯»å–å“åº”å†…å®¹")
                return
            
            try:
                organize_data = organize_res.json()
                logger.info(f"mhnotify: æ•´ç†ä»»åŠ¡å“åº”JSON: {organize_data}")
            except Exception as e:
                logger.error(f"mhnotify: è§£ææ•´ç†ä»»åŠ¡å“åº”å¤±è´¥ - {e}")
                logger.error(f"mhnotify: åŸå§‹å“åº”: {response_text}")
                return
            
            # æ£€æŸ¥å“åº”çŠ¶æ€
            # æ¥å£å¯èƒ½è¿”å› success å­—æ®µï¼Œä¹Ÿå¯èƒ½è¿”å› code å­—æ®µè¡¨ç¤ºæˆåŠŸ
            success = organize_data.get("success", False)
            code = organize_data.get("code", "")
            message = organize_data.get("message", "")
            
            # code == 200 æˆ– code == "200" ä¹Ÿè§†ä¸ºæˆåŠŸ
            if not success and str(code) != "200":
                logger.error(f"mhnotify: æ–‡ä»¶æ•´ç†ä»»åŠ¡æäº¤å¤±è´¥ - code: {code}, message: {message}")
                return
            
            organize_task_id = organize_data.get("data", {}).get("task_id")
            
            if not organize_task_id:
                logger.info(f"mhnotify: æ–‡ä»¶æ•´ç†ä»»åŠ¡å·²åˆ›å»º: {message}")
            else:
                logger.info(f"mhnotify: æ–‡ä»¶æ•´ç†ä»»åŠ¡å·²æäº¤ï¼Œtask_id: {organize_task_id}, message: {message}")
            
        except Exception as e:
            logger.error(f"mhnotify: äº‘ä¸‹è½½ç§»åŠ¨æ•´ç†å¼‚å¸¸: {e}", exc_info=True)

    def _query_downloading_task_by_hash(self, client, info_hash: str) -> Optional[Dict[str, Any]]:
        """
        ä½¿ç”¨115 Web APIæŸ¥è¯¢æ­£åœ¨ä¸‹è½½çš„ç¦»çº¿ä»»åŠ¡ï¼ˆé€šè¿‡info_hashåŒ¹é…ï¼‰
        :param client: P115Clientå®ä¾‹
        :param info_hash: ä»»åŠ¡hash
        :return: ä»»åŠ¡ä¿¡æ¯å­—å…¸æˆ–None
        """
        try:
            # æ„é€ è¯·æ±‚å‚æ•°
            # å‚è€ƒ 115-downing.txtï¼Œstat=12 è¡¨ç¤ºæŸ¥è¯¢æ­£åœ¨ä¸‹è½½çš„ä»»åŠ¡
            import time as time_module
            import hashlib
            
            # è·å–ç”¨æˆ·ID
            uid = self._get_115_uid()
            if not uid:
                logger.warning(f"mhnotify: æ— æ³•è·å–115ç”¨æˆ·ID")
                return None
            
            # æ„é€ ç­¾å
            timestamp = int(time_module.time())
            sign_str = f"{uid}{timestamp}"
            sign = hashlib.md5(sign_str.encode()).hexdigest()
            
            # è°ƒç”¨ç¦»çº¿ä»»åŠ¡åˆ—è¡¨APIï¼ˆstat=12 è¡¨ç¤ºæ­£åœ¨ä¸‹è½½ï¼‰
            url = "https://115.com/web/lixian/?ct=lixian&ac=task_lists"
            params = {
                'page': 1,
                'stat': 12,  # 12=æ­£åœ¨ä¸‹è½½
                'uid': uid,
                'sign': sign,
                'time': timestamp
            }
            
            # ä½¿ç”¨RequestUtilså‘é€è¯·æ±‚
            headers = {
                "Accept": "application/json, text/javascript, */*; q=0.01",
                "Cookie": self._p115_cookie,
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
            }
            
            response = RequestUtils(headers=headers).post_res(url, data=params)
            if not response or response.status_code != 200:
                logger.debug(f"mhnotify: æŸ¥è¯¢æ­£åœ¨ä¸‹è½½ä»»åŠ¡å¤±è´¥: {response.status_code if response else 'No response'}")
                return None
            
            result = response.json()
            if not result or not result.get('state'):
                logger.debug(f"mhnotify: æ­£åœ¨ä¸‹è½½ä»»åŠ¡åˆ—è¡¨å“åº”å¼‚å¸¸: {result}")
                return None
            
            # æŸ¥æ‰¾åŒ¹é…çš„ä»»åŠ¡
            tasks = result.get('tasks', [])
            for task in tasks:
                if task.get('info_hash', '').lower() == info_hash.lower():
                    return task
            
            return None
            
        except Exception as e:
            logger.debug(f"mhnotify: æŸ¥è¯¢æ­£åœ¨ä¸‹è½½ä»»åŠ¡å¼‚å¸¸: {e}")
            return None

    def _get_115_uid(self) -> Optional[str]:
        """
        ä» cookie ä¸­è·å– 115 ç”¨æˆ· ID
        :return: ç”¨æˆ·IDæˆ–None
        """
        try:
            cookie_dict = {}
            for item in self._p115_cookie.split(';'):
                item = item.strip()
                if '=' in item:
                    k, v = item.split('=', 1)
                    cookie_dict[k.strip()] = v.strip()
            
            uid_str = cookie_dict.get('UID', '')
            if uid_str and '_' in uid_str:
                return uid_str.split('_')[0]
            
            return None
        except Exception as e:
            logger.warning(f"mhnotify: è§£æUIDå¤±è´¥: {e}")
            return None

    def _send_cloud_download_deleted_notification(self, task_name: str):
        """
        å‘é€äº‘ä¸‹è½½ä»»åŠ¡è¢«åˆ é™¤é€šçŸ¥
        :param task_name: ä»»åŠ¡åç§°
        """
        try:
            title = "âš ï¸ æœªæ‰¾åˆ°äº‘ä¸‹è½½ä»»åŠ¡"
            text = f"ğŸ“¦ ä»»åŠ¡: {task_name}\n\næœªæ‰¾åˆ°äº‘ä¸‹è½½ä»»åŠ¡ï¼Œå¯èƒ½å·²è¢«åˆ é™¤ã€‚"
            
            self.post_message(
                mtype=None,
                title=title,
                text=text
            )
            logger.info(f"mhnotify: äº‘ä¸‹è½½ä»»åŠ¡åˆ é™¤é€šçŸ¥å·²å‘é€: {task_name}")
        except Exception as e:
            logger.error(f"mhnotify: å‘é€äº‘ä¸‹è½½åˆ é™¤é€šçŸ¥å¤±è´¥: {e}", exc_info=True)

    def _send_cloud_download_failed_notification(self, task_name: str):
        """
        å‘é€äº‘ä¸‹è½½ä»»åŠ¡å¤±è´¥é€šçŸ¥
        :param task_name: ä»»åŠ¡åç§°
        """
        try:
            title = "âŒ 115äº‘ä¸‹è½½å¤±è´¥"
            text = f"ğŸ“¦ ä»»åŠ¡: {task_name}\n\nä¸‹è½½è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼Œè¯·æ£€æŸ¥115ç½‘ç›˜ã€‚"
            
            self.post_message(
                mtype=None,
                title=title,
                text=text
            )
            logger.info(f"mhnotify: äº‘ä¸‹è½½å¤±è´¥é€šçŸ¥å·²å‘é€: {task_name}")
        except Exception as e:
            logger.error(f"mhnotify: å‘é€äº‘ä¸‹è½½å¤±è´¥é€šçŸ¥å¤±è´¥: {e}", exc_info=True)

    @eventmanager.register(EventType.SubscribeDeleted)
    def _on_subscribe_deleted(self, event: Event):
        """
        ç›‘å¬ MP è®¢é˜…å–æ¶ˆäº‹ä»¶ï¼ŒæŒ‰ tmdb_id åˆ é™¤å¯¹åº”çš„ MH è®¢é˜…ï¼ˆdrive115ï¼‰
        """
        try:
            if not event or not event.event_data:
                return
            if not self._mh_assist_auto_delete:
                return
            data = event.event_data
            sid = str(data.get("subscribe_id") or data.get("id") or "")
            if not sid:
                return
            logger.info(f"mhnotify: SubscribeDeleted äº‹ä»¶æ”¶åˆ° sid={sid}")
            try:
                import json
                logger.info(f"mhnotify: SubscribeDeleted å…¨é‡äº‹ä»¶ event_data={str(json.dumps(data, ensure_ascii=False, default=str))[:2000]}")
            except Exception:
                try:
                    logger.info(f"mhnotify: SubscribeDeleted å…¨é‡äº‹ä»¶ï¼ˆfallbackï¼‰ event_data={str(data)[:2000]}")
                except Exception:
                    pass
            # è·å– tmdb_id ä¸åª’ä½“ç±»å‹
            tmdb_id = None
            mtype = None
            season = None
            # ä¼˜å…ˆä»äº‹ä»¶ subscribe_info è¯»å–
            try:
                si = (data.get("subscribe_info") or data.get("subscribe") or {}) or {}
                tmdb_id = si.get("tmdbid") or si.get("tmdb_id")
                mtype = self.__normalize_media_type(si.get("type"), si.get("type"))
                season = si.get("season")
                logger.info(f"mhnotify: SubscribeDeleted subscribe_info æå– name={si.get('name')} tmdbid={tmdb_id} type={si.get('type')} season={season}")
            except Exception:
                pass
            try:
                with SessionFactory() as db:
                    sub = SubscribeOper(db=db).get(int(sid))
                if tmdb_id is None:
                    tmdb_id = getattr(sub, 'tmdbid', None)
                if not mtype:
                    mtype = (getattr(sub, 'type', '') or '').lower()
                if season is None:
                    season = getattr(sub, 'season', None)
            except Exception:
                pass
            if not tmdb_id:
                tmdb_id = (data.get("mediainfo") or {}).get("tmdb_id") or (data.get("mediainfo") or {}).get("tmdbid")
            if not mtype:
                mtype = (data.get("mediainfo") or {}).get("type")
            if not tmdb_id:
                logger.info("mhnotify: SubscribeDeleted æœªè·å–åˆ°tmdb_idï¼Œè·³è¿‡åˆ é™¤")
                return
            token = self.__mh_login()
            if not token:
                logger.warning("mhnotify: ç™»å½•MHå¤±è´¥ï¼Œæ— æ³•æŒ‰tmdb_idåˆ é™¤è®¢é˜…")
                return
            deleted = self.__mh_delete_by_tmdb(token, tmdb_id, media_type=mtype, season=season)
            if deleted <= 0:
                logger.info("mhnotify: æœªæŒ‰tmdb_idåˆ é™¤åˆ°ä»»ä½•MHè®¢é˜…ï¼ˆdrive115ï¼‰ï¼Œå¯èƒ½æœªåˆ›å»ºæˆ–ç±»å‹ä¸ä¸€è‡´")
        except Exception:
            logger.error("mhnotify: å¤„ç†SubscribeDeletedäº‹ä»¶å¼‚å¸¸", exc_info=True)

    @eventmanager.register(EventType.SubscribeComplete)
    def _on_subscribe_complete(self, event: Event):
        try:
            if not event or not event.event_data:
                return
            if not self._mh_assist_auto_delete:
                return
            data = event.event_data
            sid = str(data.get("subscribe_id") or data.get("id") or "")
            if not sid:
                return
            logger.info(f"mhnotify: SubscribeComplete äº‹ä»¶æ”¶åˆ° sid={sid}")
            try:
                import json
                logger.info(f"mhnotify: SubscribeComplete å…¨é‡äº‹ä»¶ event_data={str(json.dumps(data, ensure_ascii=False, default=str))[:2000]}")
            except Exception:
                try:
                    logger.info(f"mhnotify: SubscribeComplete å…¨é‡äº‹ä»¶ï¼ˆfallbackï¼‰ event_data={str(data)[:2000]}")
                except Exception:
                    pass
            tmdb_id = None
            mtype = None
            season = None
            try:
                si = (data.get("subscribe_info") or data.get("subscribe") or {}) or {}
                tmdb_id = si.get("tmdbid") or si.get("tmdb_id")
                mtype = self.__normalize_media_type(si.get("type"), si.get("type"))
                season = si.get("season")
                logger.info(f"mhnotify: SubscribeComplete subscribe_info æå– name={si.get('name')} tmdbid={tmdb_id} type={si.get('type')} season={season}")
            except Exception:
                pass
            try:
                with SessionFactory() as db:
                    sub = SubscribeOper(db=db).get(int(sid))
                if tmdb_id is None:
                    tmdb_id = getattr(sub, 'tmdbid', None)
                if not mtype:
                    mtype = (getattr(sub, 'type', '') or '').lower()
                if season is None:
                    season = getattr(sub, 'season', None)
            except Exception:
                pass
            if not tmdb_id:
                tmdb_id = (data.get("mediainfo") or {}).get("tmdb_id") or (data.get("mediainfo") or {}).get("tmdbid")
            if not mtype:
                mtype = (data.get("mediainfo") or {}).get("type")
            if not tmdb_id:
                logger.info("mhnotify: SubscribeComplete æœªè·å–åˆ°tmdb_idï¼Œè·³è¿‡åˆ é™¤")
                return
            token = self.__mh_login()
            if not token:
                logger.warning("mhnotify: ç™»å½•MHå¤±è´¥ï¼Œæ— æ³•æŒ‰tmdb_idåˆ é™¤è®¢é˜…")
                return
            deleted = self.__mh_delete_by_tmdb(token, tmdb_id, media_type=mtype, season=season)
            if deleted <= 0:
                logger.info("mhnotify: æœªæŒ‰tmdb_idåˆ é™¤åˆ°ä»»ä½•MHè®¢é˜…ï¼ˆdrive115ï¼‰ï¼Œå¯èƒ½æœªåˆ›å»ºæˆ–ç±»å‹ä¸ä¸€è‡´")
        except Exception:
            logger.error("mhnotify: å¤„ç†SubscribeCompleteäº‹ä»¶å¼‚å¸¸", exc_info=True)
    @eventmanager.register(EventType.PluginAction)
    def handle_cloud_download(self, event: Event):
        """è¿œç¨‹å‘½ä»¤è§¦å‘ï¼šæ·»åŠ 115äº‘ä¸‹è½½ä»»åŠ¡"""
        if not event:
            return
        event_data = event.event_data
        if not event_data or event_data.get("action") != "mh_add_offline":
            return

        # æ£€æŸ¥åŠŸèƒ½æ˜¯å¦å¯ç”¨
        if not self._cloud_download_enabled:
            self.post_message(
                channel=event_data.get("channel"),
                title="äº‘ä¸‹è½½åŠŸèƒ½æœªå¯ç”¨",
                text="è¯·å…ˆåœ¨æ’ä»¶é…ç½®ä¸­å¯ç”¨115äº‘ä¸‹è½½åŠŸèƒ½",
                userid=event_data.get("user")
            )
            return

        # æ£€æŸ¥115 Cookieæ˜¯å¦é…ç½®
        if not self._p115_cookie:
            self.post_message(
                channel=event_data.get("channel"),
                title="115 Cookieæœªé…ç½®",
                text="è¯·å…ˆåœ¨æ’ä»¶é…ç½®ä¸­å¡«å†™115 Cookie",
                userid=event_data.get("user")
            )
            return

        # è·å–ä¸‹è½½é“¾æ¥ï¼ˆæ”¯æŒå¤šä¸ªé“¾æ¥ï¼Œç”¨é€—å·ã€ç©ºæ ¼æˆ–æ¢è¡Œåˆ†éš”ï¼‰
        download_urls_raw = event_data.get("arg_str")
        if not download_urls_raw or not download_urls_raw.strip():
            self.post_message(
                channel=event_data.get("channel"),
                title="å‚æ•°é”™è¯¯",
                text="ç”¨æ³•: /mhol <ä¸‹è½½é“¾æ¥>\næ”¯æŒå¤šä¸ªé“¾æ¥ï¼Œç”¨é€—å·åˆ†éš”: /mhol url1,url2,url3",
                userid=event_data.get("user")
            )
            return

        # è§£æå¤šä¸ªURLï¼ˆæ”¯æŒé€—å·ã€ç©ºæ ¼ã€æ¢è¡Œåˆ†éš”ï¼‰
        import re
        download_urls = re.split(r'[,\s]+', download_urls_raw.strip())
        download_urls = [url.strip() for url in download_urls if url.strip()]
        
        if not download_urls:
            self.post_message(
                channel=event_data.get("channel"),
                title="å‚æ•°é”™è¯¯",
                text="æœªè§£æåˆ°æœ‰æ•ˆçš„ä¸‹è½½é“¾æ¥",
                userid=event_data.get("user")
            )
            return

        logger.info(f"mhnotify: æ”¶åˆ°äº‘ä¸‹è½½å‘½ä»¤ï¼Œå…± {len(download_urls)} ä¸ªé“¾æ¥")

        # åˆ¤æ–­æ˜¯å¦éœ€è¦æ‰¹é‡ç›‘æ§
        is_batch = len(download_urls) > 1
        need_monitor = self._cloud_download_remove_small_files or self._cloud_download_organize

        # æ‰§è¡Œäº‘ä¸‹è½½
        success_count = 0
        fail_count = 0
        results = []
        batch_tasks = []  # ç”¨äºæ‰¹é‡ç›‘æ§çš„ä»»åŠ¡åˆ—è¡¨
        last_message = ""
        
        for idx, download_url in enumerate(download_urls, 1):
            logger.info(f"mhnotify: å¤„ç†ç¬¬ {idx}/{len(download_urls)} ä¸ªé“¾æ¥: {download_url[:80]}...")
            # æ‰¹é‡æ¨¡å¼ä¸‹ä¸å¯åŠ¨å•ç‹¬çš„ç›‘æ§çº¿ç¨‹ï¼Œç”±ç»Ÿä¸€çš„æ‰¹é‡ç›‘æ§å¤„ç†
            success, message, task_info = self._add_offline_download(download_url, start_monitor=(not is_batch))
            last_message = message
            if success:
                success_count += 1
                results.append(f"âœ… é“¾æ¥{idx}: æˆåŠŸ")
                # æ”¶é›†ä»»åŠ¡ä¿¡æ¯ç”¨äºæ‰¹é‡ç›‘æ§
                if is_batch and need_monitor and task_info.get("info_hash"):
                    batch_tasks.append(task_info)
            else:
                fail_count += 1
                results.append(f"âŒ é“¾æ¥{idx}: {message}")

        # å‘é€ç»“æœæ¶ˆæ¯
        if len(download_urls) == 1:
            # å•ä¸ªé“¾æ¥ï¼Œä¿æŒåŸæœ‰æ ¼å¼
            if success_count == 1:
                self.post_message(
                    channel=event_data.get("channel"),
                    title="äº‘ä¸‹è½½ä»»åŠ¡æ·»åŠ æˆåŠŸ",
                    text=last_message,
                    userid=event_data.get("user")
                )
            else:
                self.post_message(
                    channel=event_data.get("channel"),
                    title="äº‘ä¸‹è½½ä»»åŠ¡æ·»åŠ å¤±è´¥",
                    text=last_message,
                    userid=event_data.get("user")
                )
        else:
            # å¤šä¸ªé“¾æ¥ï¼Œæ±‡æ€»ç»“æœ
            summary = f"å…± {len(download_urls)} ä¸ªé“¾æ¥\næˆåŠŸ: {success_count} | å¤±è´¥: {fail_count}\n\n" + "\n".join(results)
            if need_monitor and batch_tasks:
                summary += f"\n\nâ³ å°†ç»Ÿä¸€ç›‘æ§ {len(batch_tasks)} ä¸ªä»»åŠ¡ï¼Œå®Œæˆåæ‰§è¡Œæ¸…ç†å’Œæ•´ç†"
            title = "äº‘ä¸‹è½½æ‰¹é‡ä»»åŠ¡å·²æäº¤" if fail_count == 0 else f"äº‘ä¸‹è½½æ‰¹é‡ä»»åŠ¡å·²æäº¤ï¼ˆ{fail_count}ä¸ªå¤±è´¥ï¼‰"
            self.post_message(
                channel=event_data.get("channel"),
                title=title,
                text=summary,
                userid=event_data.get("user")
            )
            
            # å¯åŠ¨æ‰¹é‡ç›‘æ§çº¿ç¨‹
            if need_monitor and batch_tasks:
                try:
                    import threading
                    logger.info(f"mhnotify: å¯åŠ¨æ‰¹é‡ç›‘æ§çº¿ç¨‹ï¼Œç›‘æ§ {len(batch_tasks)} ä¸ªä»»åŠ¡")
                    threading.Thread(
                        target=self._monitor_batch_downloads,
                        args=(batch_tasks,),
                        daemon=True
                    ).start()
                except Exception as e:
                    logger.warning(f"mhnotify: å¯åŠ¨æ‰¹é‡ç›‘æ§çº¿ç¨‹å¤±è´¥: {e}")

    @eventmanager.register(EventType.PluginAction)
    def handle_ali_to_115(self, event: Event):
        """è¿œç¨‹å‘½ä»¤è§¦å‘ï¼šé˜¿é‡Œäº‘ç›˜åˆ†äº«ç§’ä¼ åˆ°115"""
        if not event:
            return
        event_data = event.event_data
        if not event_data or event_data.get("action") != "mh_ali_to_115":
            return

        # æ£€æŸ¥åŠŸèƒ½æ˜¯å¦å¯ç”¨
        if not self._ali2115_enabled:
            self.post_message(
                channel=event_data.get("channel"),
                title="é˜¿é‡Œäº‘ç›˜ç§’ä¼ åŠŸèƒ½æœªå¯ç”¨",
                text="è¯·å…ˆåœ¨æ’ä»¶é…ç½®ä¸­å¯ç”¨é˜¿é‡Œäº‘ç›˜ç§’ä¼ åŠŸèƒ½",
                userid=event_data.get("user")
            )
            return

        # æ£€æŸ¥é˜¿é‡Œäº‘ç›˜ Token æ˜¯å¦é…ç½®
        if not self._ali2115_token:
            self.post_message(
                channel=event_data.get("channel"),
                title="é˜¿é‡Œäº‘ç›˜ Token æœªé…ç½®",
                text="è¯·å…ˆåœ¨æ’ä»¶é…ç½®ä¸­å¡«å†™é˜¿é‡Œäº‘ç›˜ Refresh Token",
                userid=event_data.get("user")
            )
            return

        # æ£€æŸ¥115 Cookie æ˜¯å¦é…ç½®
        if not self._p115_cookie:
            self.post_message(
                channel=event_data.get("channel"),
                title="115 Cookie æœªé…ç½®",
                text="è¯·å…ˆåœ¨æ’ä»¶é…ç½®ä¸­å¡«å†™115 Cookie",
                userid=event_data.get("user")
            )
            return

        # è·å–åˆ†äº«é“¾æ¥
        share_url = event_data.get("arg_str")
        if not share_url or not share_url.strip():
            self.post_message(
                channel=event_data.get("channel"),
                title="å‚æ•°é”™è¯¯",
                text="ç”¨æ³•: /mhaly2115 <é˜¿é‡Œäº‘ç›˜åˆ†äº«é“¾æ¥>\nä¾‹å¦‚: /mhaly2115 https://www.alipan.com/s/xxxxx",
                userid=event_data.get("user")
            )
            return

        share_url = share_url.strip()
        logger.info(f"mhnotify: æ”¶åˆ°é˜¿é‡Œäº‘ç›˜ç§’ä¼ å‘½ä»¤: {share_url}")

        # å‘é€ç¡®è®¤æ¶ˆæ¯
        self.post_message(
            channel=event_data.get("channel"),
            title="â³ é˜¿é‡Œäº‘ç›˜ç§’ä¼ ä»»åŠ¡å¼€å§‹",
            text=f"æ­£åœ¨å¤„ç†åˆ†äº«é“¾æ¥...\n{share_url[:60]}...",
            userid=event_data.get("user")
        )

        # åœ¨åå°çº¿ç¨‹æ‰§è¡Œç§’ä¼ 
        try:
            import threading
            threading.Thread(
                target=self._execute_ali_to_115,
                args=(share_url, event_data.get("channel"), event_data.get("user")),
                daemon=True
            ).start()
        except Exception as e:
            logger.error(f"mhnotify: å¯åŠ¨é˜¿é‡Œäº‘ç›˜ç§’ä¼ çº¿ç¨‹å¤±è´¥: {e}")
            self.post_message(
                channel=event_data.get("channel"),
                title="âŒ é˜¿é‡Œäº‘ç›˜ç§’ä¼ å¤±è´¥",
                text=f"å¯åŠ¨ç§’ä¼ ä»»åŠ¡å¤±è´¥: {str(e)}",
                userid=event_data.get("user")
            )

    def _execute_ali_to_115(self, share_url: str, channel: str = None, userid: str = None):
        """
        æ‰§è¡Œé˜¿é‡Œäº‘ç›˜åˆ†äº«ç§’ä¼ åˆ°115
        :param share_url: é˜¿é‡Œäº‘ç›˜åˆ†äº«é“¾æ¥
        :param channel: æ¶ˆæ¯é€šé“
        :param userid: ç”¨æˆ·ID
        """
        from hashlib import sha1
        from time import sleep
        from urllib.parse import urlparse
        from pathlib import Path
        
        try:
            # å¯¼å…¥ä¾èµ–
            try:
                from p115client import P115Client
            except ImportError:
                logger.error("mhnotify: p115client æœªå®‰è£…")
                self.post_message(
                    channel=channel,
                    title="âŒ é˜¿é‡Œäº‘ç›˜ç§’ä¼ å¤±è´¥",
                    text="p115client æœªå®‰è£…ï¼Œè¯·å…ˆå®‰è£…ä¾èµ–",
                    userid=userid
                )
                return

            try:
                from aligo import Aligo
            except ImportError:
                logger.error("mhnotify: aligo æœªå®‰è£…")
                self.post_message(
                    channel=channel,
                    title="âŒ é˜¿é‡Œäº‘ç›˜ç§’ä¼ å¤±è´¥",
                    text="aligo æœªå®‰è£…ï¼Œè¯·å…ˆå®‰è£…ä¾èµ–",
                    userid=userid
                )
                return
            
            # æå–åˆ†äº«ç 
            share_id, share_pwd = self._extract_ali_share_code(share_url)
            if not share_id:
                logger.error(f"mhnotify: æ— æ³•ä»é“¾æ¥ä¸­æå–åˆ†äº«ç : {share_url}")
                self.post_message(
                    channel=channel,
                    title="âŒ é˜¿é‡Œäº‘ç›˜ç§’ä¼ å¤±è´¥",
                    text=f"æ— æ³•ä»é“¾æ¥ä¸­æå–åˆ†äº«ç ï¼Œè¯·æ£€æŸ¥é“¾æ¥æ ¼å¼\né“¾æ¥: {share_url}",
                    userid=userid
                )
                return
            
            logger.info(f"mhnotify: æå–åˆ°åˆ†äº«ç : {share_id}")
            
            # åˆ›å»ºé˜¿é‡Œäº‘ç›˜å®¢æˆ·ç«¯
            try:
                ali_client = Aligo(refresh_token=self._ali2115_token)
                logger.info("mhnotify: é˜¿é‡Œäº‘ç›˜å®¢æˆ·ç«¯åˆ›å»ºæˆåŠŸ")
            except Exception as e:
                logger.error(f"mhnotify: åˆ›å»ºé˜¿é‡Œäº‘ç›˜å®¢æˆ·ç«¯å¤±è´¥: {e}")
                self.post_message(
                    channel=channel,
                    title="âŒ é˜¿é‡Œäº‘ç›˜ç§’ä¼ å¤±è´¥",
                    text=f"é˜¿é‡Œäº‘ç›˜ç™»å½•å¤±è´¥: {str(e)}\nè¯·æ£€æŸ¥ Refresh Token æ˜¯å¦æœ‰æ•ˆ",
                    userid=userid
                )
                return
            
            # åˆ›å»º115å®¢æˆ·ç«¯
            try:
                p115_client = P115Client(self._p115_cookie, app="web")
                logger.info("mhnotify: 115å®¢æˆ·ç«¯åˆ›å»ºæˆåŠŸ")
            except Exception as e:
                logger.error(f"mhnotify: åˆ›å»º115å®¢æˆ·ç«¯å¤±è´¥: {e}")
                self.post_message(
                    channel=channel,
                    title="âŒ é˜¿é‡Œäº‘ç›˜ç§’ä¼ å¤±è´¥",
                    text=f"115ç™»å½•å¤±è´¥: {str(e)}\nè¯·æ£€æŸ¥ Cookie æ˜¯å¦æœ‰æ•ˆ",
                    userid=userid
                )
                return
            
            # è·å–åˆ†äº« Token
            try:
                share_token = ali_client.get_share_token(share_id, share_pwd=share_pwd)
                if not getattr(share_token, 'share_token', None):
                     raise Exception(f"è·å–Tokenå¼‚å¸¸ï¼Œè¯·æ£€æŸ¥æå–ç æˆ–é“¾æ¥æ˜¯å¦æœ‰æ•ˆ: {share_token}")
                logger.info(f"mhnotify: è·å–åˆ†äº« Token æˆåŠŸ")
            except Exception as e:
                logger.error(f"mhnotify: è·å–åˆ†äº« Token å¤±è´¥: {e}")
                self.post_message(
                    channel=channel,
                    title="âŒ é˜¿é‡Œäº‘ç›˜ç§’ä¼ å¤±è´¥",
                    text=f"è·å–åˆ†äº«ä¿¡æ¯å¤±è´¥: {str(e)}\nåˆ†äº«é“¾æ¥å¯èƒ½å·²å¤±æ•ˆ",
                    userid=userid
                )
                return
            
            # è·å–æˆ–åˆ›å»ºé˜¿é‡Œäº‘ç›˜ä¸´æ—¶æ–‡ä»¶å¤¹
            ali_folder_path = self._ali2115_ali_folder.strip() or "/ç§’ä¼ è½¬å­˜"
            if not ali_folder_path.startswith('/'):
                ali_folder_path = '/' + ali_folder_path
            ali_folder_name = ali_folder_path.lstrip('/').split('/')[0]
            
            try:
                folder_info = ali_client.get_folder_by_path(path=ali_folder_name)
                if not folder_info:
                    folder_info = ali_client.create_folder(name=ali_folder_name, check_name_mode="overwrite")
                ali_folder_id = folder_info.file_id
                logger.info(f"mhnotify: é˜¿é‡Œäº‘ç›˜ä¸´æ—¶æ–‡ä»¶å¤¹ ID: {ali_folder_id}")
            except Exception as e:
                logger.error(f"mhnotify: è·å–/åˆ›å»ºé˜¿é‡Œäº‘ç›˜ä¸´æ—¶æ–‡ä»¶å¤¹å¤±è´¥: {e}")
                self.post_message(
                    channel=channel,
                    title="âŒ é˜¿é‡Œäº‘ç›˜ç§’ä¼ å¤±è´¥",
                    text=f"åˆ›å»ºé˜¿é‡Œäº‘ç›˜ä¸´æ—¶æ–‡ä»¶å¤¹å¤±è´¥: {str(e)}",
                    userid=userid
                )
                return
            
            # è·å–115ç›®æ ‡æ–‡ä»¶å¤¹ID
            target_path = self._ali2115_115_folder.strip() or "/ç§’ä¼ æ¥æ”¶"
            if not target_path.startswith('/'):
                target_path = '/' + target_path
            target_path = target_path.rstrip('/')
            
            try:
                resp = p115_client.fs_dir_getid(target_path)
                if resp and resp.get("id"):
                    target_cid = int(resp.get("id"))
                else:
                    # ç›®å½•ä¸å­˜åœ¨ï¼Œåˆ›å»ºå®ƒ
                    mkdir_resp = p115_client.fs_makedirs_app(target_path, pid=0)
                    target_cid = int(mkdir_resp.get("cid", 0))
                logger.info(f"mhnotify: 115ç›®æ ‡æ–‡ä»¶å¤¹ ID: {target_cid}")
            except Exception as e:
                logger.error(f"mhnotify: è·å–/åˆ›å»º115ç›®æ ‡æ–‡ä»¶å¤¹å¤±è´¥: {e}")
                self.post_message(
                    channel=channel,
                    title="âŒ é˜¿é‡Œäº‘ç›˜ç§’ä¼ å¤±è´¥",
                    text=f"åˆ›å»º115ç›®æ ‡æ–‡ä»¶å¤¹å¤±è´¥: {str(e)}",
                    userid=userid
                )
                return
            
            # ä¿å­˜åˆ†äº«åˆ°é˜¿é‡Œäº‘ç›˜
            try:
                logger.info("mhnotify: å¼€å§‹ä¿å­˜åˆ†äº«åˆ°é˜¿é‡Œäº‘ç›˜...")
                ali_client.share_file_save_all_to_drive(
                    share_token=share_token,
                    to_parent_file_id=ali_folder_id,
                )
                logger.info("mhnotify: åˆ†äº«ä¿å­˜æˆåŠŸï¼Œç­‰å¾…åŒæ­¥...")
                sleep(3)
            except Exception as e:
                logger.error(f"mhnotify: ä¿å­˜åˆ†äº«å¤±è´¥: {e}")
                self.post_message(
                    channel=channel,
                    title="âŒ é˜¿é‡Œäº‘ç›˜ç§’ä¼ å¤±è´¥",
                    text=f"ä¿å­˜åˆ†äº«åˆ°é˜¿é‡Œäº‘ç›˜å¤±è´¥: {str(e)}",
                    userid=userid
                )
                return
            
            # è·å–åˆ†äº«æ–‡ä»¶åˆ—è¡¨ï¼ˆé€’å½’ï¼‰
            media_exts = ['.mp4', '.mkv', '.avi', '.wmv', '.mov', '.flv', '.rmvb', '.rm', '.ts', '.m2ts', '.webm', '.mpg', '.mpeg', '.m4v', '.3gp', '.iso']
            
            def get_share_files(share_token, parent_file_id="root"):
                """é€’å½’è·å–åˆ†äº«æ–‡ä»¶åˆ—è¡¨"""
                files = []
                try:
                    file_list = ali_client.get_share_file_list(share_token, parent_file_id=parent_file_id)
                    for file in file_list:
                        if file.type == "folder":
                            files.extend(get_share_files(share_token, file.file_id))
                        else:
                            suffix = Path(file.name).suffix.lower()
                            if suffix in media_exts:
                                files.append(file)
                            else:
                                logger.debug(f"mhnotify: è·³è¿‡éåª’ä½“æ–‡ä»¶: {file.name}")
                except Exception as e:
                    logger.warning(f"mhnotify: è·å–æ–‡ä»¶åˆ—è¡¨å¼‚å¸¸: {e}")
                return files
            
            share_files = get_share_files(share_token)
            if not share_files:
                logger.warning("mhnotify: åˆ†äº«ä¸­æ²¡æœ‰æ‰¾åˆ°åª’ä½“æ–‡ä»¶")
                self.post_message(
                    channel=channel,
                    title="âš ï¸ é˜¿é‡Œäº‘ç›˜ç§’ä¼ å®Œæˆ",
                    text="åˆ†äº«ä¸­æ²¡æœ‰æ‰¾åˆ°åª’ä½“æ–‡ä»¶ï¼ˆæ”¯æŒçš„æ ¼å¼ï¼šmp4ã€mkvã€avi ç­‰ï¼‰",
                    userid=userid
                )
                return
            
            share_file_names = [f.name for f in share_files]
            logger.info(f"mhnotify: æ‰¾åˆ° {len(share_file_names)} ä¸ªåª’ä½“æ–‡ä»¶å¾…ç§’ä¼ ")
            
            # è·å–å·²è½¬å­˜æ–‡ä»¶çš„ä¸‹è½½é“¾æ¥å’ŒSHA1
            download_url_list = []
            remove_list = []
            
            def walk_files(parent_file_id, callback):
                """éå†é˜¿é‡Œäº‘ç›˜ç›®å½•"""
                try:
                    file_list = ali_client.get_file_list(parent_file_id=parent_file_id)
                    for file in file_list:
                        callback(file)
                        if file.type == "folder":
                            walk_files(file.file_id, callback)
                except Exception as e:
                    logger.warning(f"mhnotify: éå†æ–‡ä»¶å¤¹å¼‚å¸¸: {e}")
            
            file_name_list = list(share_file_names)
            
            def collect_file_info(file):
                nonlocal file_name_list
                # è®°å½•æ‰€æœ‰éå†åˆ°çš„æ–‡ä»¶IDï¼Œä»¥ä¾¿åç»­åˆ é™¤
                if file.file_id not in remove_list:
                    remove_list.append(file.file_id)
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯æˆ‘ä»¬éœ€è¦çš„ç›®æ ‡æ–‡ä»¶
                if file.type == "file":
                    # å°è¯•åŒ¹é…æ–‡ä»¶åï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼Œä¸”å¿½ç•¥æ‰©å±•åå·®å¼‚ï¼‰
                    matched_name = None
                    for name in file_name_list:
                        if name == file.name:
                            matched_name = name
                            break
                    
                    if matched_name:
                        try:
                            url_info = ali_client.get_download_url(file_id=file.file_id)
                            if url_info and url_info.url:
                                info = {
                                    "url": url_info.url,
                                    "size": url_info.size,
                                    "name": matched_name,
                                    "sha1": str(url_info.content_hash).upper(),
                                    "file_id": file.file_id
                                }
                                download_url_list.append(info)
                                file_name_list = [n for n in file_name_list if n != matched_name]
                        except Exception as e:
                            logger.warning(f"mhnotify: è·å–æ–‡ä»¶ {file.name} ä¸‹è½½é“¾æ¥å¤±è´¥: {e}")

            # ç¬¬ä¸€æ¬¡å…¨é‡åˆ·æ–°ç›®å½•ç¼“å­˜ï¼Œç¡®ä¿èƒ½è·å–åˆ°æœ€æ–°è½¬å­˜çš„æ–‡ä»¶
            # æ³¨æ„ï¼šali_client.get_file_list é»˜è®¤å¯èƒ½æœ‰ç¼“å­˜ï¼Œå°è¯•å¼ºåˆ¶éå†
            logger.info("mhnotify: æ­£åœ¨è·å–è½¬å­˜æ–‡ä»¶åˆ—è¡¨...")
            
            # æœ€å¤šå°è¯•10æ¬¡è·å–æ‰€æœ‰æ–‡ä»¶ä¿¡æ¯ (å¢åŠ é‡è¯•æ¬¡æ•°)
            for attempt in range(10):
                if not file_name_list:
                    break
                walk_files(ali_folder_id, collect_file_info)
                if file_name_list:
                    logger.info(f"mhnotify: å°šæœ‰ {len(file_name_list)} ä¸ªæ–‡ä»¶æœªè·å–åˆ°ä¸‹è½½ä¿¡æ¯ï¼Œç­‰å¾…é‡è¯• ({attempt+1}/10)...")
                    sleep(3)
            
            if not download_url_list:
                logger.error("mhnotify: æœªèƒ½è·å–ä»»ä½•æ–‡ä»¶çš„ä¸‹è½½ä¿¡æ¯")
                # å°è¯•åˆ—å‡ºå½“å‰ç›®å½•ä¸‹çš„æ–‡ä»¶ï¼Œè¾…åŠ©æ’æŸ¥
                try:
                    logger.info("mhnotify: å½“å‰é˜¿é‡Œäº‘ç›˜ä¸´æ—¶ç›®å½•ä¸‹çš„æ–‡ä»¶åˆ—è¡¨:")
                    def log_file(f):
                        logger.info(f"  - {f.name} ({f.type})")
                    walk_files(ali_folder_id, log_file)
                except:
                    pass
                
                self.post_message(
                    channel=channel,
                    title="âŒ é˜¿é‡Œäº‘ç›˜ç§’ä¼ å¤±è´¥",
                    text="æœªèƒ½è·å–æ–‡ä»¶ä¸‹è½½ä¿¡æ¯ï¼Œå¯èƒ½æ˜¯é˜¿é‡Œäº‘ç›˜è½¬å­˜å°šæœªå®ŒæˆåŒæ­¥ï¼Œè¯·é‡è¯•",
                    userid=userid
                )
                return
            
            logger.info(f"mhnotify: è·å–åˆ° {len(download_url_list)} ä¸ªæ–‡ä»¶çš„ä¸‹è½½ä¿¡æ¯")
            
            # æ‰§è¡Œç§’ä¼ åˆ°115
            success_count = 0
            fail_count = 0
            
            # åˆ›å»ºç”¨äºäºŒæ¬¡æ ¡éªŒçš„è¾…åŠ©å‡½æ•°
            def calculate_sha1_range(url: str, sign_check: str) -> str:
                """è®¡ç®—æŒ‡å®šèŒƒå›´çš„ sha1"""
                import httpx
                headers = {
                    "Range": f"bytes={sign_check}",
                    "Referer": "https://www.aliyundrive.com/",
                    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
                }
                with httpx.stream("GET", url, headers=headers, follow_redirects=True, timeout=120) as r:
                    r.raise_for_status()
                    _sha1 = sha1()
                    for chunk in r.iter_bytes(chunk_size=8192):
                        _sha1.update(chunk)
                    return _sha1.hexdigest().upper()
            
            def make_read_range_func(file_id: str, fallback_url: str):
                """åˆ›å»ºä¸€ä¸ªè¯»å–èŒƒå›´æ•°æ®çš„å‡½æ•°ï¼Œæ¯æ¬¡è°ƒç”¨æ—¶å®æ—¶è·å–æ–°çš„ä¸‹è½½é“¾æ¥"""
                def read_range_bytes_or_hash(sign_check: str) -> str:
                    # æ¯æ¬¡äºŒæ¬¡æ ¡éªŒæ—¶éƒ½é‡æ–°è·å–ä¸‹è½½é“¾æ¥ï¼ˆé˜¿é‡Œäº‘ç›˜é“¾æ¥è¿‡æœŸå¾ˆå¿«ï¼‰
                    try:
                        url_info = ali_client.get_download_url(file_id=file_id)
                        url = url_info.url
                        logger.debug(f"mhnotify: è·å–æ–°çš„ä¸‹è½½é“¾æ¥è¿›è¡ŒäºŒæ¬¡æ ¡éªŒ")
                    except Exception as e:
                        logger.warning(f"mhnotify: äºŒæ¬¡æ ¡éªŒæ—¶è·å–ä¸‹è½½é“¾æ¥å¤±è´¥: {e}ï¼Œä½¿ç”¨å¤‡ç”¨é“¾æ¥")
                        url = fallback_url
                    return calculate_sha1_range(url, sign_check)
                return read_range_bytes_or_hash
            
            def upload_to_115(file_info: dict):
                """ä¸Šä¼ æ–‡ä»¶åˆ°115"""
                file_id = file_info.get("file_id")
                file_name = file_info.get("name")
                fallback_url = file_info.get("url")
                
                # åˆ›å»ºä¸“å±äºè¿™ä¸ªæ–‡ä»¶çš„è¯»å–å‡½æ•°
                read_range_func = make_read_range_func(file_id, fallback_url)
                
                return p115_client.upload_file_init(
                    filename=file_name,
                    filesize=file_info.get("size"),
                    filesha1=file_info.get("sha1"),
                    pid=target_cid,
                    read_range_bytes_or_hash=read_range_func,
                )
            
            # é¡ºåºä¸Šä¼ ï¼ˆé¿å…å¹¶å‘æ—¶ä¸‹è½½é“¾æ¥æ··ä¹±ï¼‰
            for file_info in download_url_list:
                file_name = file_info.get("name")
                retries = 0
                max_retries = 3
                
                while retries <= max_retries:
                    try:
                        result = upload_to_115(file_info)
                        if result and isinstance(result, dict) and result.get("status") == 2:
                            logger.info(f"mhnotify: æ–‡ä»¶ '{file_name}' ç§’ä¼ æˆåŠŸ")
                            success_count += 1
                            break
                        else:
                            status_code = result.get("status", "N/A") if isinstance(result, dict) else "N/A"
                            logger.warning(f"mhnotify: æ–‡ä»¶ '{file_name}' ä¸Šä¼ çŠ¶æ€å¼‚å¸¸ (status: {status_code})")
                    except Exception as exc:
                        logger.warning(f"mhnotify: æ–‡ä»¶ '{file_name}' ä¸Šä¼ å¼‚å¸¸: {exc}")
                    
                    retries += 1
                    if retries <= max_retries:
                        delay = 2 * (2 ** (retries - 1))
                        logger.info(f"mhnotify: æ–‡ä»¶ '{file_name}' å°†åœ¨ {delay} ç§’åè¿›è¡Œç¬¬ {retries} æ¬¡é‡è¯•...")
                        sleep(delay)
                    else:
                        logger.error(f"mhnotify: æ–‡ä»¶ '{file_name}' å·²è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œæ”¾å¼ƒä¸Šä¼ ")
                        fail_count += 1
            # æ¸…ç†é˜¿é‡Œäº‘ç›˜ä¸´æ—¶æ–‡ä»¶
            try:
                logger.info("mhnotify: å¼€å§‹æ¸…ç†é˜¿é‡Œäº‘ç›˜ä¸´æ—¶æ–‡ä»¶...")
                if remove_list:
                    # ä½¿ç”¨ aligo çš„ move_file_to_trash æˆ–ç›´æ¥åˆ é™¤
                    for file_id in remove_list:
                        try:
                            ali_client.move_file_to_trash(file_id=file_id)
                        except Exception as del_err:
                            logger.debug(f"mhnotify: åˆ é™¤æ–‡ä»¶ {file_id} å¤±è´¥: {del_err}")
                logger.info(f"mhnotify: å·²æ¸…ç† {len(remove_list)} ä¸ªä¸´æ—¶æ–‡ä»¶")
            except Exception as e:
                logger.warning(f"mhnotify: æ¸…ç†é˜¿é‡Œäº‘ç›˜ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")
            
            # å‘é€ç»“æœé€šçŸ¥
            result_text = f"ğŸ“¦ åˆ†äº«é“¾æ¥: {share_url[:50]}...\n\n"
            result_text += f"âœ… ç§’ä¼ æˆåŠŸ: {success_count} ä¸ªæ–‡ä»¶\n"
            if fail_count > 0:
                result_text += f"âŒ ç§’ä¼ å¤±è´¥: {fail_count} ä¸ªæ–‡ä»¶\n"
            result_text += f"\nğŸ“‚ ä¿å­˜è·¯å¾„: {target_path}"
            
            title = "âœ… é˜¿é‡Œäº‘ç›˜ç§’ä¼ å®Œæˆ" if fail_count == 0 else f"âš ï¸ é˜¿é‡Œäº‘ç›˜ç§’ä¼ å®Œæˆï¼ˆ{fail_count}ä¸ªå¤±è´¥ï¼‰"
            
            self.post_message(
                channel=channel,
                title=title,
                text=result_text,
                userid=userid
            )
            
            logger.info(f"mhnotify: é˜¿é‡Œäº‘ç›˜ç§’ä¼ å®Œæˆï¼ŒæˆåŠŸ {success_count} ä¸ªï¼Œå¤±è´¥ {fail_count} ä¸ª")
            
            # å¦‚æœå¼€å¯äº†ç§»åŠ¨æ•´ç†ï¼Œæ‰§è¡Œæ•´ç†
            if self._ali2115_organize and success_count > 0:
                logger.info("mhnotify: å¼€å§‹æ‰§è¡Œç§’ä¼ åç§»åŠ¨æ•´ç†...")
                try:
                    access_token = self._get_mh_access_token()
                    if access_token:
                        self._organize_cloud_download(access_token, target_path)
                        self.post_message(
                            channel=channel,
                            title="ğŸ“ ç§»åŠ¨æ•´ç†å·²å¯åŠ¨",
                            text=f"æ­£åœ¨æ•´ç† {target_path} ç›®å½•ä¸­çš„æ–‡ä»¶...",
                            userid=userid
                        )
                    else:
                        logger.error("mhnotify: æ— æ³•è·å–MH access tokenï¼Œè·³è¿‡ç§»åŠ¨æ•´ç†")
                except Exception as e:
                    logger.error(f"mhnotify: ç§’ä¼ åç§»åŠ¨æ•´ç†å¤±è´¥: {e}")
            
        except Exception as e:
            logger.error(f"mhnotify: é˜¿é‡Œäº‘ç›˜ç§’ä¼ å¼‚å¸¸: {e}", exc_info=True)
            self.post_message(
                channel=channel,
                title="âŒ é˜¿é‡Œäº‘ç›˜ç§’ä¼ å¤±è´¥",
                text=f"ç§’ä¼ è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}",
                userid=userid
            )

    @staticmethod
    def _extract_ali_share_code(url: str) -> Tuple[Optional[str], Optional[str]]:
        """
        ä»é˜¿é‡Œäº‘ç›˜åˆ†äº«é“¾æ¥ä¸­æå–åˆ†äº«ç å’Œæå–ç 
        æ”¯æŒæ ¼å¼:
        - https://www.alipan.com/s/xxxxx
        - https://www.aliyundrive.com/s/xxxxx
        - é“¾æ¥åè·Ÿæå–ç /pwd/password
        - é“¾æ¥åç›´æ¥è·Ÿ4ä½æå–ç ï¼ˆç©ºæ ¼åˆ†éš”ï¼‰
        """
        import re

        share_code = None
        share_pwd = None

        try:
            # 1. æå–åˆ†äº«ID (ä¼˜å…ˆä½¿ç”¨æ­£åˆ™åŒ¹é…)
            match_id = re.search(r'/s/([a-zA-Z0-9]+)', url)
            if match_id:
                share_code = match_id.group(1)
            else:
                # å°è¯•ç®€å•åˆ†å‰²ï¼ˆå…¼å®¹æ€§ä¿ç•™ï¼‰
                clean_url = url.split()[0].strip()
                if "/s/" in clean_url:
                    parts = clean_url.split("/s/")[-1].split("/")
                    if parts:
                        share_code = parts[0].split("?")[0]

            # 2. æå–æå–ç 
            # 2.1 æ˜¾å¼æå–ç  (æå–ç : xxxx, pwd=xxxx)
            match_pwd_explicit = re.search(r'(?:æå–ç |pwd|password|code)[:=ï¼š\s]*([a-zA-Z0-9]{4})', url, re.IGNORECASE)
            if match_pwd_explicit:
                share_pwd = match_pwd_explicit.group(1)
            
            # 2.2 éšå¼æå–ç  (ç´§è·Ÿåœ¨é“¾æ¥åçš„4ä½å­—ç¬¦)
            # ä»…å½“æœªæ‰¾åˆ°æ˜¾å¼æå–ç ï¼Œä¸”æ‰¾åˆ°äº†share_codeæ—¶å°è¯•
            if not share_pwd and share_code:
                # æ„é€ æ­£åˆ™ï¼šåŒ¹é…é“¾æ¥(åŒ…å«share_code) + å¯èƒ½çš„å°¾éƒ¨å­—ç¬¦ + ç©ºç™½ + 4ä½å­—ç¬¦ + (åˆ†éš”ç¬¦æˆ–ç»“å°¾)
                # æ’é™¤ http å¼€å¤´ä»¥é˜²åŒ¹é…åˆ°ä¸‹ä¸€ä¸ªé“¾æ¥
                pattern_implicit = rf'/s/{re.escape(share_code)}[/?]*[\s\u3000]+([a-zA-Z0-9]{{4}})(?:[;\s\nï¼Œã€‚]|$)'
                match_pwd_implicit = re.search(pattern_implicit, url)
                if match_pwd_implicit:
                    candidate = match_pwd_implicit.group(1)
                    # ç®€å•è¿‡æ»¤ï¼šä¸èƒ½æ˜¯ http æˆ– www å¼€å¤´ï¼Œä¸èƒ½çº¯æ•°å­—(é€šå¸¸æå–ç å«å­—æ¯ï¼Œä½†ä¹Ÿå¯èƒ½æ˜¯çº¯æ•°å­—ï¼Œè¿™é‡Œä¸åšå¼ºé™åˆ¶ï¼Œåªé˜²å¸¸è§å•è¯)
                    if not candidate.lower().startswith(('http', 'www', 'com', 'org', 'net')):
                        share_pwd = candidate

        except Exception as e:
            logger.warning(f"mhnotify: æå–åˆ†äº«ç å¼‚å¸¸: {e}")
        return share_code, share_pwd

    def __finish_mp_subscribe(self, subscribe):
        try:
            # æ¢å¤è®¢é˜…çŠ¶æ€ä¸º Rï¼Œä¸ä¿®æ”¹ sitesï¼Œä¿ç•™åŸæœ‰çš„ç«™ç‚¹é…ç½®ä»¥ä¾¿æ´—ç‰ˆ
            with SessionFactory() as db:
                SubscribeOper(db=db).update(subscribe.id, {"state": "R"})
            
            # ç”Ÿæˆå…ƒæ•°æ®
            from app.core.metainfo import MetaInfo
            from app.schemas.types import MediaType
            from app.chain.subscribe import SubscribeChain
            from app.core.context import MediaInfo
            meta = MetaInfo(subscribe.name)
            meta.year = subscribe.year
            meta.begin_season = subscribe.season or None
            try:
                meta.type = MediaType(subscribe.type)
            except Exception:
                pass
            # æ„é€ æœ€å°å¯ç”¨çš„ mediainfoï¼ˆç”¨äºå®Œæˆè®¢é˜…æ—¥å¿—ä¸é€šçŸ¥ï¼‰
            mediainfo = MediaInfo()
            try:
                # ç±»å‹æ˜ å°„
                st = (subscribe.type or "").strip().lower()
                if st in {"ç”µå½±", "movie", "movies"}:
                    mediainfo.type = MediaType.MOVIE
                elif st in {"ç”µè§†å‰§", "tv", "series"}:
                    mediainfo.type = MediaType.TV
                else:
                    mediainfo.type = meta.type or MediaType.MOVIE
                mediainfo.title = subscribe.name
                mediainfo.year = subscribe.year
                mediainfo.tmdb_id = getattr(subscribe, 'tmdbid', None)
                mediainfo.poster_path = getattr(subscribe, 'poster', None)
                mediainfo.backdrop_path = getattr(subscribe, 'backdrop', None)
                mediainfo.overview = getattr(subscribe, 'description', None)
                mediainfo.vote_average = getattr(subscribe, 'vote', None)
            except Exception:
                pass
            # å®Œæˆè®¢é˜…
            SubscribeChain().finish_subscribe_or_not(
                subscribe=subscribe,
                meta=meta,
                mediainfo=mediainfo,
                downloads=None,
                lefts={},
                force=True
            )
        except Exception as e:
            logger.error(f"mhnotify: å®ŒæˆMPè®¢é˜…å¤±è´¥: {e}")

    # HDHive æ¨¡å—ç¼“å­˜
    _hdhive_module: Any = None
    _hdhive_module_checked: bool = False
    
    def __get_hdhive_extension_filename(self) -> Optional[str]:
        """
        æ ¹æ®å½“å‰å¹³å°è·å– hdhive æ‰©å±•æ¨¡å—çš„æ–‡ä»¶å
        
        :return: æ–‡ä»¶åï¼Œå¦‚æœå¹³å°ä¸æ”¯æŒåˆ™è¿”å› None
        """
        import platform
        machine = platform.machine().lower()
        system = platform.system().lower()
        
        # æ˜ å°„æ¶æ„åç§°
        arch_map = {
            "x86_64": "x86_64",
            "amd64": "x86_64",
            "aarch64": "aarch64",
            "arm64": "aarch64",
        }
        arch = arch_map.get(machine, machine)
        
        if system == "windows":
            return f"hdhive.cp312-win_{arch}.pyd"
        elif system == "darwin":
            return "hdhive.cpython-312-darwin.so"
        elif system == "linux":
            return f"hdhive.cpython-312-{arch}-linux-gnu.so"
        else:
            return None
    
    def __download_hdhive_module(self) -> bool:
        """
        æ£€æŸ¥å¹¶ä¸‹è½½ hdhive æ‰©å±•æ¨¡å—
        
        :return: æ˜¯å¦æˆåŠŸè·å–æ¨¡å—
        """
        import platform
        import os
        from pathlib import Path
        import urllib.request
        import urllib.error
        
        # è·å–æ’ä»¶ç›®å½•ä¸‹çš„ lib æ–‡ä»¶å¤¹
        plugin_dir = Path(__file__).parent
        lib_dir = plugin_dir / "lib"
        lib_dir.mkdir(parents=True, exist_ok=True)
        
        system = platform.system().lower()
        machine = platform.machine().lower()
        
        ext_filename = self.__get_hdhive_extension_filename()
        if not ext_filename:
            logger.warning(f"mhnotify: ä¸æ”¯æŒçš„å¹³å°: {system}/{machine}ï¼ŒHDHive æ¨¡å—åŠŸèƒ½æ— æ³•ä½¿ç”¨")
            return False
        
        target_path = lib_dir / ext_filename
        
        # æœ¬åœ°æ–‡ä»¶å·²å­˜åœ¨
        if target_path.exists():
            logger.debug(f"mhnotify: hdhive æ‰©å±•æ¨¡å—å·²å­˜åœ¨: {target_path}")
            return True
        
        # ä» GitHub ä¸‹è½½
        base_url = "https://raw.githubusercontent.com/mrtian2016/hdhive_resource/main"
        download_url = f"{base_url}/{ext_filename}"
        
        logger.info(f"mhnotify: æœ¬åœ°æœªæ‰¾åˆ° hdhive æ‰©å±•æ¨¡å—ï¼Œå°è¯•ä¸‹è½½: {download_url}")
        # é¢å¤–å¢åŠ 5æ¬¡é‡è¯•ï¼ˆå…±æœ€å¤š6æ¬¡å°è¯•ï¼‰
        max_attempts = 6
        for attempt in range(1, max_attempts + 1):
            try:
                proxy = getattr(settings, "PROXY", None)
                if proxy:
                    if isinstance(proxy, dict):
                        proxy_handler = urllib.request.ProxyHandler(proxy)
                    else:
                        proxy_handler = urllib.request.ProxyHandler({"http": proxy, "https": proxy})
                    opener = urllib.request.build_opener(proxy_handler)
                    logger.debug(f"mhnotify: ä¸‹è½½ hdhive ä½¿ç”¨ä»£ç†: {proxy}ï¼ˆç¬¬ {attempt}/{max_attempts} æ¬¡ï¼‰")
                    response = opener.open(download_url, timeout=120)
                else:
                    logger.debug(f"mhnotify: ç›´æ¥ä¸‹è½½ hdhiveï¼ˆç¬¬ {attempt}/{max_attempts} æ¬¡ï¼‰")
                    response = urllib.request.urlopen(download_url, timeout=120)
                
                with response:
                    content = response.read()
                
                with open(target_path, "wb") as f:
                    f.write(content)
                
                # é Windows å¹³å°è®¾ç½®å¯æ‰§è¡Œæƒé™
                if system != "windows":
                    os.chmod(target_path, 0o755)
                
                logger.info(f"mhnotify: âœ“ hdhive æ‰©å±•æ¨¡å—ä¸‹è½½æˆåŠŸ: {target_path}")
                return True
            except urllib.error.HTTPError as e:
                # 404 è¡¨ç¤ºèµ„æºä¸å­˜åœ¨/ä¸æ”¯æŒå¹³å°ï¼Œä¸å¿…é‡è¯•
                if e.code == 404:
                    logger.warning(f"mhnotify: âš ï¸ hdhive æ‰©å±•æ¨¡å—æš‚ä¸æ”¯æŒå½“å‰å¹³å° ({system}/{machine})ï¼Œå°†ä½¿ç”¨ HTTP API æ¨¡å¼")
                    return False
                logger.error(f"mhnotify: ä¸‹è½½ hdhive æ‰©å±•æ¨¡å—å¤±è´¥ (HTTP {e.code})ï¼Œç¬¬ {attempt}/{max_attempts} æ¬¡: {e}")
            except urllib.error.URLError as e:
                logger.error(f"mhnotify: ä¸‹è½½ hdhive æ‰©å±•æ¨¡å—å¤±è´¥ï¼ˆç½‘ç»œé”™è¯¯ï¼‰ï¼Œç¬¬ {attempt}/{max_attempts} æ¬¡: {e}")
            except Exception as e:
                logger.error(f"mhnotify: ä¸‹è½½ hdhive æ‰©å±•æ¨¡å—å¤±è´¥ï¼ˆæœªçŸ¥é”™è¯¯ï¼‰ï¼Œç¬¬ {attempt}/{max_attempts} æ¬¡: {e}")
            # è‹¥æœªåˆ°æœ€åä¸€æ¬¡ï¼ŒåšçŸ­æš‚ç­‰å¾…åé‡è¯•
            if attempt < max_attempts:
                time.sleep(2)
        return False
    
    def __load_hdhive_module(self) -> Optional[Any]:
        """
        åŠ è½½ hdhive æ¨¡å—ï¼Œä¼˜å…ˆä½¿ç”¨æœ¬åœ°ä¸‹è½½çš„æ¨¡å—
        
        :return: hdhive æ¨¡å—å¯¹è±¡ï¼Œå¤±è´¥è¿”å› None
        """
        # æ¯æ¬¡è¿è¡Œéƒ½å…ˆæ£€æµ‹æ¨¡å—æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼›ç¼ºå¤±åˆ™å°è¯•ä¸‹è½½ï¼ˆå¸¦é‡è¯•ï¼‰
        import sys
        import importlib.util
        from pathlib import Path
        plugin_dir = Path(__file__).parent
        lib_dir = plugin_dir / "lib"
        ext_filename = self.__get_hdhive_extension_filename()
        if not ext_filename:
            return None
        module_path = lib_dir / ext_filename
        if not module_path.exists():
            if not self.__download_hdhive_module():
                return None
        # è‹¥å·²åŠ è½½è¿‡ä¸”ä»ç„¶å¯ç”¨ï¼Œç›´æ¥å¤ç”¨
        if self._hdhive_module is not None:
            return self._hdhive_module
        # åŠ¨æ€åŠ è½½æ¨¡å—
        try:
            lib_dir_str = str(lib_dir)
            if lib_dir_str not in sys.path:
                sys.path.insert(0, lib_dir_str)
            spec = importlib.util.spec_from_file_location("hdhive", str(module_path))
            if spec and spec.loader:
                module = importlib.util.module_from_spec(spec)
                sys.modules["hdhive"] = module
                spec.loader.exec_module(module)
                self._hdhive_module = module
                logger.info(f"mhnotify: âœ“ hdhive æ¨¡å—åŠ è½½æˆåŠŸ")
                return module
            logger.error("mhnotify: æ— æ³•åˆ›å»º hdhive æ¨¡å— spec")
            return None
        except Exception as e:
            logger.error(f"mhnotify: åŠ è½½ hdhive æ¨¡å—å¤±è´¥: {type(e).__name__}: {e}")
            return None
    
    def __fetch_hdhive_links(self, tmdb_id: Optional[int], media_type: Optional[str]) -> List[str]:
        """
        æ ¹æ®é…ç½®ä» HDHive æŸ¥è¯¢å…è´¹115åˆ†äº«é“¾æ¥ï¼Œè¿”å› URL åˆ—è¡¨
        ä¼˜å…ˆä½¿ç”¨ hdhive æ¨¡å—ï¼ˆLinux/macOSï¼‰ï¼Œä¸å¯ç”¨æ—¶å›é€€åˆ° HTTP API æ¨¡å¼
        """
        results: List[str] = []
        try:
            logger.debug(f"mhnotify: HDHive æŸ¥è¯¢å¼€å§‹ tmdb_id={tmdb_id} media_type={media_type} enabled={self._hdhive_enabled}")
            if not self._hdhive_enabled:
                logger.debug("mhnotify: HDHive æœªå¯ç”¨ï¼Œè·³è¿‡æŸ¥è¯¢")
                return results
            if not tmdb_id:
                logger.warning("mhnotify: ç¼ºå°‘ TMDB IDï¼Œæ— æ³•ä½¿ç”¨ HDHive æŸ¥è¯¢")
                return results
            
            # å°è¯•åŠ è½½ hdhive æ¨¡å—
            hdhive_mod = self.__load_hdhive_module()
            
            if hdhive_mod:
                # ä½¿ç”¨ hdhive æ¨¡å—æŸ¥è¯¢
                return self.__fetch_hdhive_links_with_module(tmdb_id, media_type, hdhive_mod)
            else:
                # å›é€€åˆ° HTTP API æ¨¡å¼
                logger.debug("mhnotify: hdhive æ¨¡å—ä¸å¯ç”¨ï¼Œä½¿ç”¨ HTTP API æ¨¡å¼")
                return self.__fetch_hdhive_links_with_http(tmdb_id, media_type)
                
        except Exception as e:
            logger.error(f"mhnotify: __fetch_hdhive_links å¼‚å¸¸: {type(e).__name__}: {e}", exc_info=True)
            return []
    
    def __fetch_hdhive_links_with_module(self, tmdb_id: int, media_type: str, hdhive_mod: Any) -> List[str]:
        """
        ä½¿ç”¨ hdhive æ¨¡å—æŸ¥è¯¢ HDHive èµ„æº
        """
        results: List[str] = []
        try:
            h_type_str = "movie" if (media_type or "movie").lower() == "movie" else "tv"
            
            # è·å– MediaType æšä¸¾
            MediaType = getattr(hdhive_mod, 'MediaType', None)
            if MediaType is None:
                logger.error("mhnotify: hdhive æ¨¡å—ç¼ºå°‘ MediaType ç±»")
                return self.__fetch_hdhive_links_with_http(tmdb_id, media_type)
            
            h_type = MediaType.MOVIE if h_type_str == "movie" else MediaType.TV
            
            cookie = self._hdhive_cookie or ""
            
            # Cookie æœ‰æ•ˆæ€§æ£€æŸ¥å’Œåˆ·æ–°
            if self._hdhive_auto_refresh and self._hdhive_username and self._hdhive_password:
                is_valid, reason = self.__check_hdhive_cookie_valid(cookie, self._hdhive_refresh_before)
                if not cookie or not is_valid:
                    logger.info(f"HDHive: Cookie éœ€è¦åˆ·æ–° - {reason}")
                    new_cookie = self.__refresh_hdhive_cookie(self._hdhive_username, self._hdhive_password)
                    if new_cookie:
                        cookie = new_cookie
                        self._hdhive_cookie = new_cookie
                        cfg = self.get_config()
                        if isinstance(cfg, dict):
                            cfg["hdhive_cookie"] = new_cookie
                            self.update_config(cfg)
                        logger.info("HDHive: Cookie åˆ·æ–°æˆåŠŸ")
            
            if not cookie:
                if self._hdhive_username and self._hdhive_password:
                    new_cookie = self.__refresh_hdhive_cookie(self._hdhive_username, self._hdhive_password)
                    if new_cookie:
                        cookie = new_cookie
                        self._hdhive_cookie = new_cookie
                        cfg = self.get_config()
                        if isinstance(cfg, dict):
                            cfg["hdhive_cookie"] = new_cookie
                            self.update_config(cfg)
                    else:
                        logger.warning("HDHive: æ— æ³•è·å–æœ‰æ•ˆ Cookie")
                        return results
                else:
                    logger.warning("HDHive: éœ€è¦é…ç½® Cookie æˆ–ç”¨æˆ·åå¯†ç ")
                    return results
            
            proxy = getattr(settings, "PROXY", None)
            create_client = getattr(hdhive_mod, 'create_client', None)
            
            if create_client is None:
                logger.error("mhnotify: hdhive æ¨¡å—ç¼ºå°‘ create_client å‡½æ•°")
                return self.__fetch_hdhive_links_with_http(tmdb_id, media_type)
            
            logger.debug(f"mhnotify: ä½¿ç”¨ hdhive æ¨¡å—æŸ¥è¯¢ tmdb_id={tmdb_id}")
            
            with create_client(cookie=cookie, proxy=proxy) as client:
                media = client.get_media_by_tmdb_id(tmdb_id, h_type)
                if not media:
                    logger.info(f"mhnotify: HDHive æœªæ‰¾åˆ°åª’ä½“ tmdb_id={tmdb_id}")
                    return results
                
                logger.debug(f"mhnotify: HDHive æ‰¾åˆ°åª’ä½“ slug={getattr(media, 'slug', None)}")
                
                res = client.get_resources(media.slug, h_type, media_id=media.id)
                if not res or not res.success:
                    logger.info(f"mhnotify: HDHive è·å–èµ„æºå¤±è´¥")
                    return results
                
                logger.debug(f"mhnotify: HDHive è·å–åˆ°èµ„æºæ•°é‡={len(res.resources) if hasattr(res, 'resources') else 0}")
                
                for item in res.resources:
                    website_val = getattr(item.website, 'value', '') if hasattr(item, 'website') else ''
                    is_free = getattr(item, 'is_free', False)
                    
                    if website_val == '115' and is_free:
                        share = client.get_share_url(item.slug)
                        if share and share.url:
                            logger.info(f"mhnotify: HDHive è·å–åˆ°å…è´¹åˆ†äº«é“¾æ¥: {share.url}")
                            results.append(share.url)
            
            return results
            
        except Exception as e:
            logger.error(f"mhnotify: hdhive æ¨¡å—æŸ¥è¯¢å¤±è´¥: {type(e).__name__}: {e}", exc_info=True)
            # å›é€€åˆ° HTTP API
            return self.__fetch_hdhive_links_with_http(tmdb_id, media_type)
    
    def __fetch_hdhive_links_with_http(self, tmdb_id: int, media_type: str) -> List[str]:
        """
        ä½¿ç”¨ HTTP API ç›´æ¥æŸ¥è¯¢ HDHive èµ„æºï¼ˆæ— éœ€ hdhive æ¨¡å—ï¼‰
        """
        results: List[str] = []
        try:
            import requests
            
            base_url = "https://hdhive.com"
            h_type = "movie" if (media_type or "movie").lower() == "movie" else "tv"
            logger.debug(f"mhnotify: HDHive HTTP API æŸ¥è¯¢ tmdb_id={tmdb_id} h_type={h_type}")

            # API æ¨¡å¼éœ€è¦ Cookie
            query_mode = (self._hdhive_query_mode or "api").lower()
            logger.debug(f"mhnotify: HDHive æŸ¥è¯¢æ¨¡å¼: {query_mode}")
            
            cookie = self._hdhive_cookie or ""
            
            # è‡ªåŠ¨åˆ·æ–° Cookieï¼ˆè‹¥å¼€å¯ä¸” Cookie æ— æ•ˆï¼‰
            if self._hdhive_auto_refresh and self._hdhive_username and self._hdhive_password:
                is_valid, reason = self.__check_hdhive_cookie_valid(cookie, self._hdhive_refresh_before)
                logger.debug(f"mhnotify: Cookie æœ‰æ•ˆæ€§æ£€æŸ¥: valid={is_valid}, reason={reason}")
                if not cookie or not is_valid:
                    logger.info(f"HDHive: Cookie éœ€è¦åˆ·æ–° - {reason}")
                    new_cookie = self.__refresh_hdhive_cookie(self._hdhive_username, self._hdhive_password)
                    if new_cookie:
                        cookie = new_cookie
                        self._hdhive_cookie = new_cookie
                        cfg = self.get_config()
                        if isinstance(cfg, dict):
                            cfg["hdhive_cookie"] = new_cookie
                            self.update_config(cfg)
                        logger.info("HDHive: Cookie åˆ·æ–°æˆåŠŸå¹¶å·²ä¿å­˜åˆ°é…ç½®")
                    else:
                        logger.warning("mhnotify: Cookie åˆ·æ–°å¤±è´¥")
            
            if not cookie:
                # å°è¯• Playwright æ¨¡å¼ç™»å½•è·å– Cookie
                if query_mode == "playwright" and self._hdhive_username and self._hdhive_password:
                    logger.info("mhnotify: HDHive æ— æœ‰æ•ˆ Cookieï¼Œå°è¯• Playwright ç™»å½•...")
                    new_cookie = self.__refresh_hdhive_cookie(self._hdhive_username, self._hdhive_password)
                    if new_cookie:
                        cookie = new_cookie
                        self._hdhive_cookie = new_cookie
                        cfg = self.get_config()
                        if isinstance(cfg, dict):
                            cfg["hdhive_cookie"] = new_cookie
                            self.update_config(cfg)
                        logger.info("HDHive: Playwright ç™»å½•æˆåŠŸï¼ŒCookie å·²ä¿å­˜")
                    else:
                        logger.warning("HDHive: Playwright ç™»å½•å¤±è´¥")
                        return results
                else:
                    logger.warning("HDHive: éœ€è¦æœ‰æ•ˆçš„ Cookie æˆ–é…ç½®ç”¨æˆ·åå¯†ç ä½¿ç”¨ Playwright æ¨¡å¼")
                    return results
            
            # è®¾ç½®è¯·æ±‚å¤´
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                "Accept": "application/json, text/plain, */*",
                "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
                "Referer": base_url,
                "Cookie": cookie,
            }
            
            # ä» Cookie ä¸­æå– csrf_access_tokenï¼ˆç”¨äº API è¯·æ±‚ï¼‰
            csrf_token = ""
            for part in cookie.split(';'):
                part = part.strip()
                if part.startswith('csrf_access_token='):
                    csrf_token = part.split('=', 1)[1]
                    break
            if csrf_token:
                headers["X-CSRF-TOKEN"] = csrf_token
            
            proxy = getattr(settings, "PROXY", None)
            proxies = None
            if proxy:
                if isinstance(proxy, dict):
                    proxies = proxy
                else:
                    proxies = {"http": proxy, "https": proxy}
            
            session = requests.Session()
            session.headers.update(headers)
            if proxies:
                session.proxies.update(proxies)
            
            try:
                # 1. é€šè¿‡ TMDB ID æŸ¥è¯¢åª’ä½“ä¿¡æ¯
                search_url = f"{base_url}/api/media/tmdb/{tmdb_id}?type={h_type}"
                logger.debug(f"mhnotify: HDHive æŸ¥è¯¢åª’ä½“ GET {search_url}")
                
                resp = session.get(search_url, timeout=30)
                if resp.status_code != 200:
                    logger.info(f"mhnotify: HDHive æŸ¥è¯¢åª’ä½“å¤±è´¥ï¼ŒçŠ¶æ€ç ={resp.status_code}")
                    return results
                
                media_data = resp.json()
                if not media_data:
                    logger.info(f"mhnotify: HDHive æœªæ‰¾åˆ°åª’ä½“ tmdb_id={tmdb_id}")
                    return results
                
                media_slug = media_data.get("slug")
                media_id = media_data.get("id")
                logger.debug(f"mhnotify: HDHive æ‰¾åˆ°åª’ä½“ slug={media_slug} id={media_id}")
                
                if not media_slug:
                    logger.info(f"mhnotify: HDHive åª’ä½“æ•°æ®æ—  slug")
                    return results
                
                # 2. è·å–èµ„æºåˆ—è¡¨
                resources_url = f"{base_url}/api/resource/{h_type}/{media_slug}"
                if media_id:
                    resources_url += f"?media_id={media_id}"
                logger.debug(f"mhnotify: HDHive è·å–èµ„æº GET {resources_url}")
                
                resp = session.get(resources_url, timeout=30)
                if resp.status_code != 200:
                    logger.info(f"mhnotify: HDHive è·å–èµ„æºå¤±è´¥ï¼ŒçŠ¶æ€ç ={resp.status_code}")
                    return results
                
                resources_data = resp.json()
                if not resources_data or not resources_data.get("success"):
                    logger.info(f"mhnotify: HDHive è·å–èµ„æºè¿”å› success=False")
                    return results
                
                resources = resources_data.get("resources", [])
                logger.debug(f"mhnotify: HDHive è·å–åˆ°èµ„æºæ•°é‡={len(resources)}")
                
                # 3. ç­›é€‰å…è´¹çš„ 115 èµ„æºå¹¶è·å–åˆ†äº«é“¾æ¥
                for item in resources:
                    website = item.get("website", "")
                    is_free = item.get("is_free", False)
                    res_slug = item.get("slug", "")
                    logger.debug(f"mhnotify: HDHive èµ„æºé¡¹ slug={res_slug} website={website} is_free={is_free}")
                    
                    if website == "115" and is_free and res_slug:
                        # è·å–åˆ†äº«é“¾æ¥
                        share_url_api = f"{base_url}/api/resource/{res_slug}/share"
                        logger.debug(f"mhnotify: HDHive è·å–åˆ†äº«é“¾æ¥ GET {share_url_api}")
                        
                        try:
                            share_resp = session.get(share_url_api, timeout=30)
                            if share_resp.status_code == 200:
                                share_data = share_resp.json()
                                share_link = share_data.get("url") or share_data.get("share_url")
                                if share_link:
                                    logger.info(f"mhnotify: HDHive è·å–åˆ°å…è´¹åˆ†äº«é“¾æ¥: {share_link}")
                                    results.append(share_link)
                                else:
                                    logger.debug(f"mhnotify: HDHive åˆ†äº«é“¾æ¥å“åº”æ—  url å­—æ®µ: {share_data}")
                            else:
                                logger.debug(f"mhnotify: HDHive è·å–åˆ†äº«é“¾æ¥å¤±è´¥ status={share_resp.status_code}")
                        except Exception as e:
                            logger.debug(f"mhnotify: HDHive è·å–åˆ†äº«é“¾æ¥å¼‚å¸¸: {e}")
                
                logger.debug(f"mhnotify: HDHive æŸ¥è¯¢ç»“æŸï¼Œç»“æœæ•°é‡={len(results)}")
                return results
                
            except requests.exceptions.RequestException as e:
                logger.error(f"HDHive API è¯·æ±‚å¤±è´¥: {type(e).__name__}: {e}")
                return results
                
        except Exception as e:
            logger.error(f"mhnotify: __fetch_hdhive_links å¼‚å¸¸: {type(e).__name__}: {e}", exc_info=True)
            return []
    
    def __check_hdhive_cookie_valid(self, cookie: str, refresh_before: int = 3600) -> Tuple[bool, str]:
        """
        æ£€æŸ¥ HDHive Cookie æ˜¯å¦æœ‰æ•ˆ
        
        :param cookie: Cookie å­—ç¬¦ä¸²
        :param refresh_before: åœ¨è¿‡æœŸå‰å¤šå°‘ç§’è§†ä¸ºéœ€è¦åˆ·æ–°
        :return: (æ˜¯å¦æœ‰æ•ˆ, åŸå› è¯´æ˜)
        """
        import base64
        import json
        
        if not cookie:
            return False, "Cookie ä¸ºç©º"
        
        # ä» Cookie ä¸­æå– token
        token = None
        for part in cookie.split(';'):
            part = part.strip()
            if part.startswith('token='):
                token = part.split('=', 1)[1]
                break
        
        if not token:
            return False, "Cookie ä¸­æ—  token"
        
        try:
            # JWT æ ¼å¼: header.payload.signature
            parts = token.split('.')
            if len(parts) != 3:
                return False, "token æ ¼å¼é”™è¯¯"
            
            # è§£ç  payloadï¼ˆç¬¬äºŒéƒ¨åˆ†ï¼‰
            payload = parts[1]
            # è¡¥é½ base64 padding
            padding = 4 - len(payload) % 4
            if padding != 4:
                payload += '=' * padding
            
            decoded = base64.urlsafe_b64decode(payload)
            payload_data = json.loads(decoded)
            
            exp = payload_data.get('exp')
            if not exp:
                return False, "token æ— è¿‡æœŸæ—¶é—´"
            
            import time
            now = time.time()
            time_left = exp - now
            
            if time_left <= 0:
                return False, "Cookie å·²è¿‡æœŸ"
            
            if time_left < refresh_before:
                hours_left = time_left / 3600
                return False, f"Cookie å°†åœ¨ {hours_left:.1f} å°æ—¶åè¿‡æœŸ"
            
            return True, f"Cookie æœ‰æ•ˆï¼Œè¿˜æœ‰ {time_left / 3600:.1f} å°æ—¶"
            
        except Exception as e:
            logger.debug(f"mhnotify: è§£æ HDHive Cookie å¤±è´¥: {e}")
            return False, f"è§£æå¤±è´¥: {e}"
    
    def __refresh_hdhive_cookie(self, username: str, password: str) -> Optional[str]:
        """
        ä½¿ç”¨ Playwright ç™»å½• HDHive è·å–æ–° Cookie
        
        :param username: HDHive ç”¨æˆ·å
        :param password: HDHive å¯†ç 
        :return: æ–°çš„ Cookie å­—ç¬¦ä¸²ï¼Œå¤±è´¥è¿”å› None
        """
        try:
            from playwright.sync_api import sync_playwright
        except ImportError:
            logger.error("Playwright æœªå®‰è£…ï¼Œæ— æ³•è‡ªåŠ¨åˆ·æ–° HDHive Cookie")
            logger.info("è¯·è¿è¡Œ: pip install playwright && playwright install chromium")
            return None
        
        try:
            base_url = "https://hdhive.com"
            login_url = f"{base_url}/login"
            proxy = getattr(settings, "PROXY", None)
            
            with sync_playwright() as pw:
                launch_options = {"headless": True}
                context_options = {}
                
                if proxy:
                    if isinstance(proxy, dict):
                        proxy_url = proxy.get("http") or proxy.get("https")
                    else:
                        proxy_url = proxy
                    if proxy_url:
                        context_options["proxy"] = {"server": proxy_url}
                        logger.debug(f"HDHive Cookie åˆ·æ–°ä½¿ç”¨ä»£ç†: {proxy_url}")
                
                browser = pw.chromium.launch(**launch_options)
                context = browser.new_context(**context_options)
                page = context.new_page()
                
                logger.info("HDHive: è®¿é—®ç™»å½•é¡µ...")
                page.goto(login_url, wait_until="domcontentloaded", timeout=30000)
                page.wait_for_timeout(2000)
                
                # å¡«å†™ç”¨æˆ·å
                username_selectors = ['#username', 'input[name="username"]', 'input[name="email"]', 'input[type="email"]']
                username_filled = False
                for sel in username_selectors:
                    try:
                        if page.query_selector(sel):
                            page.fill(sel, username)
                            logger.debug("HDHive: âœ“ å¡«å†™ç”¨æˆ·å")
                            username_filled = True
                            break
                    except Exception:
                        continue
                
                if not username_filled:
                    logger.error("HDHive: æœªæ‰¾åˆ°ç”¨æˆ·åè¾“å…¥æ¡†")
                    context.close()
                    browser.close()
                    return None
                
                # å¡«å†™å¯†ç 
                password_selectors = ['#password', 'input[name="password"]', 'input[type="password"]']
                password_filled = False
                for sel in password_selectors:
                    try:
                        if page.query_selector(sel):
                            page.fill(sel, password)
                            logger.debug("HDHive: âœ“ å¡«å†™å¯†ç ")
                            password_filled = True
                            break
                    except Exception:
                        continue
                
                if not password_filled:
                    logger.error("HDHive: æœªæ‰¾åˆ°å¯†ç è¾“å…¥æ¡†")
                    context.close()
                    browser.close()
                    return None
                
                # æäº¤ç™»å½•
                page.wait_for_timeout(500)
                try:
                    btn = page.query_selector('button[type="submit"]')
                    if btn:
                        btn.click()
                        logger.debug("HDHive: âœ“ ç‚¹å‡»ç™»å½•æŒ‰é’®")
                    else:
                        page.keyboard.press("Enter")
                        logger.debug("HDHive: âœ“ æŒ‰ Enter é”®æäº¤")
                except Exception:
                    page.keyboard.press("Enter")
                
                # ç­‰å¾…ç™»å½•å®Œæˆ
                try:
                    page.wait_for_load_state("domcontentloaded", timeout=15000)
                except Exception:
                    pass
                
                page.wait_for_timeout(3000)
                
                # æ£€æŸ¥ç™»å½•ç»“æœ
                current_url = page.url
                logger.debug(f"HDHive: ç™»å½•å URL: {current_url}")
                
                if "/login" in current_url:
                    logger.warning("HDHive: ç™»å½•å¯èƒ½å¤±è´¥ï¼Œä»åœ¨ç™»å½•é¡µé¢")
                
                # è·å– Cookie
                cookies = context.cookies()
                cookie_parts = []
                for c in cookies:
                    if c.get("domain", "").endswith("hdhive.com"):
                        cookie_parts.append(f"{c['name']}={c['value']}")
                
                context.close()
                browser.close()
                
                if cookie_parts:
                    cookie_str = "; ".join(cookie_parts)
                    logger.info(f"HDHive: è·å– Cookie æˆåŠŸï¼Œé•¿åº¦={len(cookie_str)}")
                    return cookie_str
                else:
                    logger.warning("HDHive: æœªè·å–åˆ°æœ‰æ•ˆ Cookie")
                    return None
                    
        except Exception as e:
            logger.error(f"HDHive Playwright ç™»å½•å¤±è´¥: {type(e).__name__}: {e}", exc_info=True)
            return None
